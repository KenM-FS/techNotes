# 詳説 Deep Learning 
Josh Patterson, Adam Gibson, 本橋 和貴(監訳), 牧野 聡(訳), 新郷 美紀(訳), 2019.8.8, O'REILLY Japan

### 目的
MLP, CNN, RNN, LSTM-RNN, TB-LSTMについて知り，論文を読み進める．

---
## 1章 機械学習の概要

### 用語定義
**データマイニング**: データから情報を取り出すこと(誤用多数)

### 課題の定義
機械学習を適用する際に必要な問いかけ
- 情報あるいはモデルを抽出しようとしている，入力のデータは何か
- 与えられたデータに対して，どのようなモデルが最も最適か
- そのモデルに基づいて，新しいデータからどのような答えを引き出したいか

### 数学の基礎知識(線形代数)
データ操作に用いる数学的知識

**テンソル(tensor)**: 多次元の配列．階数で考えると，0はスカラー，1はベクトル，2は行列，3以上で狭義のテンソルだとみなせる．(要は次元数で呼び方が変わる的な話)

**超平面(hyperplane)**: 対象の空間よりも次元が1つ下の部分空間．n次元を超平面により2分割できる．

**ドット積(dot product)**: スカラー積 or 内積．**正規化(normalization)** では内積が2つのベクトルがどの程度似ているかの指標になる．2つの正規化されたベクトルの内積を  **コサイン類似度(cosine similarity)** と呼ぶ．

**要素ごとの積(アマダール積, element-wize product)**: 2つのベクトルの中で対応する位置にある値の積

**外積(テンソル積)**

逆行列化(**matrix inversion**): `x = A^{-1}b`でパラメータベクトルを求める(この式が成り立つような`x`が存在するとき，この線形系を**無矛盾(consistent)** という)

**行列分解(matrix decomposition)**: 詳しくは紙ノートに

連立1次方程式の一般的な解法
- **直接法(direct methods)**: アルゴリズム的に計算の回数が一定であることが判明している．すべての訓練データ(`A と b`)を1台のコンピュータ上に読み込める場合に効果的．
    - **ガウスの消去法(Gaussian Elimination)**
    - **正規方程式(Normal Equation)**
- **反復法(iterative methods)**: 複数回の近似と終了条件を通じて`x`を導く．個々のデータをディスクから順に読み込んで処理を行うので，メモリよりも大きなデータでもモデル化できる．データセットがクラスター上に分散しているスケールアウトのシナリオでも有効性が示されている．
    - **確率的勾配降下法(Stochastic Gradient Descent, SDG)**
    - **共役勾配法(Conjugate Gradient Methods)**
    - **交互最小2乗法(Alternating Least Squares)**

### 数学の基礎知識(統計)
NL, DLの分野で関わる数学知識

統計学
- 用語: 確率，分布，尤度

(オッズは`(事象の場合の数)/(その事象以外の場合の数)`，確率とは分母が違う)

記述統計学(descriptive statistics)
- 用語: ヒストグラム，箱ひげ図

推定統計学(inferential statistics)
- 用語: 散布図，平均，標準偏差，相関係数，p値，信頼区間

確率と推定統計の関係
- 確率は母集団から標本の推論(演繹)
- 推定統計は標本から母集団の推論

**ベイズ統計(Bayesianism)** と**頻度論(Frequentism)**
- 頻度論: 測定の繰り返しの中では確率だけに意味がある．変数の推定のために何度も実験と測定を行う必要がある．
- ベイズ統計: 何かしらの仮説の確からしさという意味での確率．変数についての信念(分布)を扱う．

### 機械学習の仕組み
レコメンド: by 協調フィルタリング(Collaboration Filtering)

**ハイパーパラメータ**: 学習の効率を挙げるためのパラメーター

**凸最適化(convex optimization)**: 凸関数の誤差関数を扱う．コストが0となる地点が存在し，その両側ではコストが急激に上がる．

**最尤推定(MLE, maximum likehood estimation)**: 横軸-パラメータ，縦軸-尤度の放物線に沿って処理．与えられたデータの尤度が最大になるようなパラメーターの組を発見する．

**勾配降下法(gradient descent)**: **停留点(stationary point)** の位置を求める．大域的な最小値(global minimum)と極小値(local minimum)．勾配を求めてパラメータベクトルを更新する際に全ての訓練データに対する全体的な損失を計算する．

**確率的勾配降下法(Stochastic Gradient Descent)**: 訓練データごとに勾配を計算して(確率的)，パラメータベクトルを更新する．学習の高速化・並列化が可能．パッチ全体を使った勾配降下法を近似したものが確率的勾配降下法．AdaGradなどで学習率を適応させたり，ヘッセ行列などの2次元的情報を使うなどして調整可能．ノイズに強い．

ミニバッチサイズの確率勾配降下法の訓練: 訓練データのいくつかを使って勾配を計算する．訓練のインスタンスが1つの場合よりも性能が高い．収束しやすい．

**準ニュートン法による最適化**: 線形探索を伴う反復的なアルゴリズム．

ヤコビアンとヘッシアン: 紙のノートに

生成モデル(generative model): データの生成方法を把握したうえで特定の種類の応答あるいは出力を行う．データを作り出すことができる．結合確率分布`p(x,y)`を学習．データ内のわかりにくい関係をとらえる確率的グラフィカルモデルとして用意される．

識別モデル(discriminative model): 単に入力された信号の分類を返す．クラス間の境界を正しくモデル化することに特化．一般的．条件付き確率分布`p(y|x)`を学習．

### モデルの評価
- 真陽性: 正しく真と認識
- 偽陽性: 誤って真と認識(第1種過誤, type I error)
- 真陰性: 正しく陰と認識
- 偽陰性: 誤って陰と認識(第2種過誤, type II error)

**感度(sensitivity)**: `感度 = 真陽性/(真陽性+偽陰性)` 再現率(recall)とも呼ばれる．モデルがどの程度偽陰性を回避したか．

↑↓ トレードオフ

**特異度(specificity)**: `特異度 = 真陰性/(真陰性+偽陽性)` モデルがどの程度偽陽性を回避できたか．

正解率(accuracy): `(真陽性+真偽性)/(全て)`

適合率(precision): 陽性的中率(positive prediction value)とも呼ばれる．`適合率=真陽性/(真陽性+偽陽性)` 正解率も適合率も高ければ妥当な測定であるといえる．

再現率: 感度と同義．真陽性率(positive value)やヒット率(hit rate)とも．

F1値(F値，F尺度): 適合率と再現率の調和平均 `F1値 = 2*真陽性/(2*真陽性+偽陽性+偽陰性)` 0.0 - 1.0の値をとる．Higher is better.情報検索の分野で，モデルがどの程度意味のある情報を取得できたかを表現するために使われる．

## 2章 NNとDLの基礎
### 2.1 NN(ほとんど学習済み)
TLU(Threshold Logic Unit): パーセプトロンの前進．ANDとORの論理関数を学習できる．

ヘヴィサイド関数: 閾値以下なら0，以上なら1を出力する階段関数．パーセプトロンで一般的に使用される．

ReLU(Rectified Linear Unit): 現時点での最高水準．勾配が0か定数なので，勾配消失・勾配爆発問題に対して有効．

Leaky ReLU: ReLUの死(dying ReLU)問題を軽減するために考案．0ではなく0.01など小さい値に．成果に一貫性がない．

ソフトプラス: ReLUを滑らかに．全体で微分可能であり，0にならない．

ヒンジ損失: 2値分類にする．

モーメンタム: 誤差が極値から脱出できなくなるのを防ぐハイパーパラメータ．

スパース度(sparsity): 少量の特徴量しか関与していない入力に対して，スパース度によるバイアスがニューロンを強制的に活性化し，ネットワークの学習を促進する．

## 3章 深層ネットワークの基礎
### 深層ネットワークの定義
具体的な4つの深層ネットワークの主要なアーキテクチャ
- 教師なしの事前訓練済みネットワーク(Unsupervised Pretrained Network)
- 畳み込みニューラルネットワーク(Convolutional Neural Network)
- リカレントニューラルネットワーク(Recurrent Neural Network)
- リカーシブニューラルネットワーク(Recursive Neural Network)

### 最適化アルゴリズム
#### 1次的な手法
ヤコビ行列はネットワーク内のパラメータについての損失関数の偏微分

大体1章でやった．

#### 2次的な手法
ヘッセ行列はヤコビ行列の微分 → ヤコビ行列のそれぞれの点での曲率を表す．

以下紹介するアルゴリズムは，ブラックボックスの探索アルゴリズムのようなもの．

**L-BFGS(Limited-memory Broyden-Fletcher-Goldfarb-Shanno)**: 準ニュートン法に分類．メモリに保持できる勾配の量が制限されたもの(ヘッセ行列全体の計算は行われない)．ヘッセ行列の逆行列の近似を計算し，重みの調整が探索空間内でもより有望な領域へ向かうように誘導．深層ネットワークで実際に使われることはない．

**共役勾配**: 共役の情報に基づいて線形探索の方向が決定される．共役L2ノルムの最小化を焦点とする．

**Hessian-free**: ニュートン法に関連．共役勾配の反復的な適用によって，2次関数の最小化を探索する．

### ハイパーパラメータ
- 層の大きさ(ニューロンの数)
- 強度(勾配，ステップの大きさ，モーメンタム，学習率)

    - 学習率: パラメータベクトルを変化させる速度．

    - Nesterovのモーメンタム，RMSProp，Adam，AdaDelta: 勾配を調整するテクニック

    - AdaGrad: 正しい学習率の発見を容易にするテクニック．単調減少をし，収束に近づくにつれて適切に減速される．(AdaDeltaは直近の値だけを保持するようにしたもの)

    - Adam: 勾配の1次と2次のモーメントから学習率が算出される．

- 正則化(ドロップアウト，ドロップコネクト，L1，L2)

  - 過学習の対策．Geoffery Hinton曰く，NNを組み立てるための最善の方法は，「過学習を発生させ，そしてそれがなくなるまで正則化を続けましょう．」ハイパーパラメータに対する正則化は過学習が発生しない方向に勾配を変化させる．
  - ドロップアウト: 隠れている要素を無視し，NNの訓練を改善するという仕組み
  - ドロップコネクト: ある接続が無視される．
  - L1正則化: パラメータ空間が一つの方向に大きくなることを防ぐ．特徴量選択の仕組みあり．重みの絶対値が乗算される → 多くは0になる．重みを容易に解釈できるようになる．
  - L2正則化: 効率的な計算．重みの2乗が減算される．

- 活性化および活性化関数群
- 重みの初期化手法
- 損失関数
- 訓練でのエポック数(ミニバッチの大きさ)
    - ミニバッチ: 複数の入力ベクトルを学習システムに与えて学習させる．→ 効率的なハードウエアリソースの利用．
- 入力データの正則化手法(ベクトル化)

### 深層ネットワークの構成要素
- 多層フィードフォワードニューラルネットワーク
- RBM
- オートエンコーダ

### RBM (Restricted Boltzmann Machine; 制限付きボルツマン機械)
ボルツマンマシン(全てのノードが全てのノードと接続しているネットワーク)に制限を加える．確率をモデル化でき，フィードフォワードネットワークの一種．2つのバイアスが用いられる(可視ユニットと不可視ユニットの行き来)．

用途
- 特徴量抽出
- 次元数の削減
- 分類
- 回帰分析
- 協調フィルタリング
- トピックのモデル化

制限: 同じ層に含まれるノード(ユニット)間での接続は禁止

オートエンコーダーの一種でもある．DBN(Deep Belief Network)などの大きなネットワークで事前訓練の層として利用される．

#### ネットワークのレイアウト
- 可視ユニット
- 不可視ユニット
- 重み
- 可視バイアスユニット
- 不可視バイアスユニット

全てのノードを可視層と不可視層に分ける．全ての可視ユニットが全ての不可視ユニットに重み付きで接続されている．(そして「制限」がある．)

入力層(可視層)に配置された各ニューロンは隠れ層(不可視層?)の全てのニューロンと接続するが，同じ層のニューロンとは接続しない．2つ目の層は不可視層と呼ばれる．不可視ユニットは**特徴量検出(feature detector)** であり，入力データから特徴量を学習する．

#### 訓練
RBMで事前訓練(pretraining)として知られている手法は，限られたデータのサンプルから元のデータを再構成する．contrastive divergenceというアルゴリズムで勾配が計算される．

#### 再構成
そのまま．入力(例えば画像)がだんだんと学習されたデータに近づいていく．

### オートエンコーダー
フィードフォワードニューラルネットワークの一種．データセットの圧縮された表現を学習するために使われる．次元削減が一般的な用途．出力は入力データを効率的な形式へと再構成したもの．異常検出としても使用可能．

多層パーセプトロンと類似しているが，入力層と出力層のユニット数が一致する，教師なし学習を行う，入力データを圧縮した表現を生成する点で異なる．

#### 訓練
誤差逆伝搬法を用いて重みを更新する．

#### 種類
圧縮オートエンコーダー(compression autoencoder): ネットワークがくびれている(ボトルネック)

ノイズ除去オートエンコーダー(denoising autoencoder): ランダムにノイズが加えられた入力から前のデータを学習する．

#### 変分オートエンコーダー(variational autoencoder, VAE)
事前分布から`z^i ~ p(z)`が算出される．

ある条件分布`x^i ~ p(x|z)`にしたがってデータのインスタンスが生成される．なお，`z`の正確な値がわからないため`p(z|x)`を正確に推論するのは困難．そこで，`p(z|x)`と`p(x|z)`の2つの分布をもとにNNで近似する．前者にはエンコーダー，後者はデコーダーが使われる．

誤差逆伝播法で訓練データ`log(x^1,...,x^N)`の周辺尤度の下限を最大化するように訓練される．

## 4章 深層ネットワークの主要なアーキテクチャー
### 教師なしの事前訓練済みネットワーク
- オートエンコーダー: 上ですでに学習済み
- DBN(Deep Belief Network)
- GAN(Generative Adversarial Network)
#### DBN
事前訓練にはRBMの層が用いられ，微調整ではフィードフォワードネットワークが用いられる．

RBMで未加工の入力ベクトルから高レベルな特徴量を抽出する．RBMをスタックしていき，より上位の層では詳細な特徴量が生成される．

得られた特徴量はフィードフォワードネットワークでの重みの初期値として利用する．(微調整フェーズ，fine-tune phaseに続く)

微調整フェーズ: 小さな学習率で緩やかな誤差逆伝搬法を通常通り行う．

#### GAN
訓練画像から新しい画像を合成するのが得意．音声，動画，文章からの画像の生成に用いられる．

2つのモデルを並列に訓練する．
- 識別器ネットワーク: 通常のCNN，生成された画像を分類する．
- 生成的ネットワーク: 逆畳み込み層(deconvolutional layer)を使って画像を生成．識別器をだませる画像を生成することが目的．

深層畳み込みGAN(DCGAN): 乱数を基に画像を生成．

条件付GAN(Conditional GAN): クラスラベルの情報を基に条件に基づいて特定のクラスのデータを生成

## CNN
画像分類特化．音声の分析も可能．自然言語の翻訳や生成，感情分析

入力データに構造(繰り返しのパターン)がある場合に真価を発揮．

### アーキテクチャ
- 入力層
- 特徴量抽出(学習)層
    1. 畳み込み層
    2. ReLU層(Rectified Linear Unit, 正規化線形関数)
    3. プーリング層
- 分類層(全結合): 出力は`b * N`(`b`: ミニバッチサイズ，`N`:クラス数)

#### 畳み込み(convolution)
特徴量検出器(feature detector)とも．

フィルター(カーネル)で畳み込み(ニュアンス)，特徴量マップ(アクティベーションマップ)を形成する．

フィルターはその層のパラメーターとして定義される

パラメーターの共有: パラメーターを共有することで合計数が減る(訓練時間の短縮)．
- 奥行きのスライス: 奥行きごとに2次元の薄切りにしたもの(RGB画像なら3枚)
- 奥行きのスライスごとに中に含まれるニューロンは全て同じ重みとバイアスを持つという制限を加える．
- 入力に特定の構造(必ず中央に顔があるなど)がある場合，有効ではない．パラメーターの共有で変換や有働への普遍性がもたらされる．

畳み込み層の数 = フィルターの数?

ハイパーパラメータ
- フィルタのサイズ
- 出力の奥行き
- ストライド: フィルタの移動量
- ゼロパディング: 出力の立体空間的サイズを入力と同じにしたい場合に使う．

バッチ正規化(batch normalization): バッチごとに直前での層でのアクティベーションを正規化する(平均を0.0に，標準偏差を1.0に近づける)と訓練が高速化できる．

#### プーリング層
畳み込み層の間に挿入される．データの表現の空間的サイズを徐々に減らしていくため．過学習を抑制できる．奥行きには独立して適応される．

最大値プーリング(max pooling): フィルターの中から最大の値が選ばれる．入力の立体に対してフィルターを用いてダウンサンプリングする．(e.g. 32pixel to 16pixel)

## リカレントニューラルネットワーク

概要は理解できる

マスキング(masking): 時系列データと静的データが混在している場合に，時系列データだけを抽出する．

時間の概念を扱うとき，**マルコフモデル** が選択肢の一つとして挙げられるが，こちらはコンテクストのウィンドウ次第で計算量が膨大になる．RNNのほうが長いタイムステップにわたる依存関係を見出せる．

勾配消失問題: 勾配が極端だと入力の構造に対して長期の依存関係をモデル化するのが難しくなる．対策-LSTM

## LSTM
### LSTMブロック
- ゲート
    - 入力ゲート(入力調整ゲート): 無関係な入力イベントからユニットを保護する
    - 忘却ゲート: ユニットが以前の記憶を消去するのを手助けする
    - 出力ゲート: ユニットの出力として記憶セルの内容を公開するか否かを決定する
- ブロックへの入力
- 記憶セル(constant error carousel)
- 出力の活性化関数
- のぞき穴(peephole)接続

BPTTやTruncated BPTTが一般的な学習アルゴリズムになる

## リカーシブニューラルネットワーク
RNNとは，訓練用データセットの中に階層構造をモデル化できる点で異なる．




