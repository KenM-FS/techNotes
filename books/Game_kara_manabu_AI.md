# ゲームから学ぶAI 環境シミュレータx深層強化学習で広がる世界
西田圭介. 株式会社技術評論社. 2022.08.06.

Finish reading at: YYYY.MM.DD

###### Purpose
シミュレーションと深層強化学習について学ぶ．
タスクに対して適切なシミュレーション環境と強化学習手法を提案できるようになる．

## Notes
- DeepMindが発表した論文を中心に紹介

歴史
- 2016年: AlphaGo (囲碁)
- 2019年: AlphaStar (StarCraft II)
  - 多くの先行研究のもとに成り立っている
    - 2013年のDQN

### 1章 ゲームAIの歴史
世界最初の電子計算機: ENIAC (1945年)
- 数年後にチェスをプレイするコンピュータプログラムの提案
- 1951年に実際に動くものが開発されている(Turochamp)

Deep Blue
- 1997年にGarry Kasparov(チェスの世界チャンピオン)に勝利
- IBMのスーパーコンピュータがベース
  - 2つのタワー型コンピュータ
  - 30個のプロセッサ
  - 480個のチェス専用チップ
- 2億手/sec
- オープニングを持つ
  - ゲーム序盤の最善とされる定石のデータベース

その後
- 2013 Ponanzaが将棋でプロ棋士に平手で勝利
- 2016 AlphaGoが囲碁で世界チャンピオンに勝利
- 2018 AlphaZeroがチェス，将棋，囲碁で最強のAIに

以前は探索と評価によって手を決定していた
- 効率の良い探索
- 質のいい評価関数
  - 手作り

将棋
- 2006 Bonanza
  - WCSC(world computer shogi championship)優勝
  - 評価関数を機械学習によって決定
    - 既存の棋譜データから評価関数を学習
  - 大量のデータから学習する時代へ
- 2013 Ponanza
  - 四段を破りプロの棋士レベルに到達
  - 自己対戦によって改善
    - 大量のリソースを使って事前に計算する
  - 2015, 2016 にWCSC優勝
- 2017 elmo
  - WCSCでPonanzaを破る
  - 学習方法に工夫
- 2018 AlphaZero
  - elmoに大差で勝利
  - 人間には太刀打ちできない領域へ

囲碁
- チェスや将棋と比較して手数がけた違いに多い
- 人間の直観的な絞り込みを再現する必要がある
- 2016 AlphaG0
  - 世界チャンピオンに勝利
  - 評価関数だけでなく，探索にも機械学習を取り入れる
- 自己対戦による学習
  - AlphaGo Zero では自己対戦に3500万ドルが投入された
- 2018 AlphaZero
  - チェス，将棋，囲碁で世界最強のAIに
  - ゼロからの機械学習でマスターに
    - 人間が積み重ねた知識に全く頼らない
    - 新しい戦法の発見

汎用ビデオゲームプレイ
- 2013 DQN
  - Atari 2600
  - 自らゲームのやり方を学習するAI
  - 深層学習と強化学習を合わせた深層強化学習の先駆け
- 2015 TensorFlow: 深層強化学習ライブラリのリリース

なぜAtari 2600なのか?
- 古典ゲームであり，ゲームに必要なCPUやメモリが少ない
  - 研究目的での利用に向いている
- 実際はStellaを使用する
  - オープンソースエミュレータ
  - ALE(Arcade Learning Environment)
    - Stella上でゲームAIを動かすためのライブラリ
      - 操作やスコア表示などゲームAI開発に便利な機能を備える
    - 2012年リリース
    - 50以上のゲームに対応
  - これらによって広く利用されるようになった
- ゲームROMは著作権が(70年は)切れていない
  - グレーなので取り扱いには注意が必要

強化学習のマップ
- https://image.slidesharecdn.com/rlssimai1-200914094519/75/-7-2048.jpg?cb=1665847238

Atari-57ベンチマーク
- Atari 2600 から選択された57のゲーム
- ベンチマークに用いられる
- 長期的な計画が必要となるゲームが苦手
  - 謎解きなど
  - プランニング(planning, 行動計画)の領域
- MuZero
  - 独自の未来予想の仕組みを導入
  - チェス，将棋，Atari-57まで対応できるように

GVGAI (General video game AI)
- 2014年に開始されたゲームAIの学習環境
- 著作権に保護されたゲームに頼らない学習環境の提供
- 200以上のミニゲームが公開
- フォワードモデル
  - ある行動を選択した際の結果をシミュレート

深層強化学習とゲーム環境
- Malmo
  - MinecraftをプレイするAIを開発するための環境
  - 2016年3月にMicrosoftから発表
  - APIによって操作
  - Malmoを使ったAIコンペティション
- OpenAI Gym
  - 現在は Gymnasium にその役割を引き継いでる
  - 2016年4月に発表されたPythonライブラリ
  - 標準化された環境による盛んな知見の共有と結果の比較
- DeepMind Lab
  - 2016年12月に発表
  - 3Dゲーム環境
    - Quake III Arena engine (a.k.a. id Tech3) がベース

StarCraft II
- 2017年8月，DeepMind と Blizzard Entertainment が共同で発表
  - StartCraft II をAI研究の環境としてオープンにする
- 囲碁に代わる新しい研究目標を提供
- スコアを最大化するのではなく，長期的な戦略で人間と競う
- 特徴
  - RTSであり，ターン性ではない
  - 部分観測であり，かつ自発的に観測しに行かなければいけない
- StarCraft (1998-)の時からAIでプレイする方法が研究されてきた
  - 2010年からStarCraft AI Competitionが毎年開催
  - Brood War API(BWAPI)によるエージェント(bot)を作成
    - いわゆるスクリプトによる操作
  - ミクロで勝ててもマクロで勝てない
- SC2LE (StarCraft II learning environment)
  - 公式が提供するAPI
  - スクリプトではなく，深層学習によるAIを組み込めるように
- AlphaStar
  - 2018年12月, TLOやMaNaに勝利
  - オンラインリーグの上位0.2%

Dota 2
- 2017年8月にDota 2をプレイするAIがOpenAIから発表
  - 1v1においてはトッププレイヤーにすでに勝利
- 2018年6月 OpenAI Five がアマチュアチームに勝利
- 2019年4月 世界チャンピオンに勝利
- 操作はヒーローごとに独立しており，それぞれが協調して動いている
- 必要とする資源は計り知れない
  - 10か月かけて4万5千年分の自己対戦を行った
- Bot Scripting によるAPIを用いて開発
  - ゲームとしての設計であるため，クラウド環境での実行など課題があった

マルチエージェントゲーム
- Capture the Flag, from DeepMind in 2018/7
- Neural MMO, from OpenAI in 2019/3
- Google Research Football, from Goole in 2019/6
- Hide-and-Seek, from OpenAI in 2019/9
- DeepMind Lab2D, from DeepMind in 2020/12
  - テキストファイルとLuaスクリプトだけで新しいマルチエージェント環境を構築できる
- XLand, from DeepMind in 2021/7

### 2章 機械学習の基礎知識
だいたい知ってるので省略

機械学習と脳の関係
- 大脳新皮質(cerebral neocortex) - 教師なし学習
  - 受け取った情報を抽象化し，過去の経験と照らし合わせて記憶する
  - 何度も行うことで記憶が定着し，既知のものと区別できるようになる(クラスタリング)
- 小脳(cerebellum) - 教師あり学習
  - 体で覚える役割
  - 大脳による行動計画を教師データとし，小脳が行動する
  - 小脳が覚えると無意識に動くようになる
- 大脳基底核(basal ganglia) - 強化学習
  - 欲を満たすように学習する
- 誤差逆伝搬法のような仕組みは脳にはない(といわれている)
  - ニューラルネットワークは，あくまでも脳が行っている情報の抽象化を真似している

ResNet (residual network, 残差ネットワーク)
- ゲームAIでも画面の認識によく使用される
- CNNの一定以上の層を重ねたときに精度が下がる問題を解決
- 中間層では残差を計算
- スキップ接続を通して入力データが上層へ

通時的誤差逆伝搬法 (backpropagation-through-time; BPTT)
- RNNに用いられる学習法
- 時間的に連続した一連の入出力をまとめて学習することで高速化する
  - 一つ一つやらない

LTSM (long short-term memory, 長・短期記憶)
- RNNの勾配消失問題を解決
- スキップ接続により解決
- 重ねて Deep LSTM (or Stacked LSTM)に

Seq2Seq
- エンコーダー(LSTM)とデコーダー(LSTM)を用意する
- エンコーダーは入力値によって隠れ状態を更新，その値をデコーダーに与える
- デコーダーは隠れ状態を受け取り値を出力

Attention
- 2つの文章の関係を数値化する

ポインターネットワーク
- 学習になかった未知のデータにも対応できるように
  - Seq2Seqの欠点
- 入力データを参照するポインターを学習する
- デコーダーにはAttentionが用いられる

Transformer
- AttentionとLSTMは組み合わされて使われてきた
- LSTMは必要ではなく，Attentionが重要だという研究が Attention is All You Need
- Attentionに特化した機構として Transformer が提案
- Attentionを計算するエンコーダーとデコーダー
- 同時処理による並列化
- Self-Attention
  - 文章内のすべての単語同士の関係を計算することで，文脈を学習

LSTMはTransformerの登場で下火になっている
- 自然言語処理だけでなく，音声認識や画像認識でも使われるように
- 時系列データを学習するための汎用的な技術として適材適所で使用されている

強化学習
- 方策: ある状態$s$が与えられたときに，行動$a$を決定する手順
- 方策ベースの強化学習
  - 方策を機械学習のモデルとして実装し，出力として行動$a$を直接計算する手法
  - 最適な行動がわからないのが問題なので，行動$a$を直接学習できるとは限らない
- 価値ベースの強化学習
  - 考えられる行動の価値を計算して，最も価値が高い行動を選択する

価値関数: 現在の状態/行動にどれだけの価値があるかを計算する関数
- 状態価値関数 $V(s)$: 状態の価値
- 行動価値関数 $Q(s,a)$ (状態行動価値関数, Q関数): ある状態である行動をとることの価値

Q値
- Q関数によって計算される行動価値
- 将来的に得られる報酬の期待値
- $Q(s_t, a_t) = r_t + \gamma Q(s_{t+1}, a_{t+1})$
  - 直近の行動による報酬 + 将来の行動から得られるQ値

Q学習
- Q関数をQテーブルで表現
  - 全ての状態と行動の組み合わせに対してQ値を管理する
  - これが難しい(現実には選択肢が無数にある)
- 経験から学習してQ値を更新する

方策勾配法
- 方策ベースの強化学習
- 正しい価値関数を教師データとして，状態から直接行動を求める

REINFORCE
- 経験から仮の価値関数を作り，それによって方策勾配法で強化学習する
- 以下の手順を繰り返す
  1. 現在の方策を用いて行動する
  2. 状態，行動，報酬を記録する
  3. 仮の価値関数を記録した情報をもとに更新する
  4. 方策を更新する(方策勾配法)

Actor-Critic
- 価値ベースと方策ベースの組み合わせ
1. Actorが方策によって状態$s$に対する行動$a$を出力
2. 環境から次の状態$s'$と報酬$r$が得られる
3. Ciriticが状態$s'$と報酬$r$から，その価値を予測する
4. 実際の報酬と比較し得られたロスを用いてActorとCriticのモデルを更新

強化学習の使い分け
- Q学習
  - 単純なゲームで高い性能
  - 行動の種類が多すぎると学習できない
- 方策勾配法
  - 単体ではほとんど用いられない
  - 何かとの組み合わせて使用される e.g. Actor-Critic
- AC
  - 現在一番アツい方法

### 3章 囲碁を学ぶAI
モンテカルロ木探索
- ランダムな手を選択する
- 各手において，ロールアウト(ランダムな手によってゲーム終了までシミュレート)を行い，その結果を記録する
- 時間になるまでロールアウトを繰り返す
- 一番勝率が高かった手を選択する
- ランダム性が強い

AlphaGo
- DeepMindが2016年1月に発表
- 深層学習とMCTSの組み合わせ
- アーキテクチャ
  - 方策ネットワーク: 次の手の確率分布$p$を出力
    - 次の手を決定する
    - 入出力: $19\times19$
    - 中間層: CNN
      - CNNの汎化能力によって盤面を認識し，あらゆる盤面に対して有効な手を選択する
    - 出力は，$0.0 \leq p \leq 1.0$ で，その位置の手が選択される確率分布になる
  - 価値ネットワーク: ゲームに勝つ確率$v$を出力
    - 評価関数，状態価値関数として用いられる
    - 現在の状態に対する将来的に得られる報酬の期待値を出力
    - 中間層にはCNN
- 学習プロセス
  - SL方策ネットワーク (supervised learning)
    - 人間同士の対局の結果を教師データとして方策ネットワークの学習
      - データを反転，回転させたものも用いている
  - RL方策ネットワーク
    - 自己対戦による強化学習
      - 対戦相手プール(opponent pool): RL方策ネットワークの世代の集まり
      - 対戦相手プールからランダムにRL方策ネットワークを選択
        - 選択される手が偏らないように，古いものも用いる
      - 最新のRL方策ネットワークと対戦(事故対戦; self-play)
        - 確率的に手を選択するので，何度も対戦する(ミニバッチ)
      - ミニバッチを何度か繰り返し，そのデータで強化学習を行い新しいRL方策ネットワークを作る
        - RL方策ネットワークの学習にはREINFORCEを用いる
          - 自己対戦のデータから価値ネットワークを作成している
          - その価値ネットワークを仮の価値関数として，改めて自己対戦のデータで方策勾配法
    - この方法だけでも有段者レベルに
- MCTSによる先読みでさらに強く
  - 次の手の候補を絞り込むのに方策ネットワークを用いる(本来はランダム)
  - 2つの方法を組み込んだ勝敗の予測
    - ロールアウト方策
      - ロールアウトをするには処理が重い(実際の対戦と同じようにするには，方策ネットワークを用いるから)
      - ロールアウト専用のNNではない軽量な機械学習モデルを方策として用いる(1000倍早いらしい)
    - 価値ネットワーク
      - 価値ネットワークを用いて盤面を評価
- 次の手を選択するステップ(シミュレーション)
  1. 選択: Q(これまでの勝率)とP(行動が選ばれる確率)を用いて次に探索するノードを選択する
    - $u(P)$: 勝負の序盤は$P$を優先(幅優先探索)し，後になるほど$Q$(深さ優先探索)を優先する
  2. 展開: 方策ネットワークで次の手の確率分布を計算してノードを展開する
    - 新しいノードを方策ネットワークを使って新しいノードを作成する(各ノードのPの値を決定する)
  3. 評価: ロールアウトによりノードの勝敗を予測する
    - ロールアウト方策と価値ネットワークの2つを使って勝率を予測
  4. バックアップ: 各ノードのQ(勝率)を更新する
    - ゲーム木を上方向にさかのぼって各ノードの新しいQの値を計算
  - 時間の許す限り繰り返し，訪問回数の最も高い手を選択する
    - 偶然によって勝率が高い手を避ける
    - マシンパワーに比例して強くなる

AlphaGo Zero
- AlphaGoの改良版として2017年10月に発表
- 人の作った棋譜データに頼らない
- 方策ネットワークと価値ネットワークを一つのネットワークから出力されるように
  - 元々構造は同じなので，出力層だけ2つに分けて前半部分を共通化させる(マルチヘッド)
  - $p, v = f(s)$
  - 入力層
    - 盤面を17段並べたもの
      - 現在の状態(2段)と過去の7回の状態(14段), 手番を表す1段
      - 現在の黒の石，現在の白の石，1手前の黒の石，1手前の白の石, ..., 7手前の黒の石，7手前の白の石，手番を表す(黒なら全て1，白なら全て0)
  - 中間層
    - 最初にCNN
    - 19層(or 39層)のResNet
  - 出力部
    - 方策ヘッド: 次の確率分布$p$を出力
    - 価値ヘッド: 状態の価値$v$を出力($-1.0 \leq v \leq 1.0$)
    - CNNと全結合ネットワークを組み合わせてどちらも構築される
- 先読みの効率化
  - ロールアウトを廃止し，価値$v$のみを用いる
  - 選択，展開，評価，バックアップ
- 学習プロセス
  - 自己対戦
    - ネットワークをランダムに初期化
    - MCTSの確率分布$\pi$に従った手の選択と，ごく低確率でランダムな手の選択
      - 状態と確率分布を記録
    - 対戦結果を保存 $z \in \{1, -1\}$
    - 状態$s$に対する確率分布$p$と価値$v$，実際に得られた確率分布$\pi$と対戦結果$z$
      - 勾配法によりネットワークの学習
  - イテレーション
    - 25,000回の自己対戦
    - ランダムな2048面を選択して強化学習(ミニバッチ)
  - ネットワークの評価
    - 1000ミニバッチごとに評価
    - イテレーション前後のネットワークで対戦
    - 400回の対戦で勝率が$55\%$を超えれば最新のネットワークとして採用する
- 結果
  - 3日でAplhaGo Leeに勝利
    - 初代AlphaGoの改良版(Lee Sedolに勝利)
    - 学習に一週間かけていた
  - 21日にAlphaGo Masterに勝利
    - 2017年1月のオンラインで全戦全勝した当時最強の囲碁AI
  - 効率的
    - AlphaGo はGPUによる分散処理
    - AlphaGo Zero ではシングルマシンで完結

AlphaZero
- 2017年12月に発表
- どのようなボードゲームでも通用することを示した
  - 囲碁，チェス，将棋
- ドメイン知識がなくても応用できる(汎用性がある)
- 入力データはゲームに合わせて用意
  - チェスと将棋は駒の種類ごとに1枚の画像を用意
  - チェス119枚，将棋362枚
- 出力
  - 考えられるすべての駒の動きを出力に
  - チェス $72枚 \times 盤面の大きさ = 4,672$
  - 将棋 $139枚 \times 盤面の大きさ = 11,259$
  - 有効手かどうかはMCTSの段階で精査される
- ネットワークの評価を単純化
  - イテレーション前後での対戦の勝率による更新を廃止
  - ネットワークはイテレーションごとに常に更新
- 自己対戦のシミュレーション回数は1600回から800回へ
- 結果
  - TPUの使用も相まって，より効率的に強くなっている
    - チェス: 4時間でStockfish超え
    - 将棋: 2時間足らずでelmo超え
    - 囲碁: 8時間でAlphaGo Lee超え
      - AlphaGo ZeroがAlphaGo Leeを超えたのは三日後
  - 各ゲームで70万回のミニバッチが実行
    - チェス: 9時間
    - 将棋: 12時間
    - 囲碁: 34時間

MuZero
- 2019年11月
- AlphaZeroをより一般化しゲームのルールそのものを学習するように
  - ボードゲームだけでなくビデオゲームまで学習できるように
- プランニングにはルールが必要
  - MCTSで先読みができるのは，行動に対する状態の変化(ルール)が既知であるから
  - ゲーム固有のルールを経験から学習するように
  - フォワードモデルがない
- アーキテクチャ
  - $p$, $v$, $f()$, に加えて，$g$(力学ネットワーク)と$h$(表現ネットワーク)を使用
  - 表現ネットワーク(representation network)
    - 環境から得られる観測データ$o$を内部的な隠れ状態(hidden state)$s$へと変換する関数$s=h(o)$
    - 観測データは盤面
      - 囲碁と将棋は過去8手，チェスは過去100手
      - Atariは32フレーム分を $96\times96$ に縮小し，RGBに色分け，32回の行動履歴
    - $o$は16層のResNetにより変換され$s$が出力
      - Atariの隠れ状態は $6\times6$ にまで縮小される
  - 予測ネットワーク(prediction network)
    - AlphaZeroのマルチヘッドと同じ($p, v = f(x)$)
  - 力学ネットワーク(dynamics network)
    - 行動に対して状態がどう遷移するかを予測するネットワーク
    - 隠れ状態の時間変化を学習する
    - $r_t, s_t = g(s_{t-1}, a_t)$
    - 16層のResNetによる実装
  - モンテカルロ木探索
      - 3つのネットワークを結合する
      1. 表現ネットワーク $h$ を使って隠れ状態 $s_0$ を計算
        - $s_0 = h(o_0)$
      2. 予測ネットワーク $f$ を使って $p_0$, $v_0$ を計算
        - $p_0, v_0 = f(s_0)$
      3. $p_0$ の確率分布に従って次の行動 $a_1$ を決定
        - 力学ネットワーク $g$ による状態変化の予測
        - $r_1, s_1 = g(s_0, a_1)$
      - 2. と 3. の繰り返しによって何手も先読みできる
- 価値等価モデル
  - 3つのネットワークをどのように学習させるか
    - 従来のモデルでは価値 $v$ しか計算できない
    - 観測データの移り変わりを予測する必要がある
  - 3つのネットワークを組み合わせて価値を求める
    - 価値を求めることができるなら，それは価値等価(value equivalence)である
- 学習プロセス
  - データ収集
    - MCTSによってボードゲームでは800回，Atari-57では50回シミュレーションを行う
    - 確率分布 $\pi$ による次の行動 $a$ の決定
    - 実際の行動から得られる報酬 $u$
      - Atari-57においては，リアルタイムにスコアとして報酬が与えられる
    - データがボードゲームでは1回の対戦ごとに，Atari-57では200回の行動ごとに強化学習用のマシンへ送られる
  - 価値等価モデルの学習
    - データから長さ $K$ のステップが切り出される
    - 切り出されたデータと，3つのネットワークを使ってゲームの流れを再現する
      - 表現ネットワークで $s_0$ を計算
      - 力学ネットワークで隠れ状態($s_0, s_1, \ldots$)を予測
      - 予測ネットワークで各状態の $p,v$ を順次得る
    - この流れは実際のゲームをプレイするのと価値等価である
    - 予測された $p, v$ の値を実際にゲームから得られた $\pi, u$ に近づければよい
  - 通時的誤差逆伝搬法(BPTT)
    - 価値等価モデルは時間的に連続なデータを展開した状態
    - RNNと同様にBPTTで学習できる
    - 長さ $K$ のデータのそれぞれについてネットワークが予測した阿知と実際のゲームプレイから得られた値のロスを計算し，BPTTによってパラメータを更新する
- 結果
  - 各ゲームで100万回の学習を実行
  - チェス，囲碁，将棋ではAlphaZeroと互角
  - Atari-57では当時最高であったR2D2を大きく上回る結果に
    - 特に先読みが有効となるゲームで高得点に
    - 51/57のゲームで人より高いスコアを獲得
      - 残り6つは探索や先読みが不足している
  - MCTSのシミュレーション回数
    - Atari-57において，100回以上だとスコアは逆に低下
      - $6 \times 6$ では未来を十分に予測できないのではないか?
      - 1回でも十分
      - 学習時には有効だけど，学習後は必要ない
        - 人間でいう反射的に行動できるようになるということ?

未来予測について
- ResNet は状態の時間変化を学習する能力がある
- World Model
  - CNNとRNNの組み合わせ
  - 画像の時間変化を学習
  - 画像を作り出し，それを学習に利用

### 4章 Atari-57を学ぶAI
Atari 2600
- 1977年に発売された家庭用ゲーム機
- 少ないROMとRAM
- ゲーム画面
  - $160 \tiems 210$
    - 本来はゲームごとに異なるが，ALE(arcade learning environment)ではこれに統一
  - 128色
    - 各ゲームごとにカラーパレットが設定されているが，ALEではRGBの24ビットカラー，もしくは8ビットのグレイスケール
  - 60FPS
    - ALEは最大で100倍でゲームが実行可能
- ゲーム操作
  - ジョイスティック: 8方向
  - ボタン: 攻撃など
  - 合計: 18種類

Atari-57ベンチマーク
- Atari 2600から選ばれた57個のゲーム
- AIの性能を測る指標として
- 環境: MDP
  - エージェントが行動を行い，環境(ALE)が状態と報酬を返す
- ゲームの終了条件(エピソード)
  - ゲームオーバー，ゲームクリア，タイムアウトなど
- スコア
  - 1エピソードで得られたスコアがAIの性能となる
  - 参考は human-normalized score; HNS
    1. 完全にランダムに行動したときのスコア
    2. 初心者がゲームをプレイしたときのスコア
    - 1.を $0$, 2.を $100$ と換算したときにAIのスコアがどの程度なのかを計算したもの
  - 各ゲームごとのHNSの平均や中央値がそのAIの評価に

モンズデマの逆襲 (Montezuma's revenge)
- Atari-57でAIにとって難しいゲームの代表
- 迷宮を探索して宝石を集めるゲーム
- 複数の部屋を遷移する必要がある
  - マイナスの報酬が多く，動かないことが最適になってしまう
  - 十分な報酬を設定するのが難しい

ビデオゲームとボードゲーム
- 対戦相手がおらず，スコアを最大化させることが目的
  - ボードゲームよりも長期的な探索が必要
- 行動の選択肢が少ないが，観測情報が多い
- 内部状態として過去の状態を記憶しておく必要がある
- ステップ数が圧倒的に多い

DQN (Deep Q network)
- 2013年にDeepMindが発表
- Q学習に深層学習の技術を応用
  - 深層強化学習の発足
- Q学習のQテーブルをNNで置き換える
  - 状態と行動の組み合わせ爆発をNNの抽象化能力で解決
- アーキテクチャ
  - Qネットワーク: Q値を計算するためのネットワーク
    - 3層の中間層からなるCNN
    - 入力データ
      - $84 \times 110$ に縮小させ，さらにUI部分(スコア表示など)を切り取り $84 \times 84$ に
      - グレイスケール
      - 過去4フレームが渡される
    - 出力データ
      - 最大で18個のQ値が一度に出力
      - 4回連続で同じ行動を行う(計算量の削減)
      - 4回行動，4フレーム読み込みを1ステップとして，毎秒15回実行(合計60FPS)
- TD学習 (temporal difference learning)
  - ゴールからさかのぼって少しづつQ値を更新
  - 直近の行動結果だけをみて学習する
    - 次に何が起こるかという時間的な差異(temporal difference)しか注目しない
  - TD誤差 (temporal difference)
    - Q値は大まかに，今回の報酬 $r_t$ と割引された将来のQ値を足し合わせたもの
    - $Q(s_t, a_t) = r_t + \gamma Q(s_{t+1}, a_{t+1})$
      - この値が最大となるような行動を選択する
    - 実際の行動より，実際の報酬 $r_t$ と次の状態 $s_{t+1}$ が返され，それによりさらに次の行動 $a_{t+1}$ が得られる
    - 未完成なQネットワークでは誤差が生じる(TD誤差)
    - TD誤差を小さくなるように誤差逆伝搬法で学習
    - Q値の高い行動が見つかれば，Q値が伝搬していく
- 学習プロセス
  - TD学習によるネットワーク更新
  - $\epsilon$-グリーディー法 (epsilon-greedy algorithm)
    - 最もQ値が高い行動だけを選択する(Q学習)のではなく，確率的にランダムな行動を選択させる
  - 経験リプレイ (experience replay)
    - 経験の内容を一時的にリプレイバッファに書き出し，そこから過去の経験を取り出して学習
    - 報酬のたびにそれを学習すると，最初の経験が多くなる偏ったネットワークになる
    - 多様なシーンを保存し，後でまとめて学習する
  - まとめ
    1. 状態 $s_0$ を初期値にセット
    2. epsilon-greedy法によって次の行動 $a_t$ を選択
      - $5\%$ ランダム
    3. 報酬 $r_t$ と次の状態 $s_{t+1}$ をリプレイバッファに保存
    4. リプレイバッファからランダムに32個データを抽出，ミニバッチとしてTD誤差を計算 & TD学習
    - ゲームが終了するまで以上のステップを繰り返す
- 結果
  - 6/7のゲームで従来(当時)のAIを上回った
  - 更なる改良で，29/49のゲームで人間以上のスコアを獲得
    - ブロック崩し，インベーダーゲームな
  - モンデズマの逆襲は全く学習できなかった(課題)

DQN改良の歴史(until 2017)
- DDQN, 2015
- 優先度付き経験リプレイ, 2015
- デュエリングネットワーク, 2016
- Categorical DQN, 2017
- Noisy Network, 2017

Rainbow
- 2017年10月にDeepMindが発表
- 上記の6つのアイディアをまとめたもの

DDQN (Double DQN)
- Qネットワークを2つ用意することで学習効率を向上
  - DQNでは一つのQネットワークを更新し，TD誤差も計算してた
- Qネットワークを定期的にコピーしてターゲットネットワーク $Q^-$ を作成
  - これをTD誤差の計算に利用
  - 直近の経験で得られた報酬を過剰に学習してしまうのを回避
  - 幅広い経験の学習を実現

優先度付き経験リプレイ (prioritized experience replay)
- リプレイバッファから取り出す優先順位をつける
- 学習する価値の高い経験ほど高頻度に利用する
- 学習時に計算したTD誤差を記録し，値が大きいものの優先度を高くする
- 新しいデータは優先度を最大に

デュエリングネットワーク (dueling network)
- QネットワークのCNNの中間層の後に状態価値とアドバンテージを学習する新たな中間層を追加
- アドバンテージ関数 (advantage function)
  - 行動価値関数から状態価値関数を引いたもの
  - $A(s,a) = Q(s, a) - V(s)$
  - ある行動がほかの行動と比較して有利か不利かを数値化した物
- Q関数の再定義
  - $Q(s,a) = V(s) + A(s, a)$
  - この式と同じようにネットワークも構築
- 状態価値の存在が学習を効率化する
  - 行動に無関係に決まってしまう報酬があり，そこに対して試行錯誤しなくて済む
  - e.g. 四方が敵に囲まれた状態では，どう行動しても価値が低くなる

マルチステップ学習 (multi-step learning)
- 1ステップごとの学習ではなく数ステップ先まで行動してから経験を記録
- Q関数を3ステップ先まで展開
$$
Q(s_t, a_t) = r_t + \gamma^1r_{t+1} + \gamma^2r_{t+2} + \gamma^3Q(s_{t+3}, a_{t+3})
$$
- より多くの報酬を使ってQネットワークをQ関数に近似させる
- リプレイバッファには連続したステップでまとめて保存

Rainbow
- 上の6つとDQNの組み合わせ
- 結果: Atari-57で人を基準としたスコアのおよそ2倍のスコアを達成
  - DQNでは50%
  - 各々だけでは100%

Ape-X
- 2018年3月にDeepMindが発表したゲームAI
- 多数のマシンで分散処理することで高速化し，より多くのデータから学習させる
- 強化学習において分散処理は難しい
  - 少しずつパラメータを更新していくため
  - Rainbow以前は一台のマシンで10日ほど学習させたものが公開されていた
- Rainbowから，DDQN，デュエリングネットワーク，マルチステップ学習，優先度付き経験リプレイを活用
  - デュエリングネットワークと環境のループを分散化
    - 大量のデータを生成
    - データをミニバッチとしてネットワークは更新できる(高速なGPUによる)
    - 更新したネットワークをデュエリングネットワークの分散したシステム上で同期するのがボトルネックとなる
      - ネットワークを1ステップごとではなく一定時間ごとに非同期的に更新する
  - 価値のある経験を優先して学習
    - 分散システムの計算パワーを使った新しい経験の発見
    - 新しいデータを優先的に学習する方法は分散システムにおいて有効ではない
    - TD誤差によって優先度をつけて経験リプレイへ追加する
- システム構成
  - Actor: ゲームをプレイする分散サービス
  - Replay: 経験リプレイを集約して管理するサービス
  - Learner: 経験リプレイにより機械学習するサービス
  - 分散されるのはActorのみ
  - AlphaGo Zero の構成とほとんど同じらしい
- 結果
  - 360台のActorを用意
    - 1台のLearnerに対して360台のActor
    - 強化学習におけるボトルネックはデータ生成
  - DQNやRainbowのスコア(10日)を20時間で達成，5日後には人間の4倍を超える
    - 生成されたデータの量は100倍以上
  - 各アクターの $\epsilon$ によってデータが多様化

人の経験リプレイ
- 海馬による短期記憶が経験リプレイ
- 大脳新皮質による長期記憶がリプレイによる学習
- 短期記憶と長期記憶が同じように参照できるのは人間のみ

R2D2 (Recurrent replay distributed DQN, 回帰型リプレイ分散DQN)
- 2018年9月にDeepMindが発表
- Ape-XをベースとしてRNNによる時間的な変化を導入
- POMDPの導入(今まではMDP)
  - エージェントが内部状態(internal state)を保持し観測できない状態を補完する
  - 隠れ状態
    - RNNの隠れ層を内部状態とみなして行動の履歴をエンコードする
- アーキテクチャ: Ape-Xの途中(CNNのあと，デュエリングネットワークのまえ)にLSTMを追加
- システム構成
  - 連続する80ステップの経験リプレイ(リプレイシーケンス)を保存
    - 40ステップごとに保存: リプレイが途切れることがない
  - 4フレーム x 80ステップ = 320フレーム が1つのリプレイシーケンスに含まれる
  - 256のActorが4倍の速度でプレイ
    - 毎秒400以上のリプレイシーケンス
- 学習プロセス BPTT
  - 1台のLearner
  - ランダムにリプレイシーケンスを取り出し学習
  - LSTMをBPTTによって学習
  - 64個のシーケンスをミニバッチとして学習
    - 毎秒320のリプレイシーケンスを消費
  - リプレイシーケンスで80ステップを読み込み時に，隠れ状態の初期値をどうするか
    1. 初期値全部0
      - その状態に至るまでの経緯を無視
    2. リプレイバッファに隠れ状態を保存
      - データ生成時に，隠れ状態 $h$ を保存し，初期値として用いる
      - 古いネットワークによる情報であり，不完全
    3. バーンイン (burn-in, 慣らし運転)
      - 初期は0として，データの前半(20-40ステップ)の出力を捨てる
    4. 2と3の組み合わせ
      - どちらだけでも1よりは良い結果に
      - 保存された状態をRNNの初期状態と用いつつ，40ステップをバーンインで消費する
      - のこった40ステップの出力使って学習
- 結果
  - 120時間(Ape-Xと同じ学習時間)でApe-Xの4倍のスコア
  - 人の20倍, 52/57で人間以上
  - Skiing, Solaris, Private Eye でパラメータ調整で高成績
  - Mentezuma's Revenge と Pitfall! は依然としてスコア伸びず

NGU (Never give up)
- 2019年9月にDeepMindから発表
- 最後の難しいゲームの攻略に挑戦
  - 報酬の頻度が低いゲーム
- 密な報酬 (dense reward) と 疎な報酬 (sparse reward)
  - 疎な報酬なゲームでは，ランダムな行動で報酬を得るのが難しい
- 外因性報酬(extrinsic reward)と内因性報酬(intrinsic reward)
  - 好奇心(curiosity)を持たせて内因性報酬を実現
    - これまでの経験を記憶したうえで，新しい経験を得る行動を優先的に選択する
      - つまり記憶力の強化
    - 知らないことを知ろうとして，新しい行動を試す
  - 探索と活用のトレードオフ (exploration-exploitation trade-off)
    - このバランス自体もエージェントに判断させる
    - 好奇心駆動探索 (curiosity-driven exploration)
      - $r = r^e + \beta r^i$
      - 拡張された報酬 (augmented reward)
      - $r^e$: 外因性報酬
      - $r^i$: 内因性報酬
  - 新規性モジュール (novelty module)
    - エピソード新規性 (episodic novelty)
      - 新しいことをやってみる
      - 埋め込みネットワーク (embedding network)
        - 情報量の多いゲーム画面をベクトルデータに埋め込む
        - 4層のCNNで長さ32のベクトルデータに
        - 単純な比較で画面の類似度が測れる
        - このベクトルを制御可能な状態 (controllable state)と呼ぶ
        - エピソード記憶へ保存される
        - シャムネットワーク (siamese network)
          - ゲーム画面からランダム性を取り除く(AIにとって意味のある変化と区別する)
          - 連続した画面を用意し，過去の画面から次の画面を予想させる(教師あり学習で学習させる)
          - 行動が予測できているなら，中間状態には予測に必要十分な情報が埋め込まれている
          - 行動とは無関係なランダムな成分はノイズとして取り除かれる
        - 2つの画面のうち片方に注目し，中間状態を生成
      - エピソード記憶 (episodic memory)
        - 埋め込みネットワークが生成した制御可能な状態をメモリに保存
        - エピソードごとにクリアされる
        - k近傍法で制御可能な状態のユークリッド距離を計算，その大きさが内因性報酬となる
          - $r^{episodic}$
    - ライフロング新規性 (life-log novelty)
      - エージェントが学習サイクルを繰り返すたびに緩やかに変化する長期的な新規性
      - RND (random network distillation, ランダムネットワーク蒸留)
        - 2018年にOpenAIから発表されたゲームAI
        - ランダムネットワークと予測ネットワークの出力を比較して経験の新しさを区別
        - ランダムネットワーク
          - 4層のCNN，ランダムに初期化
          - 学習せず初期値のまま使い続ける
        - 予測ネットワーク
          - 4層のCNN，ランダムに初期化
          - ランダムネットワークと同じ出力をするように学習
        - 2つのネットワークは最初は全く無関係な値を出力するが，過去に同じ経験をしているのであれば似た値が出力される
        - だいぶおおよそ
      - ライフロング新規性 $\alpha$ の計算
        - $\alpha = 1 + \frac{err(x) - \mu_e}{\sigma_e}$
        - 大まかに1未満なら経験済み，1以上なら新しい
  - $r^i = r^{episodic} \times min{max{\alpha, 1}, L}$
    - Lは $\alpha$ の範囲の指定
  - UVFA (universal value function approximator, 普遍的な価値関数の近似)
    - 異なる $\beta$ の値でプレイし結果をすべて学習
    - UVFAでは複数のゴールがある場合にゴールを指定できる
    - それを $\beta$ でやる
- アーキテクチャ
  - 報酬以外はR2D2と同じ
  - 短期記憶，長期記憶に加えてエピソード記憶もできた
    - エピソード記憶の回想能力はない
- 学習プロセス: R2D2と全く同じ
  - $\beta$ の値によって期待通りの行動の変化
- 結果
  - 難しいゲームの高いスコアを達成
  - 探索を必要としないゲームでスコアが低下
  - $\beta$ もゲームによって最適な値が変化
  - 外因性報酬を0にしても平均的に人間のスコアを超越
    - 長く生き延びることが高いスコアに直結しやすいから
  - 探索を必要としないゲームではR2D2に及ばない(人の20倍に対して13.5倍)
    - 好奇心があだとなる
  - 51/57 人を上回る，特に探索が必要となるゲーム(モンテズマの逆襲やPitfall!)

Agent57
- 2020年3月にDeepMindから発表
- Atari-57 の全てのゲームで人のスコアを上回るのが目標
  - ALEが公開されてから8年
- NGUを改良したもの，DQN以降の研究成果の集大成
  - 2015年: DQN
  - 2017年: RainbowによるDQNの改良(DDQN, マルチステップ, デュエリングネットワーク,  優先度付き経験リプレイ)
  - 2018年: Ape-Xによる分散処理(Learner, Actor, Replay)
  - 2019年: R2D2による短期記憶(POMDPの導入, LSTM)
  - 2019年: NGUによる内因性報酬(エピソード記憶, UVFA, RND)
  - 2020年: Agent57によるメタコントローラー(探索と活用のトレードオフ, タイムホライズン, 多腕バンディット)
- 評価基準をHNSから5パーセンタイル(のHNS)に
  - 目的はスコアを上げるよりも，難しいゲームをクリアすること
  - パーセンタイル (percentile): 数値を小さい順に数えて何%の位置にあるか示したもの
  - Atari-57においてスコアが5パーセンタイルにあるゲームのスコアを改善させる
    - 57のゲームのうち3番目に悪いもの
    - ほぼすべてのゲームのスコアを改善させなければいけない
    - 過去のAIは5パーセンタイルスコアはほぼ改善せず
      - R2D2やNGUの短期記憶を用いた探索範囲の拡大によって改善
- 多腕バンディット問題 (multi-armed bandit problem)
  - NGUでは探索と活用のトレードオフがうまくいかなかった
    - 探索が必要のないゲームではスコアが低下
  - MCTSはトレードオフへの解答の一つ
  - 多腕バンディット問題: 不確実な意思決定が必要になる状況
    - 限られた試行回数の中で最大の報酬を得る方法を見つける問題
  - UCB法 (upper confidence bound algorithm)
    - 最初にすべての台を1回回す
    - 各台のスコアを計算する
      - $スコア = これまでに当たった回数 + 補正値$
      - 補正値は台を回すほど減っていく値
- NGUにおいて，探索は $r^i$ に従うこと，活用は $r^e$ に従うこと， $\beta$ がそのバランスを決めていた
  - このバランスをUCB法で決める
- タイムホライズン (time horizon, 時間的水平線)
  - 将来のどのくらいの期間を計画に盛り込むか
  - どれだけ先の報酬まで学習に用いるか
  - 疎な報酬のゲームでは，どの行動が報酬に結び付いたのかわからない
    - 長期ホライゾン (long-term horizon) なゲーム
  - このようなゲームでは，割引率によって報酬が少なくなり行動の改善につながらない
- メタコントローラー (meta-controller)
  - ハイパーパラメーター自体を学習させるもの
    - 探索と活用のハイパーパラメーター: $\beta$
    - タイムホライズンのハイパーパラメーター: $\gamma$
  - UCB法を使ってハイパーパラメーターを動的に決定
  - スライディングウィンドウUCB (sliding-window UCB)
    - 学習の進捗によって最適な組み合わせを見つける
    - 直近の一定回数分の結果でUCBのスコアを決定する
- アーキテクチャ
  - NGUのネットワークを複製して，その出力をつなぎ合わせる構造
  - それぞれが独立に学習する
  - 外因性報酬 $r^e$ に基づいて $Q^e_t$ を計算するネットワーク
  - 内因性報酬 $r^i$ に基づいて $Q^i_t$ を計算するネットワーク
    - 分割することで，R2D2のような一部のゲームでスコアが低下することを防ぐ
  - 2つのQ値の合算
    - $Q_t = h(h^{-1}(Q^e_t) + \beta_j h^{-1}(Q^i_t))$
    - $h$, $h_{-1}$ はベルマン作用素 (Bellman operator)
- 学習プロセス
  - 256個のActorを用いる
  - 各Actorで独立したメタコントローラー
  - 32種類の $\beta$ と $\gamma$ の組み合わせを管理
    - つまり32本の腕
  - $\epsilon$ UCB-グリーディ探索
    - $\epsilon$ UCB(低い確率)に従ってランダムな腕が選択される
    - Actorごとにこの値は初期化される
- 結果
  - 5パーセンタイルスコアで人を上回る($116.67\%$)
  - 平均値や中央値でもR2D2とほぼ同じスコア
  - 全てのゲームで人を上回る
  - 学習に使ったデータ量
    - モンテズマの逆襲: 100億フレーム程度で人超え
    - Skiing: 780億フレーム(41年分?)
- 今後の課題
  - データ量が多すぎる
  - MuZero のような先読みによる改善が期待できる
  - まだ平均的な人のスコアを超えただけ

まとめ
- 分散処理によるデータ生成
- 記憶力の強化
