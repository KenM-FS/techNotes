# ゲームから学ぶAI 環境シミュレータx深層強化学習で広がる世界
西田圭介. 株式会社技術評論社. 2022.08.06.

Finish reading at: YYYY.MM.DD

###### Purpose
シミュレーションと深層強化学習について学ぶ．
タスクに対して適切なシミュレーション環境と強化学習手法を提案できるようになる．

## Notes
- DeepMindが発表した論文を中心に紹介

歴史
- 2016年: AlphaGo (囲碁)
- 2019年: AlphaStar (StarCraft II)
  - 多くの先行研究のもとに成り立っている
    - 2013年のDQN

### 1章 ゲームAIの歴史
世界最初の電子計算機: ENIAC (1945年)
- 数年後にチェスをプレイするコンピュータプログラムの提案
- 1951年に実際に動くものが開発されている(Turochamp)

Deep Blue
- 1997年にGarry Kasparov(チェスの世界チャンピオン)に勝利
- IBMのスーパーコンピュータがベース
  - 2つのタワー型コンピュータ
  - 30個のプロセッサ
  - 480個のチェス専用チップ
- 2億手/sec
- オープニングを持つ
  - ゲーム序盤の最善とされる定石のデータベース

その後
- 2013 Ponanzaが将棋でプロ棋士に平手で勝利
- 2016 AlphaGoが囲碁で世界チャンピオンに勝利
- 2018 AlphaZeroがチェス，将棋，囲碁で最強のAIに

以前は探索と評価によって手を決定していた
- 効率の良い探索
- 質のいい評価関数
  - 手作り

将棋
- 2006 Bonanza
  - WCSC(world computer shogi championship)優勝
  - 評価関数を機械学習によって決定
    - 既存の棋譜データから評価関数を学習
  - 大量のデータから学習する時代へ
- 2013 Ponanza
  - 四段を破りプロの棋士レベルに到達
  - 自己対戦によって改善
    - 大量のリソースを使って事前に計算する
  - 2015, 2016 にWCSC優勝
- 2017 elmo
  - WCSCでPonanzaを破る
  - 学習方法に工夫
- 2018 AlphaZero
  - elmoに大差で勝利
  - 人間には太刀打ちできない領域へ

囲碁
- チェスや将棋と比較して手数がけた違いに多い
- 人間の直観的な絞り込みを再現する必要がある
- 2016 AlphaG0
  - 世界チャンピオンに勝利
  - 評価関数だけでなく，探索にも機械学習を取り入れる
- 自己対戦による学習
  - AlphaGo Zero では自己対戦に3500万ドルが投入された
- 2018 AlphaZero
  - チェス，将棋，囲碁で世界最強のAIに
  - ゼロからの機械学習でマスターに
    - 人間が積み重ねた知識に全く頼らない
    - 新しい戦法の発見

汎用ビデオゲームプレイ
- 2013 DQN
  - Atari 2600
  - 自らゲームのやり方を学習するAI
  - 深層学習と強化学習を合わせた深層強化学習の先駆け
- 2015 TensorFlow: 深層強化学習ライブラリのリリース

なぜAtari 2600なのか?
- 古典ゲームであり，ゲームに必要なCPUやメモリが少ない
  - 研究目的での利用に向いている
- 実際はStellaを使用する
  - オープンソースエミュレータ
  - ALE(Arcade Learning Environment)
    - Stella上でゲームAIを動かすためのライブラリ
      - 操作やスコア表示などゲームAI開発に便利な機能を備える
    - 2012年リリース
    - 50以上のゲームに対応
  - これらによって広く利用されるようになった
- ゲームROMは著作権が(70年は)切れていない
  - グレーなので取り扱いには注意が必要

強化学習のマップ
- https://image.slidesharecdn.com/rlssimai1-200914094519/75/-7-2048.jpg?cb=1665847238

Atari-57ベンチマーク
- Atari 2600 から選択された57のゲーム
- ベンチマークに用いられる
- 長期的な計画が必要となるゲームが苦手
  - 謎解きなど
  - プランニング(planning, 行動計画)の領域
- MuZero
  - 独自の未来予想の仕組みを導入
  - チェス，将棋，Atari-57まで対応できるように

GVGAI (General video game AI)
- 2014年に開始されたゲームAIの学習環境
- 著作権に保護されたゲームに頼らない学習環境の提供
- 200以上のミニゲームが公開
- フォワードモデル
  - ある行動を選択した際の結果をシミュレート

深層強化学習とゲーム環境
- Malmo
  - MinecraftをプレイするAIを開発するための環境
  - 2016年3月にMicrosoftから発表
  - APIによって操作
  - Malmoを使ったAIコンペティション
- OpenAI Gym
  - 現在は Gymnasium にその役割を引き継いでる
  - 2016年4月に発表されたPythonライブラリ
  - 標準化された環境による盛んな知見の共有と結果の比較
- DeepMind Lab
  - 2016年12月に発表
  - 3Dゲーム環境
    - Quake III Arena engine (a.k.a. id Tech3) がベース

StarCraft II
- 2017年8月，DeepMind と Blizzard Entertainment が共同で発表
  - StartCraft II をAI研究の環境としてオープンにする
- 囲碁に代わる新しい研究目標を提供
- スコアを最大化するのではなく，長期的な戦略で人間と競う
- 特徴
  - RTSであり，ターン性ではない
  - 部分観測であり，かつ自発的に観測しに行かなければいけない
- StarCraft (1998-)の時からAIでプレイする方法が研究されてきた
  - 2010年からStarCraft AI Competitionが毎年開催
  - Brood War API(BWAPI)によるエージェント(bot)を作成
    - いわゆるスクリプトによる操作
  - ミクロで勝ててもマクロで勝てない
- SC2LE (StarCraft II learning environment)
  - 公式が提供するAPI
  - スクリプトではなく，深層学習によるAIを組み込めるように
- AlphaStar
  - 2018年12月, TLOやMaNaに勝利
  - オンラインリーグの上位0.2%

Dota 2
- 2017年8月にDota 2をプレイするAIがOpenAIから発表
  - 1v1においてはトッププレイヤーにすでに勝利
- 2018年6月 OpenAI Five がアマチュアチームに勝利
- 2019年4月 世界チャンピオンに勝利
- 操作はヒーローごとに独立しており，それぞれが協調して動いている
- 必要とする資源は計り知れない
  - 10か月かけて4万5千年分の自己対戦を行った
- Bot Scripting によるAPIを用いて開発
  - ゲームとしての設計であるため，クラウド環境での実行など課題があった

マルチエージェントゲーム
- Capture the Flag, from DeepMind in 2018/7
- Neural MMO, from OpenAI in 2019/3
- Google Research Football, from Goole in 2019/6
- Hide-and-Seek, from OpenAI in 2019/9
- DeepMind Lab2D, from DeepMind in 2020/12
  - テキストファイルとLuaスクリプトだけで新しいマルチエージェント環境を構築できる
- XLand, from DeepMind in 2021/7

### 2章 機械学習の基礎知識
だいたい知ってるので省略

機械学習と脳の関係
- 大脳新皮質(cerebral neocortex) - 教師なし学習
  - 受け取った情報を抽象化し，過去の経験と照らし合わせて記憶する
  - 何度も行うことで記憶が定着し，既知のものと区別できるようになる(クラスタリング)
- 小脳(cerebellum) - 教師あり学習
  - 体で覚える役割
  - 大脳による行動計画を教師データとし，小脳が行動する
  - 小脳が覚えると無意識に動くようになる
- 大脳基底核(basal ganglia) - 強化学習
  - 欲を満たすように学習する
- 誤差逆伝搬法のような仕組みは脳にはない(といわれている)
  - ニューラルネットワークは，あくまでも脳が行っている情報の抽象化を真似している

ResNet (residual network, 残差ネットワーク)
- ゲームAIでも画面の認識によく使用される
- CNNの一定以上の層を重ねたときに精度が下がる問題を解決
- 中間層では残差を計算
- スキップ接続を通して入力データが上層へ

通時的誤差逆伝搬法 (backpropagation-through-time; BPTT)
- RNNに用いられる学習法
- 時間的に連続した一連の入出力をまとめて学習することで高速化する
  - 一つ一つやらない

LTSM (long short-term memory, 長・短期記憶)
- RNNの勾配消失問題を解決
- スキップ接続により解決
- 重ねて Deep LSTM (or Stacked LSTM)に

Seq2Seq
- エンコーダー(LSTM)とデコーダー(LSTM)を用意する
- エンコーダーは入力値によって隠れ状態を更新，その値をデコーダーに与える
- デコーダーは隠れ状態を受け取り値を出力

Attention
- 2つの文章の関係を数値化する

ポインターネットワーク
- 学習になかった未知のデータにも対応できるように
  - Seq2Seqの欠点
- 入力データを参照するポインターを学習する
- デコーダーにはAttentionが用いられる

Transformer
- AttentionとLSTMは組み合わされて使われてきた
- LSTMは必要ではなく，Attentionが重要だという研究が Attention is All You Need
- Attentionに特化した機構として Transformer が提案
- Attentionを計算するエンコーダーとデコーダー
- 同時処理による並列化
- Self-Attention
  - 文章内のすべての単語同士の関係を計算することで，文脈を学習

LSTMはTransformerの登場で下火になっている
- 自然言語処理だけでなく，音声認識や画像認識でも使われるように
- 時系列データを学習するための汎用的な技術として適材適所で使用されている
