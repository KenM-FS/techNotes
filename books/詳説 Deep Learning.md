# 詳説 Deep Learning 
Josh Patterson, Adam Gibson, 本橋 和貴(監訳), 牧野 聡(訳), 新郷 美紀(訳), 2019.8.8, O'REILLY Japan

### 目的
MLP, CNN, RNN, LSTM-RNN, TB-LSTMについて知り，論文を読み進める．

---
## 1章 機械学習の概要

### 用語定義
**データマイニング**: データから情報を取り出すこと(誤用多数)

### 課題の定義
機械学習を適用する際に必要な問いかけ
- 情報あるいはモデルを抽出しようとしている，入力のデータは何か
- 与えられたデータに対して，どのようなモデルが最も最適か
- そのモデルに基づいて，新しいデータからどのような答えを引き出したいか

### 数学の基礎知識(線形代数)
データ操作に用いる数学的知識

**テンソル(tensor)**: 多次元の配列．階数で考えると，0はスカラー，1はベクトル，2は行列，3以上で狭義のテンソルだとみなせる．(要は次元数で呼び方が変わる的な話)

**超平面(hyperplane)**: 対象の空間よりも次元が1つ下の部分空間．n次元を超平面により2分割できる．

**ドット積(dot product)**: スカラー積 or 内積．**正規化(normalization)** では内積が2つのベクトルがどの程度似ているかの指標になる．2つの正規化されたベクトルの内積を  **コサイン類似度(cosine similarity)** と呼ぶ．

**要素ごとの積(アマダール積, element-wize product)**: 2つのベクトルの中で対応する位置にある値の積

**外積(テンソル積)**

逆行列化(**matrix inversion**): `x = A^{-1}b`でパラメータベクトルを求める(この式が成り立つような`x`が存在するとき，この線形系を**無矛盾(consistent)** という)

**行列分解(matrix decomposition)**: 詳しくは紙ノートに

連立1次方程式の一般的な解法
- **直接法(direct methods)**: アルゴリズム的に計算の回数が一定であることが判明している．すべての訓練データ(`A と b`)を1台のコンピュータ上に読み込める場合に効果的．
    - **ガウスの消去法(Gaussian Elimination)**
    - **正規方程式(Normal Equation)**
- **反復法(iterative methods)**: 複数回の近似と終了条件を通じて`x`を導く．個々のデータをディスクから順に読み込んで処理を行うので，メモリよりも大きなデータでもモデル化できる．データセットがクラスター上に分散しているスケールアウトのシナリオでも有効性が示されている．
    - **確率的勾配降下法(Stochastic Gradient Descent, SDG)**
    - **共役勾配法(Conjugate Gradient Methods)**
    - **交互最小2乗法(Alternating Least Squares)**

### 数学の基礎知識(統計)
NL, DLの分野で関わる数学知識

統計学
- 用語: 確率，分布，尤度

(オッズは`(事象の場合の数)/(その事象以外の場合の数)`，確率とは分母が違う)

記述統計学(descriptive statistics)
- 用語: ヒストグラム，箱ひげ図

推定統計学(inferential statistics)
- 用語: 散布図，平均，標準偏差，相関係数，p値，信頼区間

確率と推定統計の関係
- 確率は母集団から標本の推論(演繹)
- 推定統計は標本から母集団の推論

**ベイズ統計(Bayesianism)** と**頻度論(Frequentism)**
- 頻度論: 測定の繰り返しの中では確率だけに意味がある．変数の推定のために何度も実験と測定を行う必要がある．
- ベイズ統計: 何かしらの仮説の確からしさという意味での確率．変数についての信念(分布)を扱う．

### 機械学習の仕組み
レコメンド: by 協調フィルタリング(Collaboration Filtering)

**ハイパーパラメータ**: 学習の効率を挙げるためのパラメーター

**凸最適化(convex optimization)**: 凸関数の誤差関数を扱う．コストが0となる地点が存在し，その両側ではコストが急激に上がる．

**最尤推定(MLE, maximum likehood estimation)**: 横軸-パラメータ，縦軸-尤度の放物線に沿って処理．与えられたデータの尤度が最大になるようなパラメーターの組を発見する．

**勾配降下法(gradient descent)**: **停留点(stationary point)** の位置を求める．大域的な最小値(global minimum)と極小値(local minimum)．勾配を求めてパラメータベクトルを更新する際に全ての訓練データに対する全体的な損失を計算する．

**確率的勾配降下法(Stochastic Gradient Descen)**: 訓練データごとに勾配を計算して，パラメータベクトルを更新する．学習の高速化・並列化が可能．パッチ全体を使った勾配降下法を近似したものが確率的勾配降下法

ミニバッチサイズの確率勾配降下法の訓練: 訓練データのいくつかを使って勾配を計算する．訓練のインスタンスが1つの場合よりも性能が高い．収束しやすい．

**準ニュートン法による最適化**: 線形探索を伴う反復的なアルゴリズム．

ヤコビアンとヘッシアン: 紙のノートに

生成モデル(generative model): データの生成方法を把握したうえで特定の種類の応答あるいは出力を行う．データを作り出すことができる．結合確率分布`p(x,y)`を学習．データ内のわかりにくい関係をとらえる確率的グラフィカルモデルとして用意される．

識別モデル(discriminative model): 単に入力された信号の分類を返す．クラス間の境界を正しくモデル化することに特化．一般的．条件付き確率分布`p(y|x)`を学習．

### モデルの評価
