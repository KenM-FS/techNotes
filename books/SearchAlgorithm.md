# ゲームで学ぶ 探索アルゴリズム実践入門 ～木探索とメタヒューリスティクス
青木 栄太. 株式会社技術評論社, 2023.03.25.

Finish reading at:

###### Purpose
強化学習を学ぶ前に基本的な探索について学んでおくことで，適切な技術を使えるようになりたい．

---
## 第1章
二人零和有限確定完全情報ゲーム
- 零和: プレイヤー間の利害が完全に対立する．一方のプレイヤーが得をすると，他方のプレイヤーは同量の損をする．
  - 将棋は零和じゃないという意見
- 有限: ゲームが必ず有限の手番で終わる
  - 千日手など厳密には有限じゃない場合も

「文脈がある」: プレイヤーの行動と共に状況が変化する

ゲームの種別とおすすめのアルゴリズム
- プレイヤー人数: 一人
  - 文脈がある
    - 多様化に自信あり -> ビームサーチ
    - 多様化に自信なし -> Chokudaiサーチ
  - 文脈がない -> 焼きなまし法
- プレイヤー人数: 二人
  - 手番: 交互
    - 全探索可能 -> AplhaBeta
    - 全探索不可能
      - 盤面評価可能 -> AlphaBeta, Thunderサーチ
      - 盤面評価不可能 -> MCTS
  - 手番: 同時 -> DUCT

## 第3章
貪欲法
- 1ターン後の盤面で一番評価が高い盤面に進める手を選択する

ビームサーチ
- 複数ターンの探索をする際に，範囲を絞って現実的な計算量で探索をする
- 手順
  - 1ターン後の盤面をすべて列挙 & スコア順にソート
  - 深さ(ターン数)と幅(盤面の数)を設定
  - スコアの高い順から幅の数だけ，次の盤面を列挙してソート
    - このとき，同じターンの中でソートする
    - スコアの合計でソートする
  - 深さの分だけ繰り返す
- $盤面列挙の数の最大 = 親ノードの数 \times 1盤面当たりの最大合法手数$
- 計算時間によって深さを固定する(計算時間を固定する)
- 深さだけあっても，多様性がない
  - 同じ場面から遷移したものがソートで上位に

Chokudaiサーチ
- 幅の狭いビームサーチを何度も繰り返して多様性を確保
  - ビームのたびに，探索された盤面を追加し，すでに選択された盤面を除いてソートし深く探索する
- 一定時間内にある程度の結果が得られる
- 多様性のために期待度の低い探索を多く行う
- 極限までチューニングされたビームサーチのほうが高性能

## 第4章
文脈がない: ゲーム中に1度しかプレイヤーが介入せず，順序性がない特性を示す

オート数字集め迷路
- 数字がランダムに配置されたグリッド
- プレイヤーはエージェントの配置のみを行う
  - その際の数字はスコアに加算されない
- エージェントは4方向で一番数字の高いグリッドに移動し，その数字だけスコアを加算する
  - 同じグリッドに移動した場合は，一回のみ加算される
- 全てのエージェントが獲得したスコアの合計を最大化する

山登り法
- 局所探索法
- 近接最適性(良い解同士は似ている)があるという前提のもと，似た構造を中心に探索する手法(集中化)
- 解をランダムな近傍(少し構造を変えた状態)に遷移させ，条件でその遷移を保たせたり，元の状態に戻したりする
  - 今回はスコアが高い場合に遷移を保つ

焼きなまし法 (Simulated Annealing)
- 山登り法の局所最適解に陥ってしまう欠点を改善
- スコアが改善しない時も遷移を保つ
  - スコアが改善するなら遷移を保つ
    - 遷移の確率 $\delta \leq 0$ の時 $1$
  - 改善しなくても， $\delta = Score(next) - Score(now)$ に応じて確率的に遷移を保持
    - 遷移の確率 $\delta < 0$ の時 $e^{\frac{\delta}{t}}$
    - $t$ は温度パラメータ．初期は高く，徐々に下げていく
- 十分な評価回数が確保できない場合は，スコアが確実に改善する山登り法のほうが良い場合もある

メタヒューリスティクス: 数理最適化を解くための方針の1つ
- 集中化(過去の良い解に似た解を探索)と多様化(過去の解とは異なる解を探索)のバランスを撮りながら探索と評価
- 遺伝的アルゴリズムやタブーサーチなどある

## 第5章
交互着手数字集め迷路

MiniMax法
- 外部要因(相手の手)を考慮した探索
- 2手先までのリーフノードを評価する(自分の手と相手の手)
  - $評価値 = Aのスコア - Bのスコア$
- 相手はこちらの評価が最小化するような手を選択するはず
  - i.e. 相手にとって評価が最大化する手
- 自分の手からの子ノードごとにMin選択を行い，選択されたノードの評価値で一番高い手を選択する(Max選択)
- ゲーム終了まで探索できるなら最善手を選択できる
- 途中までしか探索できないなら，評価値が重要になる
  - ゲームによって盤面の評価を変えると良い
- NegaMax: MiniMax法を手番プレイヤー視点で評価して実装するテクニック

AlphaBeta法
- MiniMax法の派生．計算時間が膨大になる弱点を改善した完全上位互換
- $\beta$ カット
  - min選択を踏まえて，max選択の探索を打ち切る
    - min選択の評価値$0$がある時に，一つ深いmax選択で評価値$0$以上が出たときにそれ以上の探索を打ち切る
- $\alpha$ カット
  - max選択を踏まえて，min選択の探索を打ち切る
- NegaAlpha: 常に手番プレイヤー視点で探索を行い，$\beta$ カットのみを考慮する

反復深化 (Iterative Deepening)
- 時間が許す限り深くしていくAlphaBeta法

浅い探索での解が深い探索での解となる可能性が高いという性質
- 浅い探索結果でノードを評価の高い順に並べ替えてから深い探索を行うことで $\beta$ カットを早期に発生させるテクニック

原始モンテカルロ法
- 盤面評価なしで利用できる探索手法
- プレイアウト: ゲーム終了までの1シミュレーションの単位
  - ランダムな行動での1シミュレーションのことを指すことが多い(正確にはランダムプレイアウトと呼ぶ)
- 1プレイアウトごとに，1手目の行動選択に対して累計価値($w$)と試行回数($n$)を記録する
  - e.g. 勝ち1, 引き分け0.5, 負け0
- $w/n$ (勝率)が一番高い手を選択する
- 1手目はランダムにしない
- プレイアウト数に対して勝率が大きく上がるわけではない
  - 都合の良い手も悪い手も同じ確率で選択するため

モンテカルロ法とラスベガス法
- モンテカルロ法: 必ず正しい解が得られるとは限らない代わりに時間内に終わることが保証される乱択アルゴリズム
- ラスベガス法: 計算時間が未知数だけど解が必ず正しくなる乱択アルゴリズム

MCTS (モンテカルロ木探索)
- UCB1 (Upper Confidence Bound 1)
$$
USB1 = \frac{w}{n} + C\sqrt{\frac{2\ln(t)}{n}}
$$
  - $\ln(t)=\log_e(t)$, $t=全ノードのnの総和$, $C$ は重み
  - $n$ が小さいほどバイアスが大きく， $n$ が大きいほどバイアスが小さくなる
  - 勝率とバイアスのバランスの良い探索を実現
  - UCB1 の値が高いノードを選択してプレイアウトを行う
- 2手目以降の探索
  - 試行回数が閾値を超えたノードについてはもう1手先のノード(リーフノード)を追加する
  - リーフノードの価値と試行回数はルートノードまでさかのぼって親ノードにも追加される
- 試行回数が増えるほど累積価値が洗練される
- 最終的には試行回数が最も多い行動を選択する
  - 勝率の高いノードが多く選択されている

Thunderサーチ
- MCTSのプレイアウトを盤面評価とし，探索量が少なくてもある程度信頼できる解を出す探索手法
- 筆者(青木栄太)が考案
- あらかじめ盤面評価を決めておく(どの程度自分が勝てそうかどうか)
  - 0-1の範囲に収める
  - $1 - 自分の評価 = 相手の評価$ になるようにする
-
