# ゼロから作るDeep Learning 4 -強化学習編
齊藤 康毅. 株式会社オライリー・ジャパン, 2022.04.04

Finish reading at:

###### Purpose
Learn about Reinforcement Learning for my research

## Notes
GitHub: https://github.com/oreilly-japan/deep-learning-from-scratch-4

Workbook: https://koki0702.github.io/dezero-p100/

### 1章 バンディット問題
#### 機械学習の分類と強化学習
教師あり学習，教師なし学習，強化学習とその違いについて

#### バンディット問題
Bandit Problem
- "bandit" はスロットマシンの別称 (one-armed bandit(一本腕の盗賊)と揶揄されていた)
- 正確には"多腕バンディット問題(multi-armed bandit problem)"という

概要
- 一本のレバーがあるスロットマシンが複数台ある
- スロットマシンごとに特性が異なる(当たりの多い少ない)
- 特性の情報は与えられない
- 決められた回数プレイし，得られるコインをできるだけ多くする

バンディット問題を強化学習の用語へ変換
- スロットマシン -> 環境(environment)
- プレイヤー -> エージェント(agent)
- スロットマシンの選択とプレイ -> 行動(action)
- コインの受け取り -> 報酬(reward)

バンディット問題を解くアルゴリズムを考える
##### 「良い」スロットマシンの選択
確率を考える(各スロットマシンの具体的な確率は与えられない)(確率分布表で表現される)
- 期待値(expectation value)
    - バンディット問題では「価値(value)」と呼ぶ
    - 行動に対して得られる報酬の期待値は「行動価値(action value)」と呼ぶ

報酬を$R$で表す(Reward)．$R$は${0,1,5,10}$のいずれかの値をとり，各値に取りやすさとして確率が決まっている
- 確率変数(random value): 取る値が確率的に決まる変数のこと
- $t$回目に得られる報酬は$R_t$

エージェントの行う行動を$A$で表す(Agent)．変数$A$はスロットマシン(aとbの2つなら${a,b}$)の値をとる．

確率変数に定義される期待値は$\mathbb{E}$で表す(Expectation)．
- 報酬$R$の期待値は$\mathbb{E}[R]$
- 行動Aを選んだ場合の報酬の期待値は$\mathbb{E}[R|A]$ (条件付確率)
    - aのスロットを選択した場合，$\mathbb{E}[R|A=a]$ or $\mathbb{E}[R|a]

報酬の期待値(行動価値)は$Q$,$q$で表す(Quantity)．
- 行動$A$に対する価値は$q(A) = \mathbb{E}[R|A]$
- 小文字の場合は，真の行動価値を表し，大文字の場合は推定値を表す．一般的には真の行動価値は知りえないので，それを推定する．

##### バンディットアルゴリズム
エージェントは行動の結果から自分の選択がどれだけ良かったのか(悪かったのか)を推定する必要がある
- もし各スロットマシンの価値(報酬の期待値)がわかれば，プレイヤーは最も良いスロットマシンを選ぶことができる
- しかし，各スロットマシンの価値はプレイヤーにはわからない
- よってプレイヤーには各スロットマシンの価値をできるだけ精度よく推定することが求められる

実際に得られた報酬の平均値をスロットマシンの価値の推定値とする
- 標本平均(大数の法則により真の値に近づく)

##### 平均値を求める実装
ランダムな数の生成をn回行いそれらを足し合わせ，nで割る
- nが増加すると計算量(和算)もメモリも増加する問題点がある

式変換により，
$ Q_n = (1 - 1/n)Q_{n-1} + \frac{1}{n}R_n$
と表すことができる．
- これまでの報酬($R_1,R_2,...,R_{n-1}$)が必要ない

さらに，
$Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})$
としたとき，
- 右辺第二項が，$Q_n$の$Q_{n-1}$に対する増加量である
- $1/n$は学習率としての役割がある

```py
Q += (reward - Q) / n
```

##### 戦略
greedyな行動: これまでの結果をもとに，一番良いスロットマシンを選ぶこと
- 活用(exploitation)とも呼ぶ
- 不確かさが原因に信頼の低い選択をし続けてしまう問題点

探索(exploration)により，各スロットマシンの価値をより高精度に推定する

活用と探索はトレードオフの関係にある．どうバランスをとるか
- ε-greedy法: 最も基本的で応用の利くアルゴリズム
    - εの確率(e.g. ε=0.1)で，探索を行い，それ以外は活用を行う

#### バンディットアルゴリズムの実装
- スロットマシンが返すコインの枚数は0枚 or 1枚 (負けor勝ち)
- スロットマシンに設定されている確率は勝つ確率
- 10台のスロットマシン

スロットマシンの実装(Banditクラス)
- initのnp.random.rand(arms=10)で，10個の0.0以上1.0未満の値を取る(各スロットマシンの勝率)
- playクラス
    - randして，該当するarmの勝率より小さければreturn1, else return 0

エージェントの実装
- 配列Qn: 対応するスロットマシンの推定値を格納
- $epsilon$はランダムに行動する確率
- $action_size$は行動の数(10台のスロットマシン)
- $update$メソッドで価値の推定を更新

ランダム性により実験のたびに結果が異なる
- 実験を複数回行って平均的な結果を取る
- 実験の回数ごとに各ステップで平均をとる

εごとの変化
- 0.01, 0.1, 0.3 高いと探索しすぎる，低いと最善のマシンを見つけられない．

#### 非定常問題
バンディット問題は，定常問題(stationary problem, 報酬の確率分布が定常である)

非定常問題(non-stationary problem): プレイするたびにスロットマシンの価値(勝率)が変動する

標本平均は得られた報酬の平均によって求めることができる
- $1/n$を重みとみなすことができる
- 非定常問題において，過去のデータの重要性は時間とともに低くするべき
- $Q_n = Q_{n-1} + 1/n(R_n - Q_{n-1})$において，$1/n$を固定値$\alpha$に置き換える($0<\alpha<1$)
- 固定値にすることで，過去の報酬の重みが指数関数的にちいs買う成る．
- 式変形し，nを1つ下げ，Q_{n-1}とQ_{n-2}の関係性をみるとその様子がみれる
- 指数移動平均, exponential moving average
- 指数加重移動平均, exponential weighted moving average

Q_0が必要であり，その初期値は手動で設定するため学習に偏り(バイアス)が発生する．

#### まとめ
バンディットアルゴリズムは，複数の選択肢から最善の選択肢を選ぶ問題に使う
- 売り上げに貢献するWebデザインの選択
- 有効な薬の選択

バンディットアルゴリズムのε-greedy法以外の手法
- UCB(upper confidence bound)アルゴリズム
- 勾配バンディットアルゴリズム

平均
- 定常問題の場合は標本軽金
- 非定常問題の場合は指数移動平均

### 2章 マルコフ決定過程
エージェントの行動によって状況が変化する問題
- 非定常問題ではエージェントの行動に関係なく時間とともに変化していた

このような問題の一部はマルコフ決定過程(Markov decision process; MDP)として定式化される
- 決定過程とは，エージェントが環境と相互作用しながら行動を決定する過程
- エージェントが置かれる「状態(state)」
- MDPには時間の概念が必要(タイムステップ単位)
- 目先の報酬ではなく報酬の総和を最大化することが求められる
- 行動A_tを行い，報酬R_tを得て，次の状態であるS_{t+1}へ遷移する
    - 報酬はR_tだったり，R_{t+1}だったりする(慣例)

#### 環境とエージェントの定式化
MDPにおけるエージェントと環境のやり取りを数式によって定式化
- 状態遷移
- 報酬
- 方策

決定論的(deterministic)な状態遷移: 次の状態s'が，現在の状態sと行動aによって一意に決まる事
- $s' = f(s, a)$ <- 状態遷移関数 (state transition function)

確率的(stochastic)な状態遷移
- $p(s'|s, a)$ <- 状態遷移確率 (state transition probability)
    - 状態sで行動aを選択したときに状態s'へ遷移する確率

マルコフ性(Markov property): 次の状態が現在の状態と行動にだけ依存すること
- MDPはマルコフ性を仮定することによって，遷移状態と報酬をモデル化する
- 仮定できなければ，過去のすべての状態や行動を考慮しなければならず指数関数的な情報を処理しないといけなくなる

報酬関数(reward function)
- $r(s,a,s')$: 状態sにいるときに，行動aを行い，状態s'に遷移したときに得られる報酬
- 遷移が確率的であるなら，報酬関数は報酬の期待値を返す

エージェントの方策
- 方策(policy)
- マルコフ性により，エージェントは現在の状態だけに基づいて行動を決める
- 決定論的な方策の例
    - $a = \mu(s)$: 状態sを引数として与えると，行動aが帰ってくる
- 確率的な方策の例
    - $\pi(a|s)$: ある状態sにいるときに行動aを取る確率を表す

MDPの目標
- エージェントは方策$\pi(a|s)$によって行動し，その行動と状態遷移確率$p(s'|s,a)$によって次の状態へ遷移し，報酬関数$r(s,a,s')$に従って報酬が与えられる
    - 決定論的な方策を持つ場合は$\mu$で表すことができる
- 以上の枠組みの中で最適方策(optimal policy)を見つけることがMDPの目標となる
    - 最適方策は，収益が最大となる方策
- MDPの問題は大きく2つに分類する
    - エピソードタスクと連続タスク

エピソードタスク
- 終わりのある問題
    - e.g. 囲碁(win/lose/draw)
- 終わるとまた初期状態から始まる

連続タスク
- 終わりのない問題
    - e.g. 在庫管理問題(適切な商品の仕入れを永遠に行う)

収益(return)
- $G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ...$
    - 時刻$t$に状態$S_t$である場合に，エージェントが方策$\pi$によって行動$A_t$を行い，報酬$R_t$を得て，新しい状態$S_{t+1}$に遷移する場合の収益$G_t$
- 時間が進むにしたがって報酬が割引率(discount rate)$\gamma$によって指数関数的に減衰される
    - 連続タスクにおいて収益が無限大になることを防ぐため

状態価値関数
- エージェントと環境は確率的にふるまう可能性がある
- 収益の期待値を指標とする必要がある(状態$S_t=s$，方策が$\pi$であるときエージェントが受け取る収益の期待値)
    - $v_\pi(s) = \mathbb{E}[G_t | S_t = s, \pi]$
    - $v_\pi(s)$ 状態価値関数(state-value function)
    - 別の表現 $v_\pi(s) = \mathbb{E}_\pi [G_t | S_t=s]$
- $v_\pi$は真の状態価値関数，$V_\pi$は推定値としての状態価値関数

最適方策と最適価値関数
- すべての状態において$v_{\pi'}(s) >= v_\pi(s)$となる時のみ，方策$\pi'$が方策$\pi$よりも良い方策だと言える
- MDPでは最適方策が少なくとも1つは存在し，その最適方策は決定論的方策である
    - $a = \mu_*(s)$
- 最適状態価値関数(optimal state-value function): 最適方策における状態価値関数．$v_*$

#### MDPの例
具体的な問題設定
- 2マスのグリッドワールド(L1,L2) w/ 両端に壁
- エージェントは右or左のどちらかに移動
- エージェントがL1からL2に移動したときにリンゴを受け取り$+1$の報酬を得る
- エージェントがL2からL1に移動するとリンゴが出現
- 壁にぶつかると$-1$の報酬を得る
- 連続タスク

バックアップ線図: 有向グラフによって，状態・行動・報酬を表現したもの
- エージェントの方策が決定論的な場合，一直線になる
- 確率的な場合，無限に2分(右と左)していく

最適方策を見つける
- 決定論的方策$a = \mu(s)$
- 今回は状態が2つ，行動が2つで4通りの方策がある

状態価値関数の計算($\mu_1$)
$$
v_{\mu_1}(s=L1) = 1 + 0.9(-1) + 0.9^2(-1) + ...
= 1 - 0.9(1+0.9+0.9^2+...)
= 1 - 0.9(1-0.9)
= -8
$$
- 参考: 無限等比級数の公式 $1+r+r^2+...=1/(1-r)$
- コードではfor文によって近似する

各方策の価値関数により，方策$\mu_2$が最適方策(5.26)であることがわかる．

#### まとめ
- MDPはエージェントと環境の相互のやり取りを定式化したもの
- 環境には状態遷移確率(状態遷移関数)と報酬関数がある
- エージェントには方策がある
- 最適方策とは，すべての状態において他のどの方策よりも状態価値関数の値が大きいまたは等しい方策

### 3章 ベルマン方程式
確率的なふるまいをするMDPの場合は手計算による状態価値関数を求めることができない

このような状態でも状態価値関数を求めるベルマン方程式(Bellman equation)

#### ベルマン方程式の導出
前知識
- 確率の期待値について
- 条件付確率について
- 期待値の表現

収益(リターン)
$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ...$

次のように変換できる
$G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...$

2つの式より
$G_t = R_t + \gamma G_{t+1}$...(3.3)
収益である$G_t$と$G_{t+1}$の関係性

状態価値関数(収益の期待値)
$v_\pi (s) = \mathcal{E}_\pi [G_t | S_t = s]$

式(3.3)を代入すると，
$$
v_\pi (s) = \mathcal{E}_\pi [R_t + \gamma G_{t+1} | S_t = s]
= \mathcal{E}_\pi [R_t | S_t = s] + \gamma \mathcal{E}_\pi [G_{t+1} | S_t = s]
$$
- 最後の展開は，期待値の線形性により成り立つ
- 確率的方策，確率的状態遷移を想定する(決定論的もカバーできる)

まず，$\mathcal{E}_\pi [R_t | S_t = s]$を計算する．
- 行動$a$の確率と，それらの行動について確率的な遷移$s'$がある

よって
$$
\mathcal{E}_\pi [R_t | S_t = s] = \sum_a \sum_s' \pi(a|s) p(s'|s,a) r(s,a,s')
$$
- エージェントの行動の確率$\pi$
- 遷移先となる状態の確率$p$
- 報酬関数$r$
- $\sum_a,s'$とまとめて書く

次に第2項($\mathcal{E}_\pi [G_{t+1} | S_t = s]$)について
- $v_\pi (s) = \mathcal{E}_\pi [G_{t+1} | S_{t+1} = s]$を考える
- 求めたい式の，状態の時刻を1つ進める
    - 具体例: 状態$S_t=s$で，$0.2$の確率で行動$a_1$を選び，$0.6$の確率で$s_1$に遷移する
- すべての候補について計算を行い和を求める
- 現在の状態から遷移する可能性のある遷移先での収益の期待値(期待値の期待値?)
$$
\mathcal{E}_\pi [G_{t+1} | S_t = s] = \sum_{a,s'} \pi(a|s) p(s'|s,a) \mathcal{E}_\pi [G_{t+1} | S_{t+1} = s']
= \sum_{a,s'} \pi(a|s) p(s'|s,a) v_\pi(s')
$$

以上より，ベルマン方程式
$$
v_\pi (s) = \sum_{a,s'} \pi(a|s) p(s'|s,a) {r(s,a,s') + \gamma v_\pi(s')}
$$
- 状態$s$の価値関数とその次に取りえる状態$s'$の価値関数との関係性を示した方程式
- 全ての状態$s$とすべての方策$\pi$について成り立つ

#### ベルマン方程式の例
2マスのグリッドワールドでの実演
- 今回はエージェントは50%の確率で右，50%の確率で左へ移動する
- $v_\pi (L1)$は状態がL1でランダムな方策$\pi$に従って行動したときに得られる収益の期待値
    - 無限に続く報酬の総和

ベルマン方程式を使って$v_\pi(L1)$を表現する
$$
v_\pi(s) = \sum_{a,s'} \pi(a|s)p(s'|s,a){r(s,a,s') + \gamma v_\pi (s')}
= \sum_a \pi(a|s) \sum_s' p(s'|s,a) {r(s,a,s') + \gamma v_\pi(s')}
$$
- 総和を分離
- 決定論的に遷移

状態遷移は確率$p$ではなく関数$f$によって決定する
- if $s' = f(s,a)$, $p(s'|s,a)=1$
- if $s' \neq f(s,a)$, $p(s'|s,a)=0$

よって簡略化出来る
$$
s' = f(s,a) として，
v_\pi(s) = \sum_a \pi(a|s) {r(s,a,s') + \gamma v_\pi(s')}
$$

今回の問題をあてはめる(割引率は0.9)
- 0.5の確率で行動Leftを選択し，状態L1へ遷移する．報酬は-1(壁)
$0.5{-1 + 0.9v_\pi(L1)}$

- 0.5の確率で行動Rightを選択肢，状態L2へ遷移する．報酬は1
$0.5{1 + 0.9v_\pi(L2)}$

以上より，状態L1におけるベルマン方程式は
$$
v_\pi(L1) = 0.5{-1 + 0.9v_\pi(L1)} + 0.5{1 + 0.9v_\pi(L2)}
-0.55v_\pi(L1) + 0.45v_\pi(L2) = 0
$$

同様に状態L2におけるベルマン方程式を求める
- 状態L1への遷移で報酬0(壁)
- 状態L2への遷移で報酬-1
$$
v_\pi(L2) = 0.5{0 + 0.9v_\pi(L1)} + 0.5{-1 + 0.9v_\pi(L2)}
0.45v_\pi(L1) - 0.55v_\pi(L2) = 0.5
$$

2つのベルマン方程式で連立方程式ができる
$$
v_\pi(L1) = -2.25
v_\pi(L2) = -2.75
$$
- ランダムな方策における状態価値関数
- 状態L1にいるときに将来にわたって-2.25の収益が期待される
- 状態L2にいるときに将来にわたって-2.75の収益が期待される

ベルマン方程式の意義
- 無限に続く計算を有限の連立方程式に変換することができる
- ランダムな振る舞いがあっても状態価値関数を求めることができる

#### 行動価値関数とベルマン方程式
行動価値関数 (action-value function)
- 「価値関数」とあった場合は，状態価値関数を表す
- 行動価値関数はQ関数(Q-function)と慣例的に表記する

復習: 状態価値関数
- $v_\pi(s) = \mathcal{E}_\pi[G_t | S_t = s]$
- 状態がsであること，方策が\piであること

上の2つの条件に「行動a」を追加するのが行動価値関数(Q関数)
$q_\pi(s,a) = \mathcal{E}[G_t | S_t = s, A_t = a]$
- 時刻tのときに状態sで行動aを取り，時刻t+1以降では方策piに従った行動をとる．
その時に得られる収益の期待値が$q_\pi(s,a)$
- 行動aは方策piとは関係ない．行動aは自由に決めて，その後に方策piに従って行動する

Q関数の行動aを方策piにしたがって選ぶならば，Q関数と状態価値関数は同じになる
- この場合の収益の期待値は，Q関数の重み付き和として求めることができる
    - $\sum_{a={行動の候補}} \pi(a|s)q_\pi(s,a)$
- $v_\pi(s) = \sum_a \pi(a|s)q_\pi(s,a)$

行動価値関数を使ったベルマン方程式(導出)
- Q関数を展開
$$
q_\pi (s,a) = \mathcal{E}_\pi [G_t | S_t = s, A_t = a]
= \mathcal{E}_\pi [R_t + \gamma G_{t+1} | S_t = s, A_t = a]
$$
- 状態$s$，行動$a$は決まっている
- 次の状態$s'$へは確率$p(s'|s, a)$で遷移する
- 報酬が$r(s,a,s')$で与えられる

さらなる展開
$$
q_\pi (s,a) = \mathcal{E}_\pi [R_t + \gamma G_{t+1} | S_t = s, A_t = a]
    = \mathcal{E}_\pi [R_t | S_t = s, A_t = a] + \gamma \mathcal{E}_\pi [G_{t+1} | S_t = s, A_t = a]
    = \sum_s' p(s'|s,a) r(s,a,s') + \gamma \sum_s' p(s'|s,a) \mathcal{E}_\pi [G_{t+1}|S_{t+1} = s']
    = \sum_s' p(s'|s,a) {r(s,a,s') + \gamma \mathcal{E}_\pi[G_{t+1}|S_{t+1}=s']}
    = \sum_s' p(s'|s,a) {r(s,a,s') + \gamma v_\pi (s')}
$$

状態価値関数$v_\pi(s')$を用いて
$$
q_\pi(s,a) = \sum_s' p(s'|s,a){r(s,a,s') + \gamma \sum_a' \pi(a'|s')q_\pi(s',a')}
$$
- Q関数を用いたベルマン方程式
- a'はt+1の行動

#### ベルマン最適方程式
- ベルマン方程式はある方策$\pi$に対して成り立つ方程式
- 最終的に求めたいのは最適方策(すべての状態において状態価値関数が最大となる方策)
- ベルマン最適方程式(Bellman optimality equation): 最適方策に関して成り立つ方程式

最適方策を$\pi_*(a|s)$としたときに次のベルマン方程式が成り立つ
$v_*(s) = \sum_a \pi_*(a|s) \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_*(s')}$
- $v_*(s)$は最適方策の価値関数
- 行動$a$について，最適方策$\pi_*(a|s)$はどのような行動を選ぶのか?
    - 候補からただ一つを常に選ぶ決定論的な方策
    - 決定的論方策$\mu_*(s)$によって表すことができる
- 最適方策は方程式の値が最大の行動を選び，その値がそのまま価値関数の値になる
$$
v_*(s) = \max_a \sum_a \pi_*(a|s) \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_*(s')}
$$
- これはベルマン最適方程式

行動価値関数(Q関数)におけるベルマン最適方程式
- 最適行動価値関数 (optimal action-value function), $q_*$
$$
q_*(s,a) = \sum_s' p(s'|s,a){r(s,a,s') + \gamma \sum_a' \pi_*(a'|s')q_*(s',a')}

q_*(s,a) = \sum_s' p(s'|s,a){r(s,a,s') + \gamma \max_a' q_*(s',a')}
$$
- Q関数に関するベルマン最適方程式
- MDPでは決定論的な最適方策が少なくとも1つは存在する
- 複数ある場合もあるが値は同じなので同じ記号で表せる

#### ベルマン最適方程式の適用
2マスのグリッドワールドへの適応

ベルマン最適方程式より，状態遷移が決定論的な場合
$$
s' = f(s,a) として
v_*(s) = \max_a {r(s,a,s') + \gamma v_*(s')}
$$
- L1: Left->-1 Right->1
- L2: Left->0 Right->-1
- gamma = 0.9

$$
v_*(L1) = max { -1 + 0.9v_*(L1), 1 + 0.9v_*(L2)}
v_*(L2) = max { 0.9v_*(L1), -1 + 0.9v_*(L2)}
$$
連立方程式になり，その解は
- $v_*(L1) = 5.26$
- $v_*(L2) = 4.73$

仮に最適行動価値関数$q_*(s,a)$がわかっているとする．
- 状態$s$における最適な行動は
$$
\mu_*(s) = argmax_a q_*(s,a)
$$

$$
q_\pi (s,a) = \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_*(s')}
より，(3.3 行動価値関数とベルマン方程式)

\mu_*(s) = argmax_a \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_*(s)}
$$
- 最適状態価値関数$v_*(s)$を用いて最適方策$\mu_*(s)$を得ることができる
- これらの方策はgreedyである．(局所的な候補の中から最善となる行動を求める)

2マスのグリッドワールドにおける最適方策を求める
- L1の状態で，
    - Leftを取ると，$-1+0.9v_*(L1) = 3.734$
    - Rightを取ると，$1 + 0.9v_*(L2) = 5.257$
    - 最大となるほうRightを選択する．
- 同様に，L2の状態ではLeftを選択するのが最適となる

#### まとめ
- ベルマン方程式により，連立方程式が得られて，価値関数を求めることができる
- 実践的な問題では計算量が膨大になるが，この方程式は重要な基礎となる

### 4章 動的計画法
ベルマン方程式による価値関数を求める流れ
1. 状態遷移確率p，報酬関数r，方策piが与えられる
2. ベルマン方程式
3. 連立方程式の解を求める
4. 価値関数vが求まる

状態と行動のパターン数が増えて計算量が増えた場合の動的計画法(dynamic programming; DP)

強化学習が対処する2つのタスク
- 方策評価 (policy evaluation)
    - ある方策が与えられたときに，その方策の価値関数やQ関数を求めること
- 方策制御 (policy control)
    - 方策を制御して最適方策へと調整すること

復習
- 価値関数 $v_\pi(s) = \mathcal{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s]$
- 上の無限を解決するベルマン方程式 $v_\pi(s) = \sum_{a,s'} \pi(a|s)p(s'|s,a){r(s,a,s') + \gamma v_\pi(s')}$

ベルマン方程式を更新式へと変形する
$$
V_{k+1} = \sum_{a,s'} \pi(a|s)p(s'|s,a){r(s,a,s') + \gamma V_k(s')}
$$
- $V_{k+1}(s)$や$V(s)$は推測値
- 次に取りえる状態の価値関数$V_k(s')$を使って
- 今いる状態の価値関数$V_{k+1}(s)$を更新する
- 推定値$V_k(s')$を使って別の推定値$V_{k+1}(s)$を改善すること
- 上のプロセスをブートストラッピング(bootstrapping)と呼ぶ
    - ブートストラップとは，靴の後ろについている履くための輪っか．他者の助けなしに自分だけで改善するプロセスのこと

DPを使った具体的なアルゴリズム
- $V_0(s)$の初期値を設定
- 上の更新式で$V_0(s)$から$V_1(s)$へと更新する(さらに$V_2(s)$へ)
- これを繰り返してゴールへの$v_\pi(s)$へ近づいていく
- 反復方策評価 (iterative policy evaluation)
- 更新は$v_\pi(s)$に収束することが証明されている(条件有り)

DPはアルゴリズムの総称
- 問題を小さな問題に分割して求める手法
- 同じ計算を二度としない
    - トップダウン方式 (メモ化)
    - ボトムアップ方式 (上で紹介した方法はこっち)

2マスのグリッドワールドで反復方策評価アルゴリズムの流れを見る
- ランダムな方策$\pi$(0.5で左，0.5で右)
- 状態遷移は決定論的に決まる

更新式の簡略化
- $\sum$を分けて書く
- 状態遷移が決定論的
$$
s' = f(s,a)
V_{k+1}(s) = \sum_a \pi(a|s) {r(s,a,s') + \gamma V_k(s')}
$$
- $V_0(L1)=0, V_0(L2)=0$

左は推移の確率$0.5$，報酬$-1$で状態はL1 (gamma = 0.9)
- $0.5{-1 + 0.9V_0(L1)}$

右は推移の確率$0.5$，報酬$1$で状態はL2
- $0.5{1 + 0.9V_0(L2)}$

以上より，$V_1(L1)$は
$$
V_1(L1) = 0.5{-1 + 0.9V_0(L1)} + 0.5{1 + 0.9V_0(L2)}
    = 0
$$

同様に$V_1(L2)$
$$
V_1(L2) = 0.5(0 + 0.9V_0(L1)) + 0.5(-1 + 0.9V_0(L2))
    = -0.5
$$

V_0(s) L1:0, L2:0 -> V_1(s) L1:0, L2:-0.5

Pythonでの実行結果
- L1: -2.249... L2: -2.749...
- 真の価値関数は[-2.25, -2.75]であり，近似されている

閾値を設定して更新回数を自動で決める
- 更新された値の最大値が閾値を下回るまでループ

反復方策評価の別の実装方法
- 上書き方式: 更新スピードが速い

#### より大きな問題へ
3x4のグリッドワールド
- 上下左右の4方向の行動
- 灰色のマスは壁を表す
- グリッドの外側にも壁
- 壁にぶつかった場合の報酬は0
- リンゴは報酬+1，爆弾は-1，それ以外は0
- 環境の状態遷移は一意に決まる
- エピソードタスクであり，リンゴを取ったら終了

$@property$: インスタンス変数
    - 対称のメソッドをインスタンス変数として使用できる
```py
class GridWorld:
    @property
    def height(self):
        return len(self.reward_map)

    def states(self):
        for h in range(self.height):
            for w in range(self.width):
                yield(h, w)
```
```py
env = GridWorld()
print(env.height) # 3

for state in env.states():
    print(state)
# (0,0)
# (0,1)
# ...
# (2,3)
```
- 全行動と全状態がfor文によってアクセスできる
- $yield$は$return$と同じで関数の値を返すが，加えて関数の処理を一時停止させて別の処理にうつることができる．
    - 別の処理が終わったら一時停止させたところから関数の処理を再開
    - $for$文などの反復処理と組み合わせることができる

$collections.defaultdict$について
- 価値関数はディクショナリとして実装していた$V = {}$(Vの初期化)
- keyが存在していない場合にはエラーが発生してしまう
- $V = defaultdict(lambda: 0)$
    - キーが生成されたときに，デフォルト値(今回は0)が設定される
    - (存在しないキーが参照されたときに，デフォルトの値を返す)

反復方策評価の実装
- 1ステップの更新を行うメソッド$eval_onestep$関数
- 更新値が閾値を下回るまで更新を行うメソッド$policy_eval$関数
- ランダムな方策における価値関数

#### 方策反復法
ベルマン最適方程式を満たす連立方程式を解く方法
- 状態のサイズを$S$，行動のサイズを$A$としたとき，解を求めるには$A^S$のオーダーの計算量

方策の改善
- 最適方策$\mu_*(s)$
- 最適方策における状態価値関数$v_*(s)$
- 最適方策における行動価値関数$q_*(s,a)$

最適方策$\mu_*(s) = argmax_a q_*(s,a)$を，何らかの決定的方策$\mu$に置き換える
$$
\mu'(s) = argmax_a q_\mu (s,a)
    = argmax_a \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_\mu(s')}
$$
- 現状の方策 $\mu(s)$
- 方策$\mu(s)$における状態価値関数$v_\mu (s)$
- 新たな方策$\mu'(s)$
- これによって方策を更新することを，「greedy化」と呼ぶ

greedy化された方策$\mu'(s)$の性質
- もし全ての状態sにおいて\mu(s)と\mu'(s)が同じであれば，方策\mu(s)は最適方策である
- 更新されるなら，すべての状態sにおいて$v_{\mu'}(s) >= v_\mu(s)$

方策改善定理(policy improvement theorem): 方策が改善される数学的根拠

評価と改善(更新)を繰り返して方策を改善する
- 方策$\pi_0$からスタート($\pi_0(s|a)$)
- 方策$\pi_0$における価値関数を評価して$V_0$を得る(反復方策評価アルゴリズム)
- 評価関数$V_0$を使ってgreedy化を行う．
    - greedy化された方策は常に1つの行動が選ばれるので，決定論的な方策として$\mu_1$が得られる

以上が方策反復法(policy iteration)

- プランニング問題: エージェントが実際の行動を行わずに最適方策を見つける問題

#### 方策反復法の実装
方策は決定論的なので，簡略化できる
$$
s' = f(s,a)として
\mu'(s) = argmax_a {r(s,a,s') + \gamma v_\mu(s')}
$$

- 最大の価値関数を持つ行動(max_action)を取り出して，その行動が選ばれる確率が$1.0$となるように確率分布を生成する
- 初期の方策は各行動が均等に選ばれるような方策

#### 価値反復法
一般化方策反復(generalized policy iteration): 評価と改善の2つの作業を交互に繰り返すアルゴリズム
- 汎用性があり，広範囲にわたって適用できる
- 方策反復法では評価と改善を最大限に行った
- 評価と改善を最小限に行うとどうなるか

価値反復法(value iteration)

復習: 方策反復法では価値関数を繰り返し更新する

価値反復法では，1状態を更新したらすぐに改善作業を行う．
- 改善フェーズの計算と，評価フェーズの計算が同じになる．
$$
改善
\mu(s) = argmax_a \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V(s')}

評価
a = \mu(s) として
V'(s) = \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V(s')}
$$

2つの計算をまとめる
$$
V'(s) = max_a \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V(s')}

書き換え

V_{k+1}(s) = max_a \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V_k(s')}
$$
- 方策$\mu$なしで価値関数を更新している

$V_*(s)$が得られた時の最適方策$\mu_*(s)$
$$
\mu_*(s) argmax_a \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V_*(s')}
$$

価値反復法の実装
- 3x4のグリッドワールドにおいて，状態遷移は決定論的
- 簡略化
$$
s' = f(s,a)として
V'(s) = max_a {r(s,a,s') + \gamma V(s')}
$$

#### まとめ
- 動的計画法(DP)を使った最適方策を得る手法
- 方策反復法と評価反復法

### 5章 モンテカルロ法
動的計画法を使った最適価値関数と最適方策を得る方法
- 環境のモデル(状態遷移確率と報酬関数)が既知である必要がある
- DPでも計算量が膨大になる場合も

モンテカルロ法(Monte Carlo method)
- データのサンプリングを繰り返して推定する手法の総称
- 強化学習では価値関数を推定する

分布モデル(distribution model): 確率分布として表されたモデル

サンプルモデル(sample model): サンプリングによるモデル(シミュレーション?)

サンプルモデルを使って期待値を計算
- たくさんのサンプルを取って，平均値を取る(モンテカルロ法)
- 平均を取る際にはインクリメンタルな実装で効率化する

#### モンテカルロ法の基礎
サイコロの例

#### モンテカルロ法による方策評価
価値関数$v_\pi(s) = \mathcal{E}_\pi [G | s]$
- 状態$s$からスタートして得られる収益$G$
    - 収益: 割引率付きの報酬和
- 方策$\pi$に従った行動の収益の期待値

モンテカルロ法による価値関数の計算
- 方策$\pi$に従った行動による収益のサンプリング
$$
V_\pi(s) = \frac{G^{(1)} + G^{(2)} + ... + G^{(3)}}{n}
$$
- $G^{i}$は，$i$回目のエピソードで得られた収益
- モンテカルロ法はエピソードタスクでしか扱えない

全ての状態について，価値関数を求める
- 単純には，ある状態に注目して価値関数を求めること(上のプロセス)を全状態で繰り返す
    - 計算公立X
    - 任意の状態から開始できる問題のみ適応可能
- $A -(R_0)-> B -(R_1)-> C -(R_2)-> Goal$
    - $G_A = R_0 + \gamma R_1 + \gamma^2 R_2$
    - $G_B = R_1 + \gamma R_2$
    - $G_C = R_2$
- 開始状態が固定されていても，全ての状態を経由できるなら(e.g. ランダム方策)適応可能

上の方法の効率化
$$
G_A = R_0 + \gamma R_1 + \gamma^2 R_2
G_B = R_1 + \gamma R_2
G_C = R_2

G_A = R_0 \gamma G_B
G_B = R_1 \gamma G_C
G_C = R_2
$$
- $G_C$から計算する

#### モンテカルロ法の実装
3x4のグリッドワールドの問題

- 最後の状態はメモリに保存しない
    - 最後の状態(ゴール)の価値関数は常に0であるから

ランダムな方策において，モンテカルロ法での価値関数の評価はDPでのと変わらない

#### モンテカルロ法による方策制御
価値関数Vに関して評価(前回)
- 一般的には環境のモデル( p(s'|s,a), r(s,a,s') )は知りえない
- Q関数を使って方策の改善を行う
    - $Q(s,a)$が最大となる行動$a$を取り出すだけなので環境のモデルが不必要

状態遷移関数に関する評価をQ関数に関する評価に切り替える
$$
Before
V_n(s) = \frac{G^(1) + G^(2) + ... + G^(n)}{n}
V_n(s) = V_{n-1}(s) + 1/n{G^(n) - V_{n-1}(s)}

After
Q_n(s,a) = \frac{G^(1) + G^(2) + ... + G^(n)}{n}
Q_n(s,a) = Q_{n-1}(s,a) + 1/n{G^(n) - Q_{n-1}(s,a)}
$$

注意点
- 完全なgreedyではなく，ε-greedyにする
    - 完全なgreedy方式だと，ルートが固定されてしまい全ての状態を試せない
- Qの更新は「固定値α方式」で行う
    - 収益(サンプルデータ)が生成される確率分布が時間とともに変動するから
    - i.e. エピソードが進むに従い方策が更新されていくため
    - 指数移動平均が適している

状態と行動についての価値関数(Q関数)が求まる
- greedyな行動を取り出す(greedyな方策)

#### 方策オフ型と重点サンプリング
モンテカルロ法とε-greedy法による最適に近い方策(前節)
- 完全には最適ではない
- Q関数の値が最大の行動だけを取りたい
    - 探索ができなくなる
- εは一種の妥協

方策オフ型(off-policy)と方策オン型(on-policy)
- ターゲット方策(target policy): 評価と改善の対象となる方策
- 挙動方策(behaviour policy): エージェントが実際に行動を起こす際に使う方策
- 2つの方策を同じに -> 方策オン型， 分けて -> 方策オフ型

方策オフ型
- 別の方策(挙動方策)から得られた経験をもとに自分の方策(ターゲット方策)を評価・改善する
    - 挙動方策に探索をさせる
    - ターゲット方策に活用させる

挙動方策から得られたサンプルデータを使ってターゲット方策に関連する期待値を求める

重点サンプリング(importance sampling)
- ある確率分布の期待値を別の確率分布からサンプリングしたデータを使って計算する手法
- ある単純な期待値$\mathcal{E}_\pi [x] = \sum x\pi(x)$を考える
    - $x$は確率変数
    - $x$の確率は$\pi(s)$

この期待値のモンテカルロ法による近似($x$を確率分布$\pi$からサンプリングして平均をとる)
$$
sampling: x^(i) ~ \pi (i=1,2,...,n)
\mathcal{E} \simeq \frac{x^{(1)} + x^{(2)} + ... + x^{(n)}}{n}
$$
- $x^(i) ~ \pi$ 確率分布$\pi$から$i$番目のデータ$x^{(i)}$がサンプリングされたことを表す

$x$が$\pi$ではなく$b$という確率分布からサンプリングされた場合はどうなる?
$$
\mathcal{E}_\pi [x] = \sum x\pi(x)
    = \sum x \frac{b(x)}{b(x)}\pi(x)
    = \sum x \frac{\pi(x)}{b(x)}b(x)
    = \mathcal{E}_b [x \frac{\pi(x)}{b(x)}]
$$
- 確率分布$b(x)$における期待値とみなす
- $\rho(x) = \pi(x)/b(x)$とすれば重みになる
$$
sampling: x^{(i)} ~ b (i=1,2,...,n)
\mathcal{E}_\pi[x] \simeq \frac{\rho(x^{(1)})x^{(1)} + \rho(x^{(2)})x^{(2)} + ... + \rho(x^{(n)})x^{(n)}}{n}
$$

通常のモンテカルロ法による確率分布$\pi$の期待値
- 真値$2.7$
- MC: 2.78
- Var: 0.27

期待値と分散の関係
$$
Var[X] = \mathcal{E}[(X - \mathcal{E}[X])^2]
$$

重点サンプリングによる期待値
- 2.95 (まあまあ近い)
- Var: 10.63 (でかい)

分散が小さいほど少ないサンプル数で精度よく近似できる
- 重みによって，得た値が変化する
    - 確率分布$\pi$にとって代表的な値はその値が多くサンプリングされる
    - しかし，$b$ではそうとは限らない
    - 重みによってそのギャップを埋める
- これによって分散が大きくなる
- bとpiを近づければ分散は小さくなる

ターゲット方策に関する期待値を計算
- [付録A](#付録a) 方策オフ型のモンテカルロ法

#### まとめ
-

### 6章 TD法
モンテカルロ法
- 環境のモデルを使わずに方策を評価
- 評価と改善を交互に繰り返して最適方策(近い方策)を得る
- 終わらない(収益が確定しない)と価値関数の更新ができない
    - 連続タスクで使えない
    - エピソードが長くても使いづらい

行動を1つ行うたびに価値関数を更新する手法
- TD法 (temporal difference，時間差)
- 一定時間進むごとに方策の評価と改善

#### TD法による方策評価
モンテカルロ(MC)法と動的計画(DP)法を合わせた手法

- 収益の復習

TD法の特徴
- DPのようにブートストラップにより価値関数を逐次更新できる
- MC法のように，環境に関する知識を必要とせずにサンプリングされたデータを使って価値関数を更新できる

$$
v_\pi(s) = \sum_{a,s'} \pi(a|s)p(s'|s,a){r(s,a,s') + \gamma v_\pi(s')}
    = \mathcal{E}_\pi [R_t + \gamma v_\pi (S_{t+1}) | S_t = s]
$$
- $R_t + \gamma v_\pi(S_{t+1})$をサンプルデータから近似

TD法の更新式
$$
V'_\pi(S_t) = V_\pi(S_t) + \alpha{R_t + \gamma V_\pi(S_{t+1}) - V_\pi(S_t)}
$$
- $V_\pi$は価値関数の推定値
- 目的地は$R_t + \gamma V_\pi(S_{t+1})$(TDターゲット)
- nステップのTD法(上は1ステップ)

MC法とTD法の比較
- どちらも環境のモデルが未知の場合に使える
- 多くの問題はTD法が早く学習できる

$$
MC法: V'_\pi(S_t) = V_\pi(S_t) + \alpha {G_t - V_\pi(S_t)} \\
TD法: V'_\pi(S_t) = V_\pi(S_t) + \alpha {R_t + \gamma V_\pi (S_{t+1}) - V_\pi(S_t)}
$$

- MC法はG_tをターゲットとしてV_\piを更新
    - G_tはゴールにたどり着いてから得られる収益のサンプルデータ
    - 時間を積み重ねるのでばらつき(分散，バリアンス)がおおきい
    - 偏り無し
- TD法のターゲットは1ステップ先の情報を元に更新
    - 1ステップごとなので効率が期待できる
    - 分散が小さい
    - TDターゲットには推定値を含むため正確な値ではない(バイアス，偏りがある)
    - 更新ごとにバイアスは小さくなる(最終的に0になる)

TD法の実装
- MCと違い`eval`関数を毎ステップ呼ぶ
- おおよそ正しい結果

#### SARSA
TD法による方策評価(前節)．次は方策制御
- 評価と改善の繰り返しによる最適方策への近似
- 方策オン型のSARSA(さあさ，さるさ)
- 価値関数$V_\pi(s)$の評価(前節)
- 方策制御では$V_\pi(s)$ではなく$Q_\pi(s,a)$を対象に
- $V_\pi(s)$では環境モデルが必要であった
- $Q_\pi(s,a)$では
$$
\mu(s) = argmax_a Q_\pi(s,a)
$$

価値関数$V_\pi(s)$に関してのTD法の更新式
$$
V'_\pi(S_t) = V_\pi(S_t) + \alpha \{ R_t + \gamma V_\pi(S_{t+1}) - V_\pi(S_t)\}
$$

状態価値関数$V_\pi(S_t)$から行動価値関数$Q_\pi(S_t, A_t)$へと変更
- もろもろ置き換え
$$
Q'_\pi(S_t,, A_t) = Q_\pi(S_t, A_t) + \alpha \{ R_t + \gamma Q_\pi(S_{t+1}, A_{t+1}) - Q_\pi(S_{t+1}, A_{t+1})\}
$$
- これがQ関数を対象にしたTD法の更新式
- 方策オン型であり，挙動方策とターゲット方策が一致する
    - 探索ができなくなるため完全なgreedy化はできない(妥協する)
- 状態と行動がペアである
- $(S_t, A_t, R_t, S_{t+1}, A_{t+1})$が得られたとき，上式を更新できる
- 更新したら改善に入る
$$
\pi'(a|S_t) =
\left\{
    \begin{array}{}
    argmax_a Q_\pi (S_t, a) & (1-\epsilon の確率) \\
    ランダムな行動 & (\epsilon の確率)
    \end{array}
\right.
$$

- 価値関数の評価と方策の更新を繰り返す
- SARSAは，TD法で使用するデータ$(S_t, A_t, R_t, S_{t+1}, A_{t+1})$の頭文字

SARSAの実装
- 方策のランダム性から，できるだけ爆弾から遠ざかるようになる

#### 方策オフ型のSARSA
方策オフ型の注意点
- 挙動方策とターゲット方策は似たような確率分布である法が結果は安定する
    - Q関数に関して挙動方策はε-greedyに更新し，ターゲット方策はgreedyに更新する
- 2つの方策が異なるので重点サンプリング使って重み$\rho$による補正を行う

SARSAの更新式
$$
Q'_\pi(S_t, A_t) = Q_\pi(S_t, A_t) + \alpha \{ R_t + \gamma Q_t(S_{t+1}, A_{t+1}) - Q_\pi(S_t, A_t)\}
$$

方策$\pi$によって行動が選ばれる
$$
sampling: A_{t+1} \sim \pi \\
Q'_\pi(S_t, A_t) = Q_\pi(S_t, A_t) + \alpha \{R_t + \gamma Q_\pi(S_{t+1}, A_{t+1}) - Q_\pi(S_t, A_t)\}
$$
- $Q_\pi(S_t, A_t)$を$R_t + \gamma Q_\pi(S_{t+1}, A_{t+1})$の方向へ更新することを表している
- $R_t + \gamma Q_\pi(S_{t+1}, A_{t+1})$がTDターゲット

行動$A_{t+1}$が方策$b$によってサンプリングされた場合を考える

重み$\rho$は
$$
\rho = \frac{\pi(A_{t+1} | S_{t+1})}{b(A_{t+1} | S_{t+1})}
$$

以上より，方策オフ型のSARSAの更新式
$$
sampling: A_{t+1} \sim b \\
Q'_\pi(S_t ,A_t) = Q_\pi (S_t, A_t) + \alpha \{\rho (R_t + \gamma Q_\pi(S_{t+1}, A_{t+1})) - Q_\pi(S_t, A_t)\}
$$

方策オフ型のSARSAの実装
- 行動を取り出すメソッドでは`b`の確率分布から行動を取り出す
- ターゲット方策がgreedyに改善し，挙動方策はε-greedyに改善
- 結果は改善の余地あり

#### Q学習
- 重点サンプリングは出来れば避けたい手法(不安定)

Q学習(Q-learning)
- TD法
- 方策オフ型
- 重点サンプリングを使わない

ベルマン方程式とSARSA
- 方策$\pi$におけるQ関数を$q_\pi(s,a)$としたときのベルマン方程式
$$
q_\pi(s,a) = \sum_{s'} p(s'|s,a) \{r(s,a,s') + \gamma \sum_{a'} \pi(a'|s')q_\pi(s',a'
)\}
$$
- 環境の状態遷移確率$p(s'|s,a)$によって次のすべての状態遷移を考慮していること
- エージェントの方策$\pi$に従って次の全ての行動を考慮していること
- SARSAはベルマン方程式のサンプリング版
    - ある1つのサンプリングされたデータを使っている
    - 次の状態$S_{t+1}$が$p(s'|s,a)$に基づいてサンプリングされている
    - 次の行動$A_{t+1}$が方策$\pi(a|s)$に基づきサンプリングされる
    - TDターゲットは$R_t + \gamma Q_\pi(S_{t+1}, A_{t+1})$
    - ターゲットの方向へQ関数を少しだけ更新
- ベルマン最適方程式に対応したものがQ学習

ベルマン最適方程式とQ学習
- 価値反復法は最適方策を得るための評価と改善を1つにまとめた手法
    - ベルマン最適方程式に基づくただ1つの更新式を繰り返すことで最適方策が得られること
- ベルマン最適方程式による更新で，なおかつそれをサンプリング版にした手法

ベルマン最適方程式
$$
q_*(s,a) = \sum_{s'} p(s'|s,a) \{r(s,a,s') + \gamma max_{a'} q_*(s',a')\}
$$

Q関数への更新
$$
Q'(S_t, A_t) = Q(S_t, A_t) + \alpha \{R_t + \gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t)\}
$$
- 上式による更新で最適方策におけるQ関数へと近づく
- 行動$A_{t+1}$がQ関数の最大値によってえらばれている

Q学習のまとめ
- 方策オフ型の手法(ターゲット方策と挙動方策を持つ)
- 現在の推定値であるQ関数をε-greedy化した方策
- 方策に従ってサンプルデータを集めてQ関数を更新する

Q学習の実装
- 結果は毎回変化するが，たいてい最適方策が得られる

#### 分布モデルとサンプリングモデル
エージェントの実装方法
- 分布モデル(今まではこれ)
- サンプルモデル(シンプル)

分布モデル
- e.g. ランダムに行動するエージェント
    - 行動の確率分布を明示的に保持している

サンプルモデル
- 条件: サンプリングできること
- 確率分布を持つ必要がないためシンプルに実装できる

サンプルモデル版のQ学習
- 以前のエージェントは分布モデルであった
    - 方策`self.pi`と`self.b`は確率分布を保持している
    - `update`メソッドにて両者が更新されている
- 下準備の変更
    - `self.pi`を削除
    - `self.b`の更新を`get_actions`メソッドで行う
- サンプルモデルへ変更
    - `self.b`を削除
        - ε-greedyによる行動の選択をQ関数を使って直接コーディング
- 確率分布，方策を保持していない

#### まとめ
- TD法は今と次の情報だけから価値関数を更新する
    - モンテカルロ法はゴールにたどり着いてから
- TD法の方策制御(SARSAとQ学習)
- SARSA
    - 方策オン型
    - Q関数をTD法で評価
    - ε-greedy化で方策を改善
    - 方策オフ型に拡張可能
- Q学習
    - ベルマン最適方程式から導く
    - 方策オフ型の手法
    - 重点サンプリングを用いない

### 7章 ニューラルネットワークとQ学習
状態の数が多い場合
- テーブルとして保持するのが困難
- テーブルの各要素を独立して評価・改善するのが困難
- 解決策
    - Q関数をコンパクトな関数で近似する
    - ディープラーニング

#### DeZeroの基礎
- ゼロつく3(フレームワーク編)で作成したフレームワーク
- PyTorch版のコードがGitHubに用意されている

ローゼンブロック関数 $y=100(x_1-x_0^2)^2+(x_0-1)^2$ の最小値を見つける
- 真の最小値の探索が難しく，関数の形状に特徴がある事から最適化のベンチマーク問題としてよく使用される
- 目標: ローゼンブロック関数の出力が最小となる $x_0$ と $x_1$ をみつけること
    - 答えは $(x_0, x_1) = (1, 1)$

ローゼンブロック関数の $(x_0,x_1) = (0.0,2.0)$ における微分を求める
- $\frac{\partial y}{\partial x_0}$ と $\frac{\partial y}{\partial x_1}$
```py
import numpy as np
from dezero import Variable
def rosenbrock(x0, x1):
    # 省略 returns y

x0 = Variable(np.array(0.0))
x1 = Variable(np.array(2.0))
y = rosenbrock(x0, x1)
y.backward() # これだけで微分が求まる
print(x0.grad, x1.grad)
```
- 出力 `variable(-2.0) variable(400.0)`
- $(-2.0, 400.0)$ <- 勾配(gradient)，勾配ベクトル

勾配降下法(gradient descent)
```py
lr = 0.001  # 学習率(learning rate)
iters = 10000  # 繰り返しの回数

for i in range(iters):
    print(x0, x1)
    y = rosenblock(x0, x1)

    x0.cleargrad()
    x1.cleargrad()
    y.backward()

    x0.data -= lr * x0.grad.data
    x1.data -= lr * x1.grad.data

print(x0, x1)
```
- おおよそ近い値が得られる

#### 線形回帰
トイ・データセット(toy dataset)の生成
- 小さなデータセット
- $x$ と $y$ の変数から， $y$ にノイズ(乱数)が加算される
- 目標: $x$ の値から $y$ の値を予測できるモデル(数式)を作る
- 平均2乗誤差 (mean squad error)

#### ニューラルネットワーク
- 非線形なデータへの拡張
- 線形変換(linear transformation), アフィン変換(affine transformation)
- 活性化関数
- 層の管理(追加)
- オプティマイザ(Stochastic Gradient Descent; SGD)
    - 勾配を使った最適化手法: Momentum, AdaGrad, AdaDelta, Adamなど

#### Q学習とニューラルネットワーク
Q学習とニューラルネットワークを融合させる

ニューラルネットワークでのカテゴリデータ
- one-hotベクトル( S/M/L -> $(0,0,1)$/$(0,1,0)$/$(1,0,0)$)
- 3x4のグリッドワールドでは状態(12種類，$(y,x)$の形)をカテゴリデータとして考えることができる
    - $(1,12)$ の形状のテンソルに(one-hotベクトル)
        - もとの形状は $(12,)$ だが，バッチ処理を想定して軸を追加している
        - 複数のデータをまとめて処理することがある(バッチ処理)

Q関数を表すニューラルネットワーク
- Q関数はテーブル(Pythonでは辞書)として表現されていた
- ニューラルネットワークの入出力の構造
    1. 入力: 状態と行動， 出力: Q関数の値を1つ
        - 計算コストの問題: Q関数の最大値 $max_a Q(s,a)$ を求めるコストが大きい
    2. 入力: 状態， 出力: Q関数の候補の値をすべて出力

Q学習におけるQ関数の更新式
$$
Q'(S_t, A_t) = Q(S_t, A_t) + \alpha \{ R_t + \gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t)\}
$$
- ターゲット $R_t + \gamma max_a Q(S_{t+1},a)$ を $T$ で表すと
$$
Q'(S_t, A_t) = Q(S_t, A_t) + \alpha \{ T - Q(S_t, A_t)\}
$$
- 入力が $S_t$, $A_t$ のとき出力が $T$ となるようにQ関数を更新すると解釈できる
- NNの文脈では，入力が $S_t$, $A_t$ で出力が $T$ となるように学習させる
    - $T$ は正解ラベル
    - $T$ はスカラ値であり，回帰問題として考えることができる

実装
- `QLearningAgent`クラスではNNとオプティマイザを初期化
    - オプティマイザにNNを設定
- `get_action`メソッドではε-greedyによって行動を選択
    - 引数`state`にはone-hotベクトル化された状態が入力されることを想定
- `update`メソッドではQ関数を更新
    - Q関数の値`next_q`を求める(ただし，doneがTrueの場合0
        - `next_q`は正解ラベルを作るために用いられる
    - `target`を求め，現在の状態におけるQ関数`q`を求める
    - 損失関数として`target`と`q`の平均2乗誤差を求める
    - バックプロパゲーションによりパラメータを更新
    - if文を使わない実装では，ミニバッチによる学習を行うときに活用できる

実行
- 安定した結果が得られないことが多々ある
    - 変化の振れ幅がおおきいが，大域的にはエピソードが重なるごとに損失は小さくなる
- 毎回の実行で結果は変わるが，概ね良い結果が得られる
- エピソード回数を増やすことで最適方策に近い方策が安定して得られる

#### まとめ
- DeZero(フレームワーク)
    - バックプロパゲーションによる微分
- ニューラルネットワークを使ったQ学習の実装

### 8章 DQN
Deep Q Network; DQN
- Q学習とニューラルネットワークの融合(前節)
- $+$ 新しい技術
    - 経験値再生
    - ターゲットネットワーク

#### OpenAI Gym
- オープンソースライブラリ
- 様々な強化学習のタスク(環境)が用意されている

OpenAI Gymの`gym`もジュールをインストール(by pip)
- `$ pop install gym`

`CartPole-v0`(バランス棒)の生成
- v0は200ステップが上限，v1は500ステップが上限
```py
import gym
env = gym.make('CartPole-v0')
```

```py
state = env.reset()
print(state)  # 初期状態

action_space = env.action_space
print(action_space)  # 行動の次元数
```
- 状態の配列
    - カートの位置
    - カートの速度
    - 棒の角度
    - 棒の角速度
- `Discrete(2)`は独自のクラス
    - 2つの行動の候補
    - `0`(左向き)，`1`(右向き)

時間を1進める
```py
action = 0  # 0 or 1
next_state, reward, done, info = env.step(action)
print(next_state)
```
- `env.step()`の返り値
    - 次の状態 (next_state)
    - 報酬 (reward)
        - スカラ値(float)，このタスクにおいてバランスを保っている間は常に`1`
    - 終了かどうかのフラグ (done)
    - 追加の情報 (info)
        - デバッグに有益な情報

ランダムな方策のエージェント
```py
import numpy as np
import gym

env = gym.make('CartPole-v0')
state = env.reset()
done = False

while not done:
    env.render()
    action = np.random.choice([0, 1])
    next_state, reward, done, info = env.step(action)
env.close()
```
- `env.render()`によってタスクの可視化
- OpenAI Gym では状態(state)の代わりに観測(observation)が用いられている
    - 状態とは，環境に関しての完全な記述(情報)
    - 観測とは，状態の部分的な記述

#### DQNのコア技術
Q学習では推定値を使って推定値を更新(ブートストラッピング)
- 正確ではないため，Q学習(広く言えばTD法)は不安定になりやすい
- そこにニューラルネットワークのような表現力の高い関数近似手法が加わるとさらに不安定になる

DQNはQ学習とニューラルネットワークを組み合わせた手法
- 経験再生(experience replay)
- ターゲットネットワーク(target network)

経験再生(experience replay)
- ニューラルネットワークを使って強化学習の問題を上手く解いた例はDQN以前はなかった
    - バックギャモンの学習に成功した例はある
    - 教師あり学習とQ学習の違い
        - (教師あり学習)ミニバッチ: 訓練データセットから一部をランダムに取り出したデータ．データに偏りがない
        - (Q学習)行動によるデータ生成，Q関数の更新．データに強い相関がある
    - 違いを埋めるための経験再生
- エージェントが経験したデータ $E_t = (S_t, A_t, R_t, S_{t+1})$ を一度バッファする
- バッファから経験データをランダムに取り出してQ関数を更新する
- 経験データ間の相関が弱まる(データ効率が良くなる)
- 方策オフ型のアルゴリズムでのみ使用可能
    - 方策オフ型は現時点の方策から得たデータでしか使えないため

経験再生の実装
- `deque`によって，バッファサイズを超えるデータは古いデータから削除
- `get_batch`メソッドによるミニバッチの作成
    - ニューラルネットワークへの変換

ターゲットネットワーク(target network)
- 教師あり学習とQ学習の比較
    - (教師あり学習)データに正解ラベルが付与される
    - (Q学習) $Q(S_t, A_t)$ の値が $R_t + \gamma max_a Q(S_{t+1}, a)$ (TDターゲット)となるようにQ関数を更新する
        - TDターゲットは正解ラベルに相当するが，その値はQ関数が更新されると変動する
- Q関数を表すオリジナルのネットワーク(qnet)を用意する
- もう1つ同じ構造のネットワーク(qnet_target)を用意する
- qnetは通常のQ学習によって更新を行う
- qnet_targetは定期的にqnetの重みと同期するようにして，それ以外重みパラメータを固定したままにする
    - qnet_targetを使ってTDターゲットの値を計算する
    - 教師ラベルであるTDターゲットの変動が抑えられる
    - TDターゲットが常には変動しないのでニューラルネットワークの学習が安定することが期待できる

ターゲットネットワークの実装
- `deepcopy`によるコピー(浅いコピー`copy`では参照がコピーされる，つまりパラメータが共有されてしまう)

#### DQNとAtari
- OpenAI GymにはAtariのゲーム環境が用意されている(別途インストール)
- Pongはボールを打ち合うゲーム
    - 状態はゲームの画像
    - 行動はボードを上下に動かすこと

Pongの場合，MDPの要件は満たさない
- ボールの進行方向が分からない
- POMDP(部分観測マルコフ決定過程; Partially Observable Markov Decision Process)になる
    - RNNを用いた手法が有力

PongのPOMDPからMDPへの変換
- 連続するフレームを重ね合わせて(論文では4フレーム)状態として扱う
    - 状態の繊維が変わる(ボールの動きも)

論文における前処理
- 画像の周囲をトリミング(画像の周囲には無駄な要素がある)
- グレイスケールへの変換
- 画像のリサイズ
- 正規化

ニューラルネットワークにはCNNを用いている
- 入力に近いところで
- 出力に近い層では全結合層
- 活性化関数にはReLU関数

その他の工夫
- GPUの使用(並列処理)
- $\epsilon$の調整
    - 初期では探索を増やし，学習の進行度によって減らしていく
- 報酬クリッピング(Reward Clipping)
    - 報酬を$-1.0$から$1.0$の範囲に収まるように調整
    - Pongにおいての報酬は${-1,0,1}$なので必要ない

#### DQNの拡張
Double DQN
- メインとなるネットワークとターゲットネットワーク
    - パラメータをそれぞれ $\Theta$ と $\Theta'$ とする
    - Q関数をそれぞれ $Q_\Theta(s,a)$ と $Q_\Theta'(s,a)$ で表す
    - Q関数の更新で用いるターゲット
        - $R_t + \gamma max_a Q_\Theta' (S_{t+1}, a)$
    - $Q_\Theta(s,a)$ (TDターゲット)に近づけるように学習する
    - $max_a Q_\Theta'(S_{t+1},a)$ が問題となる
        - 誤差が含まれる推定値($Q_\Theta'$)に対してmax演算子を使うと真のQ関数を使って計算する場合に比べて過大に評価される
- Double DQNにおけるTDターゲット
    - $R_t + \gamma Q_\Theta'(S_{t+1}, argmax_a Q_\Theta(S_{t+1}, a))$
    - $Q_\Theta(s,a)$ を使って最大となる行動を選び，実際の値は $Q_\Theta'(s,a)$ から取得すること
    - 過大評価が解消され学習が安定する
        - 過大評価については[付録C](#付録c)で説明

優先度付き経験再生(prioritized experience replay)
- 経験データをランダムに選ぶのではなく，優先度に応じて選ばれやすく
- 経験データの優先度
    - $\delta_t = | R_t + \gamma max_a Q_\Theta'(S_{t+1}, a) - Q_\Theta(S_t, A_t) |$
    - TDターゲットと $Q_\Theta$ の差分をとってその絶対値を $\delta_t$ とする
    - $\delta_t$ が大きければ大きいほど修正量，学習すべきことが大きいということ(逆も然り)
    - 経験データをバッファに保存する際に $\delta_t$ も計算し，それに従った確率で選ばれる
        - $N$個のデータがある場合，$i$番目の経験データが選ばれる確率は
            - $p_i = \frac{\delta_i}{\sum_{k=0}^{N} \delta_k}

Dueling DQN
- ニューラルネットワークを工夫した手法
- アドバンテージ関数(advantage function)
    - Q関数と価値関数の差分で定義される
    - $A_\pi(s,a) = Q_\pi(s,a) - V_\pi(s)$
    - $a$という行動が方策$\pi$に比べてどれだけ良いか(悪いか)を表す
        - $Q_\pi(s,a)$ は状態$s$において特定の行動$a$を行い，それ以降は$\pi$にしたがって行動したときに得られる収益の期待値
        - $V_\pi(s)$ は状態$s$において以降すべて方策$\pi$に従って行動したときに得られる収益の期待値
    - アドバンテージ関数をもとにしたQ関数
        - $Q_\pi(s,a) = A_\pi(s,a) + V_\pi(s)$
- どのような行動をとっても同じ結果になるような状態で利点がある
    - 何をやっても負ける状態での学習を省ける

#### まとめ
- DQN
    - 経験再生とターゲットネットワーク
- DoubleDQN
- 優先度付き経験再生
- Dueling DQN

### 9章 方策勾配法
これまでの手法は価値ベースの手法(Value-based Method)に分類される
- 「価値」は行動価値関数(Q関数)や状態価値関数を指す
- 価値関数をモデル化し，学習する
- 価値関数を経由して方策を得る
- 一般化方策反復に基づいた最適方策の模索(価値関数の評価と改善を繰り返すこと)

価値関数を経由せずに方策を直接表す手法
- 方策ベースの手法(policy-based method)
- 方策をニューラルネットワークなどでモデル化し勾配を使って方策を最適化する手法
    - 方策勾配法(policy gradient method)

確率的な方策 $\pi(a|s)$
- 状態$s$において行動$a$を取る確率
- 方策をニューラルネットワークでモデル化する
    - すべての重みパラメータを $\theta$ という記号で集約して表す
- ニューラルネットワークによる方策 $\pi_\theta(a|s)$

方策 $\pi_\theta$ を使って目的関数を設定
- 問題設定
    - エピソードタスク
    - 方策 $\pi_\theta$ によって行動を選ぶ
- 状態，行動，報酬からなる時系列データが得られたと仮定
    - $\tau = (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_{T+1})$
    - $\tau$ は軌道(trajectory)と呼ばれる
- 収益(リターン)は割引率 $\gamma$ を用いて
    - $G(\tau) = R_0 + \gamma R_1 + \gamma^2 R_2 + \ldots + \gamma^T R_T$
    - 収益が $\tau$ から計算できる($G(\tau)$ と表記)
- 目的関数 $J(\theta)$ は
    - $J(\theta) = \mathbb{E}_{\tau \sim \pi_\Theta}[G(\tau)]$
    - 収益 $G(\tau)$ は確率的に変動するのでその期待値が目的関数になる
    - $\tau \sim \pi_\theta$ は $\tau$ が $\pi_\theta$ によって生成されることを示す
        - $\tau$ の生成過程には $p(s'|s,a)$ と $r(s,a,s')$ も関係するが，制御できないので省略

目的関数の勾配を求める
- パラメータ $\theta$ に関する勾配 $\nabla_\theta$ で表す
$$
\nabla_\theta J(\theta) = \delta_\theta \mathbb{E}_{\tau \sim \pi_\theta}[G(\tau)]
    = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum^T_{t=0} G(\tau)\nabla_\theta\log\pi_\theta(A_t|G_t)\right]
$$
- 導出過程は[D.1方策勾配法の導出](#d1-方策勾配法の導出)
- $\nabla_\theta$ が $\mathbb{E}$ の中にある
    - 勾配の計算は $\nabla_\theta\log\pi_\theta(A_t|S_t)$ として行われる

続いてニューラルネットワークのパラメータを更新する
- 最適化手法は複数ある
- 単純な方法
    - $\theta \leftarrow \theta + \alpha\nabla_\theta J(\theta)$
    - $\alpha$ は学習率
    - 勾配上昇法

$\nabla_\theta J(\theta)$ は期待値として表され(上式)，モンテカルロ法によって求めることができる
- 方策 $\pi_\theta$ のエージェントに実際に行動させて軌道 $\tau$ を $n$ 個得られたとする
- 各 $\tau$ について期待値の中身を計算し，その平均を求めることで $\nabla_\theta J(\theta)$ を近似
$$
sampling: \tau^{(i)} \sim \pi_\theta (i = 1,2,\ldots,n) \\
x^{(i)} = \sum^T_{t=0} G(\tau^{(i)})\nabla_\theta\log\pi_\theta\left(A^{(i)}_t|S^{(i)}_t\right) \\
\nabla_\theta J(\theta) \simeq \frac{x^{(1)}+x^{(2)}+\ldots+x^{(n)}}{n}
$$
- $\tau^{(i)}$: $i$ 番目のエピソードで得られた軌道
- $A^{(i)}_t$: $i$ 番目のエピソードの時刻 $t$ における行動
- $S^{(i)}_t$: $i$ 番目のエピソードの時刻 $t$ における状態

以降， $n=1$ の場合について考える

$$
\nabla_\theta\log\pi_\theta(A_t|S_t) = \frac{\nabla_\theta\pi_\theta(A_t|S_t)}{\pi_\theta(A_t|S_t)}
$$
- $\nabla_\theta\log\pi_\theta(A_t|S_t)$ は $\nabla_\theta\pi_\theta(A_t|S_t)$ という勾配ベクトルを $\frac{1}{\pi_\theta(A_t|S_t)}$ 倍したもの
- $\nabla_\theta\log\pi_\theta(A_t|S_t)$ と $\nabla_\theta\pi_\theta(A_t|S_t)$ は同じ方向を指す
- $\nabla_\theta\pi_\theta(A_t|S_t)$ は状態 $S_t$ で行動 $A_t$ をとる確率が最も増える方向を指す
- $\nabla_\theta\log\pi_\theta(A_t|S_t)$ も状態 $S_t$ で行動 $A_t$ をとる確率が最も増える方向指す
- その方向に対して $G(\tau)$ という重みがかけられる

方策勾配法の実装
- ニューラルネットワーク: 2層の全結合からなるモデル
    - 出力: ソフトマックス関数(各行動に対する確率が得られる)
- `self.pi(state)`によってNNの順伝播を行い確率分布`probs`を得る
- その確率分布に従って行動を1つサンプリング(選んだ行動の確率は`probs[action]`で返す)

変数を数式と照らし合わせる
$$
G(\tau)\nabla_\theta\log\pi_\theta(A_0|S_0)
$$
- `prob` (Dezero.Variable) : $\pi_\theta(A_0|S_0)$
- `G` (float) : $G(\tau)$
- `J` (Dezero.Variable) : $G(\tau)\log\pi_\theta(A_0|S_0)$
- `J`が求まれば，`J.backward()`によって $G(\tau)\log\pi_\theta(A_0|S_0)$ が求まる

- 損失関数は`-F.log(prob)`
    - 目的関数のマイナスを損失関数とする$-J(\theta)$
    - 勾配降下法の最適化手法(SGD, Adam, etc)によってパラメータ更新ができる

#### REINFORCE
方策勾配法を改善した手法
- REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility

復習
- 最も単純な勾配方策法
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau\sim\pi_\theta}\left[\sum^T_{t=0}G(\tau)\nabla_\theta\log\pi_\theta(A_t|S_t)\right]
$$
- $G(\tau)$ 割引率付きの報酬の総和
- どの時刻においても常に一定の重みを使って行動をとる確率を大きく(小さく)している問題点
    - エージェントの行動の良し悪しはその行動の後に得られた報酬の総和によって評価される
    - i.e. ある行動を起こす前に得られた報酬はその行動の良し悪しとは関係ない

本来関係のない過去の報酬($G(\tau)$ に含まれる)を除去する
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau\sim\pi_\theta}\left[\sum^T_{t=0}G_t\nabla_\theta\log\pi_\theta(A_t|S_t)\right] \\
G_t = R_t + \gamma R_{t+1} + \ldots + \gamma^{T-t}R_T
$$
- $G_t$ は時刻$t$から$T$までの間に得られる報酬の総和
- このアイディアに基づくアルゴリズムはREINFORCEとして知られている
- 証明には文献を参照

REINFORCEの実装
- `self.memory`の要素を後ろから順にたどって`G`を求める

#### ベースライン
ベースライン(baseline)
- 分散を減らす方法
- 過去のデータから予測値を求め，その差分を考える

ベースライン付きの方策勾配法
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau\sim\pi_\theta}\left[\sum^T_{t=0}G_t\nabla_\theta\log\pi_\theta(A_t|S_t)\right] \\
    = \mathbb{E}_{\tau\sim\pi_\theta}\left[\sum^T_{t=0}(G_t-b(S_t))\nabla_\theta\log\pi_\theta(A_t|S_t)\right]
$$
- $b(S_t)$ は任意の関数(ベースライン)
- 式変形の証明は[D.2 ベースラインの導出](#d2-ベースラインの導出)
- 関数の例
    - これまでに得られた報酬の平均
    - 価値関数 $V_{\pi_\theta}(S_t)$

#### Actor-Critic
- 価値ベースの手法と方策ベースの手法
    - DQNやSARSAは価値ベースの手法
- 価値ベースかつ方策ベースの手法を考える
    - ベースライン付きのREINFORCEで，ベースラインに価値関数を用いるなら，価値ベースかつ方策ベースと考えられる

Actor-Criticを考える(価値ベースかつ方策ベース)

ベースライン付きの

### 付録A

### 付録C

### 付録D
#### D.1 方策勾配法の導出
#### D.2 ベースラインの導出
