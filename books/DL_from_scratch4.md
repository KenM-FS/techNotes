# ゼロから作るDeep Learning 4 -強化学習編
齊藤 康毅. 株式会社オライリー・ジャパン, 2022.04.04

Finish reading at:

###### Purpose
Learn about Reinforcement Learning for my research

## Notes
GitHub: https://github.com/oreilly-japan/deep-learning-from-scratch-4

Workbook: https://koki0702.github.io/dezero-p100/

### 1章 バンディット問題
#### 機械学習の分類と強化学習
教師あり学習，教師なし学習，強化学習とその違いについて

#### バンディット問題
Bandit Problem
- "bandit" はスロットマシンの別称 (one-armed bandit(一本腕の盗賊)と揶揄されていた)
- 正確には"多腕バンディット問題(multi-armed bandit problem)"という

概要
- 一本のレバーがあるスロットマシンが複数台ある
- スロットマシンごとに特性が異なる(当たりの多い少ない)
- 特性の情報は与えられない
- 決められた回数プレイし，得られるコインをできるだけ多くする

バンディット問題を強化学習の用語へ変換
- スロットマシン -> 環境(environment)
- プレイヤー -> エージェント(agent)
- スロットマシンの選択とプレイ -> 行動(action)
- コインの受け取り -> 報酬(reward)

バンディット問題を解くアルゴリズムを考える
##### 「良い」スロットマシンの選択
確率を考える(各スロットマシンの具体的な確率は与えられない)(確率分布表で表現される)
- 期待値(expectation value)
    - バンディット問題では「価値(value)」と呼ぶ
    - 行動に対して得られる報酬の期待値は「行動価値(action value)」と呼ぶ

報酬を`R`で表す(Reward)．`R`は`{0,1,5,10}`のいずれかの値をとり，各値に取りやすさとして確率が決まっている
- 確率変数(random value): 取る値が確率的に決まる変数のこと
- `t`回目に得られる報酬は`R_t`

エージェントの行う行動を`A`で表す(Agent)．変数`A`はスロットマシン(aとbの2つなら`{a,b}`)の値をとる．

確率変数に定義される期待値は`\mathbb{E}`で表す(Expectation)．
- 報酬`R`の期待値は`\mathbb{E}[R]`
- 行動Aを選んだ場合の報酬の期待値は`\mathbb{E}[R|A]` (条件付確率)
    - aのスロットを選択した場合，`\mathbb{E}[R|A=a]` or `\mathbb{E}[R|a]

報酬の期待値(行動価値)は`Q`,`q`で表す(Quantity)．
- 行動`A`に対する価値は`q(A) = \mathbb{E}[R|A]`
- 小文字の場合は，真の行動価値を表し，大文字の場合は推定値を表す．一般的には真の行動価値は知りえないので，それを推定する．

##### バンディットアルゴリズム
エージェントは行動の結果から自分の選択がどれだけ良かったのか(悪かったのか)を推定する必要がある
- もし各スロットマシンの価値(報酬の期待値)がわかれば，プレイヤーは最も良いスロットマシンを選ぶことができる
- しかし，各スロットマシンの価値はプレイヤーにはわからない
- よってプレイヤーには各スロットマシンの価値をできるだけ精度よく推定することが求められる

実際に得られた報酬の平均値をスロットマシンの価値の推定値とする
- 標本平均(大数の法則により真の値に近づく)

##### 平均値を求める実装
ランダムな数の生成をn回行いそれらを足し合わせ，nで割る
- nが増加すると計算量(和算)もメモリも増加する問題点がある

式変換により，
` Q_n = (1 - 1/n)Q_{n-1} + \frac{1}{n}R_n`
と表すことができる．
- これまでの報酬(`R_1,R_2,...,R_{n-1}`)が必要ない

さらに，
`Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})`
としたとき，
- 右辺第二項が，`Q_n`の`Q_{n-1}`に対する増加量である
- `1/n`は学習率としての役割がある

```py
Q += (reward - Q) / n
```

##### 戦略
greedyな行動: これまでの結果をもとに，一番良いスロットマシンを選ぶこと
- 活用(exploitation)とも呼ぶ
- 不確かさが原因に信頼の低い選択をし続けてしまう問題点

探索(exploration)により，各スロットマシンの価値をより高精度に推定する

活用と探索はトレードオフの関係にある．どうバランスをとるか
- ε-greedy法: 最も基本的で応用の利くアルゴリズム
    - εの確率(e.g. ε=0.1)で，探索を行い，それ以外は活用を行う

#### バンディットアルゴリズムの実装
- スロットマシンが返すコインの枚数は0枚 or 1枚 (負けor勝ち)
- スロットマシンに設定されている確率は勝つ確率
- 10台のスロットマシン

スロットマシンの実装(Banditクラス)
- initのnp.random.rand(arms=10)で，10個の0.0以上1.0未満の値を取る(各スロットマシンの勝率)
- playクラス
    - randして，該当するarmの勝率より小さければreturn1, else return 0

エージェントの実装
- 配列Qn: 対応するスロットマシンの推定値を格納
- `epsilon`はランダムに行動する確率
- `action_size`は行動の数(10台のスロットマシン)
- `update`メソッドで価値の推定を更新

ランダム性により実験のたびに結果が異なる
- 実験を複数回行って平均的な結果を取る
- 実験の回数ごとに各ステップで平均をとる

εごとの変化
- 0.01, 0.1, 0.3 高いと探索しすぎる，低いと最善のマシンを見つけられない．

#### 非定常問題
バンディット問題は，定常問題(stationary problem, 報酬の確率分布が定常である)

非定常問題(non-stationary problem): プレイするたびにスロットマシンの価値(勝率)が変動する

標本平均は得られた報酬の平均によって求めることができる
- `1/n`を重みとみなすことができる
- 非定常問題において，過去のデータの重要性は時間とともに低くするべき
- `Q_n = Q_{n-1} + 1/n(R_n - Q_{n-1})`において，`1/n`を固定値`\alpha`に置き換える(`0<\alpha<1`)
- 固定値にすることで，過去の報酬の重みが指数関数的にちいs買う成る．
- 式変形し，nを1つ下げ，Q_{n-1}とQ_{n-2}の関係性をみるとその様子がみれる
- 指数移動平均, exponential moving average
- 指数加重移動平均, exponential weighted moving average

Q_0が必要であり，その初期値は手動で設定するため学習に偏り(バイアス)が発生する．

#### まとめ
バンディットアルゴリズムは，複数の選択肢から最善の選択肢を選ぶ問題に使う
- 売り上げに貢献するWebデザインの選択
- 有効な薬の選択

バンディットアルゴリズムのε-greedy法以外の手法
- UCB(upper confidence bound)アルゴリズム
- 勾配バンディットアルゴリズム

平均
- 定常問題の場合は標本軽金
- 非定常問題の場合は指数移動平均

### 2章 マルコフ決定過程
エージェントの行動によって状況が変化する問題
- 非定常問題ではエージェントの行動に関係なく時間とともに変化していた

このような問題の一部はマルコフ決定過程(Markov decision process; MDP)として定式化される
- 決定過程とは，エージェントが環境と相互作用しながら行動を決定する過程
- エージェントが置かれる「状態(state)」
- MDPには時間の概念が必要(タイムステップ単位)
- 目先の報酬ではなく報酬の総和を最大化することが求められる
- 行動A_tを行い，報酬R_tを得て，次の状態であるS_{t+1}へ遷移する
    - 報酬はR_tだったり，R_{t+1}だったりする(慣例)

#### 環境とエージェントの定式化
MDPにおけるエージェントと環境のやり取りを数式によって定式化
- 状態遷移
- 報酬
- 方策

決定論的(deterministic)な状態遷移: 次の状態s'が，現在の状態sと行動aによって一意に決まる事
- `s' = f(s, a)` <- 状態遷移関数 (state transition function)

確率的(stochastic)な状態遷移
- `p(s'|s, a)` <- 状態遷移確率 (state transition probability)
    - 状態sで行動aを選択したときに状態s'へ遷移する確率

マルコフ性(Markov property): 次の状態が現在の状態と行動にだけ依存すること
- MDPはマルコフ性を仮定することによって，遷移状態と報酬をモデル化する
- 仮定できなければ，過去のすべての状態や行動を考慮しなければならず指数関数的な情報を処理しないといけなくなる

報酬関数(reward function)
- `r(s,a,s')`: 状態sにいるときに，行動aを行い，状態s'に遷移したときに得られる報酬
- 遷移が確率的であるなら，報酬関数は報酬の期待値を返す

エージェントの方策
- 方策(policy)
- マルコフ性により，エージェントは現在の状態だけに基づいて行動を決める
- 決定論的な方策の例
    - `a = \mu(s)`: 状態sを引数として与えると，行動aが帰ってくる
- 確率的な方策の例
    - `\pi(a|s)`: ある状態sにいるときに行動aを取る確率を表す

MDPの目標
- エージェントは方策`\pi(a|s)`によって行動し，その行動と状態遷移確率`p(s'|s,a)`によって次の状態へ遷移し，報酬関数`r(s,a,s')`に従って報酬が与えられる
    - 決定論的な方策を持つ場合は`\mu`で表すことができる
- 以上の枠組みの中で最適方策(optimal policy)を見つけることがMDPの目標となる
    - 最適方策は，収益が最大となる方策
- MDPの問題は大きく2つに分類する
    - エピソードタスクと連続タスク

エピソードタスク
- 終わりのある問題
    - e.g. 囲碁(win/lose/draw)
- 終わるとまた初期状態から始まる

連続タスク
- 終わりのない問題
    - e.g. 在庫管理問題(適切な商品の仕入れを永遠に行う)

収益(return)
- `G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ...`
    - 時刻`t`に状態`S_t`である場合に，エージェントが方策`\pi`によって行動`A_t`を行い，報酬`R_t`を得て，新しい状態`S_{t+1}`に遷移する場合の収益`G_t`
- 時間が進むにしたがって報酬が割引率(discount rate)`\gamma`によって指数関数的に減衰される
    - 連続タスクにおいて収益が無限大になることを防ぐため

状態価値関数
- エージェントと環境は確率的にふるまう可能性がある
- 収益の期待値を指標とする必要がある(状態`S_t=s`，方策が`\pi`であるときエージェントが受け取る収益の期待値)
    - `v_\pi(s) = \mathbb{E}[G_t | S_t = s, \pi]`
    - `v_\pi(s)` 状態価値関数(state-value function)
    - 別の表現 `v_\pi(s) = \mathbb{E}_\pi [G_t | S_t=s]`
- `v_\pi`は真の状態価値関数，`V_\pi`は推定値としての状態価値関数

最適方策と最適価値関数
- すべての状態において`v_{\pi'}(s) >= v_\pi(s)`となる時のみ，方策`\pi'`が方策`\pi`よりも良い方策だと言える
- MDPでは最適方策が少なくとも1つは存在し，その最適方策は決定論的方策である
    - `a = \mu_*(s)`
- 最適状態価値関数(optimal state-value function): 最適方策における状態価値関数．`v_*`

#### MDPの例
具体的な問題設定
- 2マスのグリッドワールド(L1,L2) w/ 両端に壁
- エージェントは右or左のどちらかに移動
- エージェントがL1からL2に移動したときにリンゴを受け取り`+1`の報酬を得る
- エージェントがL2からL1に移動するとリンゴが出現
- 壁にぶつかると`-1`の報酬を得る
- 連続タスク

バックアップ線図: 有向グラフによって，状態・行動・報酬を表現したもの
- エージェントの方策が決定論的な場合，一直線になる
- 確率的な場合，無限に2分(右と左)していく

最適方策を見つける
- 決定論的方策`a = \mu(s)`
- 今回は状態が2つ，行動が2つで4通りの方策がある

状態価値関数の計算(`\mu_1`)
```
v_{\mu_1}(s=L1) = 1 + 0.9(-1) + 0.9^2(-1) + ...
= 1 - 0.9(1+0.9+0.9^2+...)
= 1 - 0.9(1-0.9)
= -8
```
- 参考: 無限等比級数の公式 `1+r+r^2+...=1/(1-r)`
- コードではfor文によって近似する

各方策の価値関数により，方策`\mu_2`が最適方策(5.26)であることがわかる．

#### まとめ
- MDPはエージェントと環境の相互のやり取りを定式化したもの
- 環境には状態遷移確率(状態遷移関数)と報酬関数がある
- エージェントには方策がある
- 最適方策とは，すべての状態において他のどの方策よりも状態価値関数の値が大きいまたは等しい方策

### 3章 ベルマン方程式
確率的なふるまいをするMDPの場合は手計算による状態価値関数を求めることができない

このような状態でも状態価値関数を求めるベルマン方程式(Bellman equation)

#### ベルマン方程式の導出
前知識
- 確率の期待値について
- 条件付確率について
- 期待値の表現

収益(リターン)
`G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ...`

次のように変換できる
`G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...`

2つの式より
`G_t = R_t + \gamma G_{t+1}`...(3.3)
収益である`G_t`と`G_{t+1}`の関係性

状態価値関数(収益の期待値)
`v_\pi (s) = \mathcal{E}_\pi [G_t | S_t = s]`

式(3.3)を代入すると，
```
v_\pi (s) = \mathcal{E}_\pi [R_t + \gamma G_{t+1} | S_t = s]
= \mathcal{E}_\pi [R_t | S_t = s] + \gamma \mathcal{E}_\pi [G_{t+1} | S_t = s]
```
- 最後の展開は，期待値の線形性により成り立つ
- 確率的方策，確率的状態遷移を想定する(決定論的もカバーできる)

まず，`\mathcal{E}_\pi [R_t | S_t = s]`を計算する．
- 行動`a`の確率と，それらの行動について確率的な遷移`s'`がある

よって
```
\mathcal{E}_\pi [R_t | S_t = s] = \sum_a \sum_s' \pi(a|s) p(s'|s,a) r(s,a,s')
```
- エージェントの行動の確率`\pi`
- 遷移先となる状態の確率`p`
- 報酬関数`r`
- `\sum_a,s'`とまとめて書く

次に第2項(`\mathcal{E}_\pi [G_{t+1} | S_t = s]`)について
- `v_\pi (s) = \mathcal{E}_\pi [G_{t+1} | S_{t+1} = s]`を考える
- 求めたい式の，状態の時刻を1つ進める
    - 具体例: 状態`S_t=s`で，`0.2`の確率で行動`a_1`を選び，`0.6`の確率で`s_1`に遷移する
- すべての候補について計算を行い和を求める
- 現在の状態から遷移する可能性のある遷移先での収益の期待値(期待値の期待値?)
```
\mathcal{E}_\pi [G_{t+1} | S_t = s] = \sum_{a,s'} \pi(a|s) p(s'|s,a) \mathcal{E}_\pi [G_{t+1} | S_{t+1} = s']
= \sum_{a,s'} \pi(a|s) p(s'|s,a) v_\pi(s')
```

以上より，ベルマン方程式
```
v_\pi (s) = \sum_{a,s'} \pi(a|s) p(s'|s,a) {r(s,a,s') + \gamma v_\pi(s')}
```
- 状態`s`の価値関数とその次に取りえる状態`s'`の価値関数との関係性を示した方程式
- 全ての状態`s`とすべての方策`\pi`について成り立つ

#### ベルマン方程式の例
2マスのグリッドワールドでの実演
- 今回はエージェントは50%の確率で右，50%の確率で左へ移動する
- `v_\pi (L1)`は状態がL1でランダムな方策`\pi`に従って行動したときに得られる収益の期待値
    - 無限に続く報酬の総和

ベルマン方程式を使って`v_\pi(L1)`を表現する
```
v_\pi(s) = \sum_{a,s'} \pi(a|s)p(s'|s,a){r(s,a,s') + \gamma v_\pi (s')}
= \sum_a \pi(a|s) \sum_s' p(s'|s,a) {r(s,a,s') + \gamma v_\pi(s')}
```
- 総和を分離
- 決定論的に遷移

状態遷移は確率`p`ではなく関数`f`によって決定する
- if `s' = f(s,a)`, `p(s'|s,a)=1`
- if `s' \neq f(s,a)`, `p(s'|s,a)=0`

よって簡略化出来る
```
s' = f(s,a) として，
v_\pi(s) = \sum_a \pi(a|s) {r(s,a,s') + \gamma v_\pi(s')}
```

今回の問題をあてはめる(割引率は0.9)
- 0.5の確率で行動Leftを選択し，状態L1へ遷移する．報酬は-1(壁)
`0.5{-1 + 0.9v_\pi(L1)}`

- 0.5の確率で行動Rightを選択肢，状態L2へ遷移する．報酬は1
`0.5{1 + 0.9v_\pi(L2)}`

以上より，状態L1におけるベルマン方程式は
```
v_\pi(L1) = 0.5{-1 + 0.9v_\pi(L1)} + 0.5{1 + 0.9v_\pi(L2)}
-0.55v_\pi(L1) + 0.45v_\pi(L2) = 0
```

同様に状態L2におけるベルマン方程式を求める
- 状態L1への遷移で報酬0(壁)
- 状態L2への遷移で報酬-1
```
v_\pi(L2) = 0.5{0 + 0.9v_\pi(L1)} + 0.5{-1 + 0.9v_\pi(L2)}
0.45v_\pi(L1) - 0.55v_\pi(L2) = 0.5
```

2つのベルマン方程式で連立方程式ができる
```
v_\pi(L1) = -2.25
v_\pi(L2) = -2.75
```
- ランダムな方策における状態価値関数
- 状態L1にいるときに将来にわたって-2.25の収益が期待される
- 状態L2にいるときに将来にわたって-2.75の収益が期待される

ベルマン方程式の意義
- 無限に続く計算を有限の連立方程式に変換することができる
- ランダムな振る舞いがあっても状態価値関数を求めることができる

#### 行動価値関数とベルマン方程式
行動価値関数 (action-value function)
- 「価値関数」とあった場合は，状態価値関数を表す
- 行動価値関数はQ関数(Q-function)と慣例的に表記する

復習: 状態価値関数
- `v_\pi(s) = \mathcal{E}_\pi[G_t | S_t = s]`
- 状態がsであること，方策が\piであること

上の2つの条件に「行動a」を追加するのが行動価値関数(Q関数)
`q_\pi(s,a) = \mathcal{E}[G_t | S_t = s, A_t = a]`
- 時刻tのときに状態sで行動aを取り，時刻t+1以降では方策piに従った行動をとる．
その時に得られる収益の期待値が`q_\pi(s,a)`
- 行動aは方策piとは関係ない．行動aは自由に決めて，その後に方策piに従って行動する

Q関数の行動aを方策piにしたがって選ぶならば，Q関数と状態価値関数は同じになる
- この場合の収益の期待値は，Q関数の重み付き和として求めることができる
    - `\sum_{a={行動の候補}} \pi(a|s)q_\pi(s,a)`
- `v_\pi(s) = \sum_a \pi(a|s)q_\pi(s,a)`

行動価値関数を使ったベルマン方程式(導出)
- Q関数を展開
```
q_\pi (s,a) = \mathcal{E}_\pi [G_t | S_t = s, A_t = a]
= \mathcal{E}_\pi [R_t + \gamma G_{t+1} | S_t = s, A_t = a]
```
- 状態`s`，行動`a`は決まっている
- 次の状態`s'`へは確率`p(s'|s, a)`で遷移する
- 報酬が`r(s,a,s')`で与えられる

さらなる展開
```
q_\pi (s,a) = \mathcal{E}_\pi [R_t + \gamma G_{t+1} | S_t = s, A_t = a]
    = \mathcal{E}_\pi [R_t | S_t = s, A_t = a] + \gamma \mathcal{E}_\pi [G_{t+1} | S_t = s, A_t = a]
    = \sum_s' p(s'|s,a) r(s,a,s') + \gamma \sum_s' p(s'|s,a) \mathcal{E}_\pi [G_{t+1}|S_{t+1} = s']
    = \sum_s' p(s'|s,a) {r(s,a,s') + \gamma \mathcal{E}_\pi[G_{t+1}|S_{t+1}=s']}
    = \sum_s' p(s'|s,a) {r(s,a,s') + \gamma v_\pi (s')}
```

状態価値関数`v_\pi(s')`を用いて
```
q_\pi(s,a) = \sum_s' p(s'|s,a){r(s,a,s') + \gamma \sum_a' \pi(a'|s')q_\pi(s',a')}
```
- Q関数を用いたベルマン方程式
- a'はt+1の行動

#### ベルマン最適方程式
- ベルマン方程式はある方策`\pi`に対して成り立つ方程式
- 最終的に求めたいのは最適方策(すべての状態において状態価値関数が最大となる方策)
- ベルマン最適方程式(Bellman optimality equation): 最適方策に関して成り立つ方程式

最適方策を`\pi_*(a|s)`としたときに次のベルマン方程式が成り立つ
`v_*(s) = \sum_a \pi_*(a|s) \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_*(s')}`
- `v_*(s)`は最適方策の価値関数
- 行動`a`について，最適方策`\pi_*(a|s)`はどのような行動を選ぶのか?
    - 候補からただ一つを常に選ぶ決定論的な方策
    - 決定的論方策`\mu_*(s)`によって表すことができる
- 最適方策は方程式の値が最大の行動を選び，その値がそのまま価値関数の値になる
```
v_*(s) = \max_a \sum_a \pi_*(a|s) \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_*(s')}
```
- これはベルマン最適方程式

行動価値関数(Q関数)におけるベルマン最適方程式
- 最適行動価値関数 (optimal action-value function), `q_*`
```
q_*(s,a) = \sum_s' p(s'|s,a){r(s,a,s') + \gamma \sum_a' \pi_*(a'|s')q_*(s',a')}

q_*(s,a) = \sum_s' p(s'|s,a){r(s,a,s') + \gamma \max_a' q_*(s',a')}
```
- Q関数に関するベルマン最適方程式
- MDPでは決定論的な最適方策が少なくとも1つは存在する
- 複数ある場合もあるが値は同じなので同じ記号で表せる

#### ベルマン最適方程式の適用
2マスのグリッドワールドへの適応

ベルマン最適方程式より，状態遷移が決定論的な場合
```
s' = f(s,a) として
v_*(s) = \max_a {r(s,a,s') + \gamma v_*(s')}
```
- L1: Left->-1 Right->1
- L2: Left->0 Right->-1
- gamma = 0.9

```
v_*(L1) = max { -1 + 0.9v_*(L1), 1 + 0.9v_*(L2)}
v_*(L2) = max { 0.9v_*(L1), -1 + 0.9v_*(L2)}
```
連立方程式になり，その解は
- `v_*(L1) = 5.26`
- `v_*(L2) = 4.73`

仮に最適行動価値関数`q_*(s,a)`がわかっているとする．
- 状態`s`における最適な行動は
```
\mu_*(s) = argmax_a q_*(s,a)
```

```
q_\pi (s,a) = \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_*(s')}
より，(3.3 行動価値関数とベルマン方程式)

\mu_*(s) = argmax_a \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_*(s)}
```
- 最適状態価値関数`v_*(s)`を用いて最適方策`\mu_*(s)`を得ることができる
- これらの方策はgreedyである．(局所的な候補の中から最善となる行動を求める)

2マスのグリッドワールドにおける最適方策を求める
- L1の状態で，
    - Leftを取ると，`-1+0.9v_*(L1) = 3.734`
    - Rightを取ると，`1 + 0.9v_*(L2) = 5.257`
    - 最大となるほうRightを選択する．
- 同様に，L2の状態ではLeftを選択するのが最適となる

#### まとめ
- ベルマン方程式により，連立方程式が得られて，価値関数を求めることができる
- 実践的な問題では計算量が膨大になるが，この方程式は重要な基礎となる

### 4章 動的計画法
ベルマン方程式による価値関数を求める流れ
1. 状態遷移確率p，報酬関数r，方策piが与えられる
2. ベルマン方程式
3. 連立方程式の解を求める
4. 価値関数vが求まる

状態と行動のパターン数が増えて計算量が増えた場合の動的計画法(dynamic programming; DP)

強化学習が対処する2つのタスク
- 方策評価 (policy evaluation)
    - ある方策が与えられたときに，その方策の価値関数やQ関数を求めること
- 方策制御 (policy control)
    - 方策を制御して最適方策へと調整すること

復習
- 価値関数 `v_\pi(s) = \mathcal{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s]`
- 上の無限を解決するベルマン方程式 `v_\pi(s) = \sum_{a,s'} \pi(a|s)p(s'|s,a){r(s,a,s') + \gamma v_\pi(s')}`

ベルマン方程式を更新式へと変形する
```
V_{k+1} = \sum_{a,s'} \pi(a|s)p(s'|s,a){r(s,a,s') + \gamma V_k(s')}
```
- `V_{k+1}(s)`や`V(s)`は推測値
- 次に取りえる状態の価値関数`V_k(s')`を使って
- 今いる状態の価値関数`V_{k+1}(s)`を更新する
- 推定値`V_k(s')`を使って別の推定値`V_{k+1}(s)`を改善すること
- 上のプロセスをブートストラッピング(bootstrapping)と呼ぶ
    - ブートストラップとは，靴の後ろについている履くための輪っか．他者の助けなしに自分だけで改善するプロセスのこと

DPを使った具体的なアルゴリズム
- `V_0(s)`の初期値を設定
- 上の更新式で`V_0(s)`から`V_1(s)`へと更新する(さらに`V_2(s)`へ)
- これを繰り返してゴールへの`v_\pi(s)`へ近づいていく
- 反復方策評価 (iterative policy evaluation)
- 更新は`v_\pi(s)`に収束することが証明されている(条件有り)

DPはアルゴリズムの総称
- 問題を小さな問題に分割して求める手法
- 同じ計算を二度としない
    - トップダウン方式 (メモ化)
    - ボトムアップ方式 (上で紹介した方法はこっち)

2マスのグリッドワールドで反復方策評価アルゴリズムの流れを見る
- ランダムな方策`\pi`(0.5で左，0.5で右)
- 状態遷移は決定論的に決まる

更新式の簡略化
- `\sum`を分けて書く
- 状態遷移が決定論的
```
s' = f(s,a)
V_{k+1}(s) = \sum_a \pi(a|s) {r(s,a,s') + \gamma V_k(s')}
```
- `V_0(L1)=0, V_0(L2)=0`

左は推移の確率`0.5`，報酬`-1`で状態はL1 (gamma = 0.9)
- `0.5{-1 + 0.9V_0(L1)}`

右は推移の確率`0.5`，報酬`1`で状態はL2
- `0.5{1 + 0.9V_0(L2)}`

以上より，`V_1(L1)`は
```
V_1(L1) = 0.5{-1 + 0.9V_0(L1)} + 0.5{1 + 0.9V_0(L2)}
    = 0
```

同様に`V_1(L2)`
```
V_1(L2) = 0.5(0 + 0.9V_0(L1)) + 0.5(-1 + 0.9V_0(L2))
    = -0.5
```

V_0(s) L1:0, L2:0 -> V_1(s) L1:0, L2:-0.5

Pythonでの実行結果
- L1: -2.249... L2: -2.749...
- 真の価値関数は[-2.25, -2.75]であり，近似されている

閾値を設定して更新回数を自動で決める
- 更新された値の最大値が閾値を下回るまでループ

反復方策評価の別の実装方法
- 上書き方式: 更新スピードが速い

#### より大きな問題へ
3x4のグリッドワールド
- 上下左右の4方向の行動
- 灰色のマスは壁を表す
- グリッドの外側にも壁
- 壁にぶつかった場合の報酬は0
- リンゴは報酬+1，爆弾は-1，それ以外は0
- 環境の状態遷移は一意に決まる
- エピソードタスクであり，リンゴを取ったら終了

`@property`: インスタンス変数
    - 対称のメソッドをインスタンス変数として使用できる
```py
class GridWorld:
    @property
    def height(self):
        return len(self.reward_map)

    def states(self):
        for h in range(self.height):
            for w in range(self.width):
                yield(h, w)
```
```py
env = GridWorld()
print(env.height) # 3

for state in env.states():
    print(state)
# (0,0)
# (0,1)
# ...
# (2,3)
```
- 全行動と全状態がfor文によってアクセスできる
- `yield`は`return`と同じで関数の値を返すが，加えて関数の処理を一時停止させて別の処理にうつることができる．
    - 別の処理が終わったら一時停止させたところから関数の処理を再開
    - `for`文などの反復処理と組み合わせることができる

`collections.defaultdict`について
- 価値関数はディクショナリとして実装していた`V = {}`(Vの初期化)
- keyが存在していない場合にはエラーが発生してしまう
- `V = defaultdict(lambda: 0)`
    - キーが生成されたときに，デフォルト値(今回は0)が設定される
    - (存在しないキーが参照されたときに，デフォルトの値を返す)

反復方策評価の実装
- 1ステップの更新を行うメソッド`eval_onestep`関数
- 更新値が閾値を下回るまで更新を行うメソッド`policy_eval`関数
- ランダムな方策における価値関数

#### 方策反復法
ベルマン最適方程式を満たす連立方程式を解く方法
- 状態のサイズを`S`，行動のサイズを`A`としたとき，解を求めるには`A^S`のオーダーの計算量

方策の改善
- 最適方策`\mu_*(s)`
- 最適方策における状態価値関数`v_*(s)`
- 最適方策における行動価値関数`q_*(s,a)`

最適方策`\mu_*(s) = argmax_a q_*(s,a)`を，何らかの決定的方策`\mu`に置き換える
```
\mu'(s) = argmax_a q_\mu (s,a)
    = argmax_a \sum_s' p(s'|s,a){r(s,a,s') + \gamma v_\mu(s')}
```
- 現状の方策 `\mu(s)`
- 方策`\mu(s)`における状態価値関数`v_\mu (s)`
- 新たな方策`\mu'(s)`
- これによって方策を更新することを，「greedy化」と呼ぶ

greedy化された方策`\mu'(s)`の性質
- もし全ての状態sにおいて\mu(s)と\mu'(s)が同じであれば，方策\mu(s)は最適方策である
- 更新されるなら，すべての状態sにおいて`v_{\mu'}(s) >= v_\mu(s)`

方策改善定理(policy improvement theorem): 方策が改善される数学的根拠

評価と改善(更新)を繰り返して方策を改善する
- 方策`\pi_0`からスタート(`\pi_0(s|a)`)
- 方策`\pi_0`における価値関数を評価して`V_0`を得る(反復方策評価アルゴリズム)
- 評価関数`V_0`を使ってgreedy化を行う．
    - greedy化された方策は常に1つの行動が選ばれるので，決定論的な方策として`\mu_1`が得られる

以上が方策反復法(policy iteration)

- プランニング問題: エージェントが実際の行動を行わずに最適方策を見つける問題

#### 方策反復法の実装
方策は決定論的なので，簡略化できる
```
s' = f(s,a)として
\mu'(s) = argmax_a {r(s,a,s') + \gamma v_\mu(s')}
```

- 最大の価値関数を持つ行動(max_action)を取り出して，その行動が選ばれる確率が`1.0`となるように確率分布を生成する
- 初期の方策は各行動が均等に選ばれるような方策

### 価値反復法
一般化方策反復(generalized policy iteration): 評価と改善の2つの作業を交互に繰り返すアルゴリズム
- 汎用性があり，広範囲にわたって適用できる
- 方策反復法では評価と改善を最大限に行った
- 評価と改善を最小限に行うとどうなるか

価値反復法(value iteration)

復習: 方策反復法では価値関数を繰り返し更新する

価値反復法では，1状態を更新したらすぐに改善作業を行う．
- 改善フェーズの計算と，評価フェーズの計算が同じになる．
```
改善
\mu(s) = argmax_a \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V(s')}

評価
a = \mu(s) として
V'(s) = \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V(s')}
```

2つの計算をまとめる
```
V'(s) = max_a \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V(s')}

書き換え

V_{k+1}(s) = max_a \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V_k(s')}
```
- 方策`\mu`なしで価値関数を更新している

`V_*(s)`が得られた時の最適方策`\mu_*(s)`
```
\mu_*(s) argmax_a \sum_{s'} p(s'|s,a){r(s,a,s') + \gamma V_*(s')}
```

価値反復法の実装
- 3x4のグリッドワールドにおいて，状態遷移は決定論的
- 簡略化
```
s' = f(s,a)として
V'(s) = max_a {r(s,a,s') + \gamma V(s')}
```

#### まとめ
- 動的計画法(DP)を使った最適方策を得る手法
- 方策反復法と評価反復法

## 5章 モンテカルロ法
動的計画法を使った最適価値関数と最適方策を得る方法
- 環境のモデル(状態遷移確率と報酬関数)が既知である必要がある
- DPでも計算量が膨大になる場合も

モンテカルロ法(Monte Carlo method)
- データのサンプリングを繰り返して推定する手法の総称
- 強化学習では価値関数を推定する

分布モデル(distribution model): 確率分布として表されたモデル

サンプルモデル(sample model): サンプリングによるモデル(シミュレーション?)

サンプルモデルを使って期待値を計算
- たくさんのサンプルを取って，平均値を取る(モンテカルロ法)
- 平均を取る際にはインクリメンタルな実装で効率化する
