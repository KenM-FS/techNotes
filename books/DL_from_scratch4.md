# ゼロから作るDeep Learning 4 -強化学習編
齊藤 康毅. 株式会社オライリー・ジャパン, 2022.04.04

Finish reading at:

###### Purpose
Learn about Reinforcement Learning for my research

## Notes
GitHub: https://github.com/oreilly-japan/deep-learning-from-scratch-4

Workbook: https://koki0702.github.io/dezero-p100/

### 1章 バンディット問題
#### 1.1 機械学習の分類と強化学習
教師あり学習，教師なし学習，強化学習とその違いについて

#### 1.2 バンディット問題
Bandit Problem
- "bandit" はスロットマシンの別称 (one-armed bandit(一本腕の盗賊)と揶揄されていた)
- 正確には"多腕バンディット問題(multi-armed bandit problem)"という

概要
- 一本のレバーがあるスロットマシンが複数台ある
- スロットマシンごとに特性が異なる(当たりの多い少ない)
- 特性の情報は与えられない
- 決められた回数プレイし，得られるコインをできるだけ多くする

バンディット問題を強化学習の用語へ変換
- スロットマシン -> 環境(environment)
- プレイヤー -> エージェント(agent)
- スロットマシンの選択とプレイ -> 行動(action)
- コインの受け取り -> 報酬(reward)

バンディット問題を解くアルゴリズムを考える
##### 「良い」スロットマシンの選択
確率を考える(各スロットマシンの具体的な確率は与えられない)(確率分布表で表現される)
- 期待値(expectation value)
    - バンディット問題では「価値(value)」と呼ぶ
    - 行動に対して得られる報酬の期待値は「行動価値(action value)」と呼ぶ

報酬を`R`で表す(Reward)．`R`は`{0,1,5,10}`のいずれかの値をとり，各値に取りやすさとして確率が決まっている
- 確率変数(random value): 取る値が確率的に決まる変数のこと
- `t`回目に得られる報酬は`R_t`

エージェントの行う行動を`A`で表す(Agent)．変数`A`はスロットマシン(aとbの2つなら`{a,b}`)の値をとる．

確率変数に定義される期待値は`\mathbb{E}`で表す(Expectation)．
- 報酬`R`の期待値は`\mathbb{E}[R]`
- 行動Aを選んだ場合の報酬の期待値は`\mathbb{E}[R|A]` (条件付確率)
    - aのスロットを選択した場合，`\mathbb{E}[R|A=a]` or `\mathbb{E}[R|a]

報酬の期待値(行動価値)は`Q`,`q`で表す(Quantity)．
- 行動`A`に対する価値は`q(A) = \mathbb{E}[R|A]`
- 小文字の場合は，真の行動価値を表し，大文字の場合は推定値を表す．一般的には真の行動価値は知りえないので，それを推定する．

##### バンディットアルゴリズム
エージェントは行動の結果から自分の選択がどれだけ良かったのか(悪かったのか)を推定する必要がある
- もし各スロットマシンの価値(報酬の期待値)がわかれば，プレイヤーは最も良いスロットマシンを選ぶことができる
- しかし，各スロットマシンの価値はプレイヤーにはわからない
- よってプレイヤーには各スロットマシンの価値をできるだけ精度よく推定することが求められる

実際に得られた報酬の平均値をスロットマシンの価値の推定値とする
- 標本平均(大数の法則により真の値に近づく)

##### 平均値を求める実装
ランダムな数の生成をn回行いそれらを足し合わせ，nで割る
- nが増加すると計算量(和算)もメモリも増加する問題点がある

式変換により，
` Q_n = (1 - 1/n)Q_{n-1} + \frac{1}{n}R_n`
と表すことができる．
- これまでの報酬(`R_1,R_2,...,R_{n-1}`)が必要ない

さらに，
`Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})`
としたとき，
- 右辺第二項が，`Q_n`の`Q_{n-1}`に対する増加量である
- `1/n`は学習率としての役割がある

```py
Q += (reward - Q) / n
```

##### 戦略
greedyな行動: これまでの結果をもとに，一番良いスロットマシンを選ぶこと
- 活用(exploitation)とも呼ぶ
- 不確かさが原因に信頼の低い選択をし続けてしまう問題点

探索(exploration)により，各スロットマシンの価値をより高精度に推定する

活用と探索はトレードオフの関係にある．どうバランスをとるか
- ε-greedy法: 最も基本的で応用の利くアルゴリズム
    - εの確率(e.g. ε=0.1)で，探索を行い，それ以外は活用を行う

#### 1.4 バンディットアルゴリズムの実装
- スロットマシンが返すコインの枚数は0枚 or 1枚 (負けor勝ち)
- スロットマシンに設定されている確率は勝つ確率
- 10台のスロットマシン

スロットマシンの実装(Banditクラス)
- initのnp.random.rand(arms=10)で，10個の0.0以上1.0未満の値を取る(各スロットマシンの勝率)
- playクラス
    - randして，該当するarmの勝率より小さければreturn1, else return 0

エージェントの実装
- 配列Qn: 対応するスロットマシンの推定値を格納
- `epsilon`はランダムに行動する確率
- `action_size`は行動の数(10台のスロットマシン)
- `update`メソッドで価値の推定を更新

ランダム性により実験のたびに結果が異なる
- 実験を複数回行って平均的な結果を取る
- 実験の回数ごとに各ステップで平均をとる

εごとの変化
- 0.01, 0.1, 0.3 高いと探索しすぎる，低いと最善のマシンを見つけられない．

#### 1.5 非定常問題
バンディット問題は，定常問題(stationary problem, 報酬の確率分布が定常である)

非定常問題(non-stationary problem): プレイするたびにスロットマシンの価値(勝率)が変動する

標本平均は得られた報酬の平均によって求めることができる
- `1/n`を重みとみなすことができる
- 非定常問題において，過去のデータの重要性は時間とともに低くするべき
- `Q_n = Q_{n-1} + 1/n(R_n - Q_{n-1})`において，`1/n`を固定値`\alpha`に置き換える(`0<\alpha<1`)
- 固定値にすることで，過去の報酬の重みが指数関数的にちいs買う成る．
- 式変形し，nを1つ下げ，Q_{n-1}とQ_{n-2}の関係性をみるとその様子がみれる
- 指数移動平均, exponential moving average
- 指数加重移動平均, exponential weighted moving average

Q_0が必要であり，その初期値は手動で設定するため学習に偏り(バイアス)が発生する．

#### 1.6 まとめ
バンディットアルゴリズムは，複数の選択肢から最善の選択肢を選ぶ問題に使う
- 売り上げに貢献するWebデザインの選択
- 有効な薬の選択

バンディットアルゴリズムのε-greedy法以外の手法
- UCB(upper confidence bound)アルゴリズム
- 勾配バンディットアルゴリズム

平均
- 定常問題の場合は標本軽金
- 非定常問題の場合は指数移動平均

### 第2章 マルコフ決定過程
エージェントの行動によって状況が変化する問題
- 非定常問題ではエージェントの行動に関係なく時間とともに変化していた

このような問題の一部はマルコフ決定過程(Markov decision process; MDP)として定式化される
- 決定過程とは，エージェントが環境と相互作用しながら行動を決定する過程
- エージェントが置かれる「状態(state)」
- MDPには時間の概念が必要(タイムステップ単位)
- 目先の報酬ではなく報酬の総和を最大化することが求められる
- 行動A_tを行い，報酬R_tを得て，次の状態であるS_{t+1}へ遷移する
    - 報酬はR_tだったり，R_{t+1}だったりする(慣例)

#### 環境とエージェントの定式化
MDPにおけるエージェントと環境のやり取りを数式によって定式化
- 状態遷移
- 報酬
- 方策

決定論的(deterministic)な状態遷移: 次の状態s'が，現在の状態sと行動aによって一意に決まる事
- `s' = f(s, a)` <- 状態遷移関数 (state transition function)

確率的(stochastic)な状態遷移
- `p(s'|s, a)` <- 状態遷移確率 (state transition probability)
    - 状態sで行動aを選択したときに状態s'へ遷移する確率

マルコフ性(Markov property): 次の状態が現在の状態と行動にだけ依存すること
- MDPはマルコフ性を仮定することによって，遷移状態と報酬をモデル化する
- 仮定できなければ，過去のすべての状態や行動を考慮しなければならず指数関数的な情報を処理しないといけなくなる

報酬関数(reward function)
- `r(s,a,s')`: 状態sにいるときに，行動aを行い，状態s'に遷移したときに得られる報酬
- 遷移が確率的であるなら，報酬関数は報酬の期待値を返す

エージェントの方策
- 方策(policy)
- マルコフ性により，エージェントは現在の状態だけに基づいて行動を決める
- 決定論的な方策の例
    - `a = \mu(s)`: 状態sを引数として与えると，行動aが帰ってくる
- 確率的な方策の例
    - `\pi(a|s)`: ある状態sにいるときに行動aを取る確率を表す

MDPの目標
- エージェントは方策`\pi(a|s)`によって行動し，その行動と状態遷移確率`p(s'|s,a)`によって次の状態へ遷移し，報酬関数`r(s,a,s')`に従って報酬が与えられる
    - 決定論的な方策を持つ場合は`\mu`で表すことができる
- 以上の枠組みの中で最適方策(optimal policy)を見つけることがMDPの目標となる
    - 最適方策は，収益が最大となる方策
- MDPの問題は大きく2つに分類する
    - エピソードタスクと連続タスク

エピソードタスク
- 終わりのある問題
    - e.g. 囲碁(win/lose/draw)
- 終わるとまた初期状態から始まる

連続タスク
- 終わりのない問題
    - e.g. 在庫管理問題(適切な商品の仕入れを永遠に行う)

収益(return)
- `G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ...`
    - 時刻`t`に状態`S_t`である場合に，エージェントが方策`\pi`によって行動`A_t`を行い，報酬`R_t`を得て，新しい状態`S_{t+1}`に遷移する場合の収益`G_t`
- 時間が進むにしたがって報酬が割引率(discount rate)`\gamma`によって指数関数的に減衰される
    - 連続タスクにおいて収益が無限大になることを防ぐため

状態価値関数
- エージェントと環境は確率的にふるまう可能性がある
- 収益の期待値を指標とする必要がある(状態`S_t=s`，方策が`\pi`であるときエージェントが受け取る収益の期待値)
    - `v_\pi(s) = \mathbb{E}[G_t | S_t = s, \pi]`
    - `v_\pi(s)` 状態価値関数(state-value function)
    - 別の表現 `v_\pi(s) = \mathbb{E}_\pi [G_t | S_t=s]`
- `v_\pi`は真の状態価値関数，`V_\pi`は推定値としての状態価値関数

最適方策と最適価値関数
- すべての状態において`v_{\pi'}(s) >= v_\pi(s)`となる時のみ，方策`\pi'`が方策`\pi`よりも良い方策だと言える
- MDPでは最適方策が少なくとも1つは存在し，その最適方策は決定論的方策である
    - `a = \mu_*(s)`
- 最適状態価値関数(optimal state-value function): 最適方策における状態価値関数．`v_*`

#### MDPの例
具体的な問題設定
- 2マスのグリッドワールド(L1,L2) w/ 両端に壁
- エージェントは右or左のどちらかに移動
- エージェントがL1からL2に移動したときにリンゴを受け取り`+1`の報酬を得る
- エージェントがL2からL1に移動するとリンゴが出現
- 壁にぶつかると`-1`の報酬を得る
- 連続タスク

バックアップ線図: 有向グラフによって，状態・行動・報酬を表現したもの
- エージェントの方策が決定論的な場合，一直線になる
- 確率的な場合，無限に2分(右と左)していく

最適方策を見つける
- 決定論的方策`a = \mu(s)`
- 今回は状態が2つ，行動が2つで4通りの方策がある

状態価値関数の計算(`\mu_1`)
```
v_{\mu_1}(s=L1) = 1 + 0.9(-1) + 0.9^2(-1) + ...
= 1 - 0.9(1+0.9+0.9^2+...)
= 1 - 0.9(1-0.9)
= -8
```
- 参考: 無限等比級数の公式 `1+r+r^2+...=1/(1-r)`
- コードではfor文によって近似する

各方策の価値関数により，方策`\mu_2`が最適方策(5.26)であることがわかる．

### まとめ
- MDPはエージェントと環境の相互のやり取りを定式化したもの
- 環境には状態遷移確率(状態遷移関数)と報酬関数がある
- エージェントには方策がある
- 最適方策とは，すべての状態において他のどの方策よりも状態価値関数の値が大きいまたは等しい方策

## 第3章 ベルマン方程式
確率的なふるまいをするMDPの場合は手計算による状態価値関数を求めることができない

このような状態でも状態価値関数を求めるベルマン方程式(Bellman equation)

### ベルマン方程式の導出
前知識
- 確率の期待値について
- 条件付確率について
- 期待値の表現

収益(リターン)
`G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ...`

次のように変換できる
`G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...`

2つの式より
`G_t = R_t + \gamma G_{t+1}`...(3.3)
収益である`G_t`と`G_{t+1}`の関係性

状態価値関数(収益の期待値)
`v_\pi (s) = \mathcal{E}_\pi [G_t | S_t = s]`

式(3.3)を代入すると，
```
v_\pi (s) = \mathcal{E}_\pi [R_t + \gamma G_{t+1} | S_t = s]
= \mathcal{E}_\pi [R_t | S_t = s] + \gamma \mathcal{E}_\pi [G_{t+1} | S_t = s]
```
- 最後の展開は，期待値の線形性により成り立つ
- 確率的方策，確率的状態遷移を想定する(決定論的もカバーできる)

まず，`\mathcal{E}_\pi [R_t | S_t = s]`を計算する．
- 行動`a`の確率と，それらの行動について確率的な遷移`s'`がある

よって
```
\mathcal{E}_\pi [R_t | S_t = s] = \sum_a \sum_s' \pi(a|s) p(s'|s,a) r(s,a,s')
```
- エージェントの行動の確率`\pi`
- 遷移先となる状態の確率`p`
- 報酬関数`r`
- `\sum_a,s'`とまとめて書く

次に第2項(`\mathcal{E}_\pi [G_{t+1} | S_t = s]`)について
- `v_\pi (s) = \mathcal{E}_\pi [G_{t+1} | S_{t+1} = s]`を考える
- 求めたい式の，状態の時刻を1つ進める
    - 具体例: 状態`S_t=s`で，`0.2`の確率で行動`a_1`を選び，`0.6`の確率で`s_1`に遷移する
- すべての候補について計算を行い和を求める
- 現在の状態から遷移する可能性のある遷移先での収益の期待値(期待値の期待値?)
```
\mathcal{E}_\pi [G_{t+1} | S_t = s] = \sum_{a,s'} \pi(a|s) p(s'|s,a) \mathcal{E}_\pi [G_{t+1} | S_{t+1} = s']
= \sum_{a,s'} \pi(a|s) p(s'|s,a) v_\pi(s')
```

以上より，ベルマン方程式
```
v_\pi (s) = \sum_{a,s'} \pi(a|s) p(s'|s,a) {r(s,a,s') + \gamma v_\pi(s')}
```
- 状態`s`の価値関数とその次に取りえる状態`s'`の価値関数との関係性を示した方程式
- 全ての状態`s`とすべての方策`\pi`について成り立つ
