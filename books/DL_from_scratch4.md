# ゼロから作るDeep Learning 4 -強化学習編
齊藤 康毅. 株式会社オライリー・ジャパン, 2022.04.04

Finish reading at:

###### Purpose
Learn about Reinforcement Learning for my research

## Notes
GitHub: https://github.com/oreilly-japan/deep-learning-from-scratch-4

Workbook: https://koki0702.github.io/dezero-p100/

### 1章 バンディット問題
#### 1.1 機械学習の分類と強化学習
教師あり学習，教師なし学習，強化学習とその違いについて

#### 1.2 バンディット問題
Bandit Problem
- "bandit" はスロットマシンの別称 (one-armed bandit(一本腕の盗賊)と揶揄されていた)
- 正確には"多腕バンディット問題(multi-armed bandit problem)"という

概要
- 一本のレバーがあるスロットマシンが複数台ある
- スロットマシンごとに特性が異なる(当たりの多い少ない)
- 特性の情報は与えられない
- 決められた回数プレイし，得られるコインをできるだけ多くする

バンディット問題を強化学習の用語へ変換
- スロットマシン -> 環境(environment)
- プレイヤー -> エージェント(agent)
- スロットマシンの選択とプレイ -> 行動(action)
- コインの受け取り -> 報酬(reward)

バンディット問題を解くアルゴリズムを考える
##### 「良い」スロットマシンの選択
確率を考える(各スロットマシンの具体的な確率は与えられない)(確率分布表で表現される)
- 期待値(expectation value)
    - バンディット問題では「価値(value)」と呼ぶ
    - 行動に対して得られる報酬の期待値は「行動価値(action value)」と呼ぶ

報酬を`R`で表す(Reward)．`R`は`{0,1,5,10}`のいずれかの値をとり，各値に取りやすさとして確率が決まっている
- 確率変数(random value): 取る値が確率的に決まる変数のこと
- `t`回目に得られる報酬は`R_t`

エージェントの行う行動を`A`で表す(Agent)．変数`A`はスロットマシン(aとbの2つなら`{a,b}`)の値をとる．

確率変数に定義される期待値は`\bf{E}`で表す(Expectation)．
- 報酬`R`の期待値は`\bf{E}[R]`
- 行動Aを選んだ場合の報酬の期待値は`\bf{E}[R|A]` (条件付確率)
    - aのスロットを選択した場合，`\bf{E}[R|A=a]` or `\bf{E}[R|a]

報酬の期待値(行動価値)は`Q`,`q`で表す(Quantity)．
- 行動`A`に対する価値は`q(A) = \bf{E}[R|A]`
- 小文字の場合は，真の行動価値を表し，大文字の場合は推定値を表す．一般的には真の行動価値は知りえないので，それを推定する．

##### バンディットアルゴリズム
エージェントは行動の結果から自分の選択がどれだけ良かったのか(悪かったのか)を推定する必要がある
- もし各スロットマシンの価値(報酬の期待値)がわかれば，プレイヤーは最も良いスロットマシンを選ぶことができる
- しかし，各スロットマシンの価値はプレイヤーにはわからない
- よってプレイヤーには各スロットマシンの価値をできるだけ精度よく推定することが求められる

実際に得られた報酬の平均値をスロットマシンの価値の推定値とする
- 標本平均(大数の法則により真の値に近づく)

##### 平均値を求める実装
ランダムな数の生成をn回行いそれらを足し合わせ，nで割る
- nが増加すると計算量(和算)もメモリも増加する問題点がある

式変換により，
` Q_n = (1 - 1/n)Q_{n-1} + \frac{1}{n}R_n`
と表すことができる．
- これまでの報酬(`R_1,R_2,...,R_{n-1}`)が必要ない

さらに，
`Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})`
としたとき，
- 右辺第二項が，`Q_n`の`Q_{n-1}`に対する増加量である
- `1/n`は学習率としての役割がある

```py
Q += (reward - Q) / n
```

##### 戦略
greedyな行動: これまでの結果をもとに，一番良いスロットマシンを選ぶこと
- 活用(exploitation)とも呼ぶ
- 不確かさが原因に信頼の低い選択をし続けてしまう問題点

探索(exploration)により，各スロットマシンの価値をより高精度に推定する

活用と探索はトレードオフの関係にある．どうバランスをとるか
- ε-greedy法: 最も基本的で応用の利くアルゴリズム
    - εの確率(e.g. ε=0.1)で，探索を行い，それ以外は活用を行う
