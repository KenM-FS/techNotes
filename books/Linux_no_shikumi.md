# [試して理解] Linuxの仕組み ～実験と図解で学ぶOSとハードウェアの基礎知識
武内 覚. 技術評論社, 2018.03.08

Finish reading at:

###### Purpose
To learn the basics and fundamentals of Linus OS in order to introduce Arch Linux to my VAIO laptop.

## Notes
https://github.com/satoru-takeuchi/linux-in-practice/

### 第1章 コンピュータシステムの概要
- ユーザーモード: プロセス
- カーネルモード: デバイスドライバ->デバイス，プロセス管理システム，プロセススケジューラ，メモリ管理システム

カーネルモードで動作する(OSの核となる)処理をまとめたプログラム -> カーネル

プロセスはシステムコールを介してカーネルにカーネルの機能を依頼

### 第2章 ユーザーモードで実現する機能
#### システムコール
システムコール: カーネルへの処理の依頼
- プロセス生成，削除
- メモリ確保，解放
- プロセス間通信
- ネットワーク
- ファイルシステム操作
- ファイル操作(デバイスアクセス)

`strace -o hoge.log python3 ./code.py`でシステムコールを確認することができる

`sar -P ALL 1`: 1秒ごとに各CPUコアの処理を見る
- `%user`, `%nice`: ユーザーモードの処理(各プロセスの違いは4章)
- `%system`: カーネルモードでシステムコールなどの処理
- `%idle`: アイドル状態(詳しくは4章)

`sar -P ALL 1 1`: 情報を取得する回数を第4引数に追加

`%system`が数十程度なら，むやみなシステムコールを発行していたり，システムの付加が高いなど良くない状態であることが多い

`strace -T -o hello.log ./hello`: helloプログラムに対してどのシステムコールに時間がかかっているか分かる
(`-tt`オプションでm秒表示)

#### システムコールのラッパー関数
ラッパー関数: OSにある，内部的にシステムコールを呼び出す関数．
各高級言語(e.g. C言語)はラッパー関数を呼び出すだけでよくなる．
さもなくば，各プログラムがシステムコールのたびにアセンブリ言語(アーキテクチャ依存)を書かなければならない．

#### 標準Cライブラリ
glibc: GNUプロジェクトによる標準Cライブラリ(ISOによって定められたC言語の標準ライブラリに準している)．
通常はLinuxも含めこのライブラリが用いられる．
システムコールのラッパー関数を含む．
また，POSIX(UNIX系OSが備えているべき各種機能を定めた規格)に定義されている関数も提供．

`ldd`によってプログラムがどのようなライブラリをリンクしているか見る
- `ldd /bin/echo`: echoコマンド，書籍ではlibcがリンクされていた．

例えばPython3もlibcがリンクされており，`python3`コマンド自身は内部的には標準Cライブラリを用いている．
C言語は現在もOSレベルでは重要な言語

#### OSが提供するプログラム
OSの提供するプログラム(一例)
- touch, sysctl, grep, gcc, python, bash, ect

### 第3章 プロセス管理
仮想記憶がない単純な場合の説明．実際のプロセスの生成，削除の仕組みは第5章で．

#### 2段階のプロセス生成
Linuxにおけるプロセス生成の目的
- 同じプログラムの処理を複数のプロセスに分けて処理する(e.g. Webサーバーによる複数リクエストの受付)`fork()`(システムコール`clone()`)
- 全く別のプログラムを生成する(e.g. bashから各種プログラムの新規生成)`execve()`(システムコール`execve()`)

#### fork()関数
fork()関数の発行で，発行したプロセス(親プロセス)をもとに，新たにプロセス(子プロセス)を1つ生成する．

プロセスの新規生成プロセスは以下の通り
1. 子プロセス用メモリ領域を作成して，親プロセスのメモリをコピー
2. 親プロセスと子プロセスが違うコードを実行するように分岐(fork()関数の戻り値が親プロセスと子プロセスで異なることを利用)

fork()関数の戻り値
- 親プロセスなら，子プロセスのプロセスID
- 子プロセスなら，0

#### execve()関数
カーネルがそれぞれのプロセスを実行するまでの流れ
1. 実行ファイルを読みだして，プロセスのメモリマップに必要な情報を読みだす
2. 現在のプロセスのメモリを新しいプロセスのデータで上書き
3. 新しいプロセスの最初の命令から実行開始する

実行ファイルはプロセス実行中に用いるコードとデータ以外にも，実行開始に必要なデータも保持している
- コードを含むデータ領域のファイル上オフセット(基準からの距離)，サイズ，およびメモリマップ開始アドレス
- コード以外の変数などデータ領域についての上記と同じ情報
- 最初に実行する命令のメモリアドレス(エントリポイント)

Linuxの実行ファイルでは，Eexecutable Linkable Format(ELF)を用いる．
ELFの各種情報は`readelf`コマンドで得られる．
- `readelf -h /bin/sleep`: /bin/sleepの開始アドレスを得る
- `-S`オプションで，コードとデータのファイル内オフセット，サイズ，開始アドレスを得る

`cat /proc/{pid}/maps`でプログラム実行時に作成されたプロセスのメモリマップを得られる
- e.g. sleepコマンド: `/bin/sleep 10000 &` -> `cat /proc/{pid}/maps`

全く別のプロセスを新規生成する際には，親プロセスからfork()を発行して，復帰後に子プロセスがexec()を呼ぶことが多い(fork and exec)
- e.g. bashのプロセスが，echoのプロセスを生成

Pythonでは，`os.exec()`関数を経由してexecve()システムコールを読みだす

#### 終了処理
_exit()関数(システムコールexit_group())を使用してプログラムを終了．
プロセスに割り当てていたメモリをすべて回収．

### 第4章 プロセススケジューラ
プロセススケジューラ: 複数のプロセスを同時に動作させる(ように見せかける)機能

#### 実験プログラムの仕様
- ある時点で，論理CPU上ではどのプロセスが動作しているか
- それぞれの進捗はどれだけか
#### 実験プログラムの実装
https://github.com/satoru-takeuchi/linux-in-practice/blob/master/04-process-scheduler/sched.c

loops_per_msec()関数: 最初に適当な回数(NLOOP_FOR_ESTIMATION)だけ何もしないループを回して，その所要時間を測定し，それをNLOOP_FOR_ESTIMATIONで割ることで何回ループを回せばCPUの処理で1ミリ秒かかるか推定

#### 実験
`$ taskset -c 0 {プログラムへのpath}`で0番の論理CPUのみを持ちいて実行させることができる

#### 考察
- 同時に何個のプロセスが実行していようとも，ある瞬間に論理CPU上で動作出来るプロセスは1つだけ
- ラウンドロビン方式で複数のプロセスを回している
- 各プロセスのタイムスライスはほぼ同じ
- プロセス終了までの時間経過はプロセス数に比例して増加

#### コンテキストスイッチ
論理CPU上で動作するプロセスが切り替わること．

コンテキストスイッチはプロセスが実行しているコードに関わらずタイムスライスによって発生する．

#### プロセスの状態
`$ ps ax`: プロセスの一覧表示

`$ ps ax | wc -l`: 行数をカウントして，プロセスの個数を見る

プロセスの状態(一部)
- 実行状態: 現在論理CPUを使っている
- 実行待ち状態: CPU時間が割り当てられるのを待機している
- スリープ状態: 何らかのイベントが発生するのを待っている．CPU時間は使わない
    - 所定時間の経過・Input・ストレージへの読み書き・ネットワークのデータ送受信
- ゾンビ状態: プロセスが終了した後に親プロセスが終了状態を受け取るのを待っている

`ps ax`の"STAT"にて状態がわかる．
最初の一文字の種類
- `R`: 実行状態 or 実行待ち状態
- `S`: シグナル待ちによるスリープ状態
- `D`: アクセス待ち(ストレージ)によるスリープ状態
- `Z`: ゾンビ状態

#### 状態遷移
図04-12 プロセスの状態

#### アイドル状態
アイドル中は，「何もしない」特殊なプロセスが動作する．
- 無駄なループをする -> 電気の無駄
- 論理CPUを特殊な命令により休止状態にして，1つ以上のプロセスが実行可能状態になるまで消費電力を抑えた状態で待機

#### さまざまな状態遷移
状態が遷移する場面を考える
- ユーザーからの入力を受け取る
- 入力をもとにファイルを読みだす

図04-16 & 図04-17

ポイント
1. 論理CPU上で一度に実行できるプロセスは1つだけ
2. スリープ状態においてはCPU時間を使わない

#### スループットとレイテンシ
- スループット: 単位時間あたりの総仕事量．higher is better
    - `スループット = 完了したプロセスの量 / 経過時間`
    - アイドル時間の割合が低いほど高くなる(スループットのみを考えれば．レイテンシなし)
- レイテンシ: それぞれの処理の開始から終了までの経過時間．shorter is better
    - `レイテンシ = 処理終了時刻 - 処理開始時刻`

論理CPUだけでなく，ストレージデバイスの性能も含まれる(が，本書では論理CPUのみ考える)

まとめ
- 論理CPUの能力を使い切っている場合，いくらプロセスを増やしてもスループットは変わらない
- プロセスを増やすほどレイテンシが悪化
- 各プロセスの平均レイテンシは等しい

#### 実際のシステム
実際のシステムにおける論理CPUでは以下の状態が目まぐるしく変化
- アイドル状態: スループットが落ちる傾向にある
- プロセスが動作中: (実行待ちのプロセスがないとき)理想的な状態，実行可能プロセスが発生するとレイテンシが全て長くなる
- プロセスの動作中 w/ 実行待ちプロセス: スループットは高いがレイテンシが長くなる傾向にある

以上を考えて，sarの%idleやsar -qのrunq-szフィールド(実行中および実行待ちプロセスの数を示す)のデータをもとにシステムをチューニングすることになる．

#### 論理CPUが複数の場合のスケジューリング
スケジューラの中の，複数論理CPUを扱うためのロードバランサ or グローバルスケジューラが機能する．
ロードバランサは(簡単にいうと，)複数の論理CPU間でプロセスを公平に分配する．
プロセスを割り振られた各論理CPU内で，各プロセスに平等にCPU時間を分配

#### 実験方法
前回の実験で用いたプログラムを複数動かす．

`grep -c processor /proc/cpuinfo`: マシンに搭載されている論理CPUの数を求める

`taskset -c 0,4 ./sched プロセス数 1000 1`: "0,4"でCPUを指定．
CPU0とCPU4を指定する理由は，キャッシュメモリを共有しておらず独立性が高い(性能測定に適している)(キャッシュメモリについて詳しくは第6章)
CPUの数が異なる場合は，論理CPU0と，「論理CPU数/2」であるCPUを選べばたいていよい．

ハイパースプレッドが有効な環境では期待とは異なる結果になる(詳しくは第6章)

#### 実験結果
- プロセス数=1: 単一論理CPUと同じ
- プロセス数=2: アイドル状態がないので計算リソースを最大限使用できる
- プロセス数=4: 2つの論理CPUで，2つのプロセスが交互に動作．所要時間が各プロセスが論理CPUを独占できていた時と比べて倍

#### スループットとレイテンシ
| プロセス数 | スループット  | レイテンシ |
|-----------:|:--------------|:-----------|
|           1| 10プロセス/秒 | 100ミリ秒  |
|           2| 20プロセス/秒 | 100ミリ秒  |
|           4| 20プロセス/秒 | 200ミリ秒  |

#### 考察
- 1つのCPU上で同時に処理するプロセスは1つだけ
- 複数のプロセスが実行可能な場合，個々のプロセスの適当な長さの時間(タイムスライス)ごとにCPU上で順番に処理
- マルチコアCPU環境では，複数プロセスを同時に動かさないとスループットが上がらない
- 単一論理CPUの場合と同様にプロセス数を論理CPU数より多くしてもスループットは上がらない

#### 経過時間と使用時間
timeコマンドにより，該当プロセスの開始から終了までの時間までの経過時間と使用時間が得られる
- 経過時間: プロセスが開始してから終了するまでの経過時間
- 使用時間: プロセスが実際に論理CPUを使用した時間

`time taskset -c 0 ./sched 1 10000 10000`
- `real`が経過時間
- `user` + `sys`が使用時間
    - `user`はユーザーモードでCPU時間を使用した時間
    - `sys`はプロセス実行中に依頼によってカーネルがシステムコールを実行していた時間
- 論理CPU数が増えれば，使用時間は増える

#### スリープするプロセス
`time sleep 10`
- `real`は10
- `user`&`sys`は0

#### 現実のプロセス
timeコマンド以外にプロセスの使用時間と経過時間を得る方法
- `ps -eo`コマンドのetimeフィールド(プロセス開始から現在までの経過時間)とtimeフィールド(使用時間)
    - `ps -eo pid.comm.time.etime`: プロセスごとのID,コマンド名，経過時間，使用時間

#### 優先度の変更
特定のプロセスに`nice()`システムコールによって実行優先度を付けることができる
- -19 から 20 の間で設定(デフォルトは0)
- 優先度の高いプロセスは多くのCPU時間を得られる(逆もまた然り)
    - 優先度を挙げられるのはroot権限を持つユーザのみ(下げるのは誰でもOK)

sched_nice.c で実装

処理終了時間は変わらない．

`nice -n 5 echo hello`: `nice`コマンドでも可能

`sar`コマンドにおいて，"%nice"フィールドは，優先度を変更したプログラムを実行している時間の割合

`taskset`コマンドも，スケジューラ関係のプログラム．
実行中にプロセスの実行を特定の倫理CPUに制限する`sched_setaffinity()`システムコールを呼び出している．

### 第5章 メモリ管理
#### メモリに関する統計情報
`free`コマンド: システムが搭載するメモリの量 & 使用中のメモリの量．

'Mem:'行
- totalフィールド: システムに搭載されている全メモリの量
- freeフィールド: 見かけ上の空きメモリ
- buff/cacheフィールド: バッファキャッシュ，ページキャッシュ(詳しくは第6章)が利用するメモリ．システムの空きメモリ(freeフィールド)が減少してきたら解放される
- availableフィールド: 実質的な空きメモリ．freeフィールド + 解放できるカーネル内メモリ領域のサイズ．解放できるメモリは，バッファキャッシュやページキャッシュ，その他カーネル内メモリの一部(の解放可能な部分)．

`sar -r 1`によって1秒間隔でメモリに関する統計情報
- kbmemfree: freeコマンドで言うfreeフィールド
- kbbuffers + kbcached: freeコマンドで言うbuff/chacheフィールド
- その他: 該当なし

#### Out of Memory
空きメモリが少なくなってくると，メモリ管理システムがカーネル内の解放可能なメモリ領域を開放する．

OOM: Out Of Memory, システムが何をするにもメモリが足りなくなり身動きが出来なくなっている状態
適当なプロセスを選んで強制終了(kill)することによってメモリを開放する OOM killer という機能

適当なプロセスを強制終了されるのが困る場合(サーバー等)，`sysctl`の'vm.panic_on_oom'パラメータを`1`にすることで防がれる

#### 単純なメモリ割り当て
単純なメモリ割り当ての仕組み(仮想記憶なし)

カーネルがプロセスにメモリを割り当てるタイミング
- プロセス生成時
- プロセス生成後，追加で動的にメモリを割り当てるとき
    - プロセスがメモリ獲得用のシステムコールで要求
    - カーネルは空きメモリ領域から切り出して，先頭アドレスを返す
    - このようなメモリ割り当ての問題点
        - メモリの断片化
        - 別用途のメモリにアクセスできてしまう
        - マルチプロセスの扱いが困難

メモリの断片化: わかる

別用途のメモリにアクセスできてしまう: アドレスを指定できればどこでもアクセスできてしまう(単純な仕組みでは)

マルチプロセスの扱いが困難: プロセスを複製してメモリにマップする際，命令とデータが示すメモリアドレスが異なる(元のプロセスと同じアドレスじゃないといけないが，同じだと困る)

#### 仮想記憶
プロセスからシステムに搭載されているメモリに直接アクセスさせず，仮想アドレスを用いて間接的にアクセスさせる．
- 仮想アドレス: プロセスから見えるメモリのアドレス
- 物理アドレス: システムに搭載されているメモリの実際のアドレス

#### ページテーブル
仮想アドレスから物理アドレスへの変換に用いられる．カーネルが使うメモリ内に保存されている

すべてのメモリをページ単位で区切り，管理．変換もページ単位で．

ページテーブルエントリ: ページテーブル中の1つのページに対応するデータ．
仮想アドレスと物理アドレスの対応情報が入っている

ページサイズはCPUアーキテクチャごとに異なる(x86_64アーキテクチャにおいては4Kバイト)

物理メモリが存在しないページテーブルエントリにアクセスしようとした場合は，ページフォルトという割込みが発生．
実行中の命令が中断されてカーネル内のページフォルトハンドラという処理が働く．
その後，'SIGSEGV'でプロセスに通知し，一般的には強制終了される

#### 実験
不正なアドレスにアクセスするプログラム'segv.c'
```c
int *p = NULL;
*p = 0;
```
結果: `Segmentation fault (core dumped)`

#### プロセスへのメモリ割り当て
仮想記憶においてのプロセスへのメモリ割り当て

プロセス生成時
- 実行ファイルを読みだす．
    - プログラムの実行に必要なメモリサイズ = コード領域サイズ + データ領域サイズ
- 物理メモリ上に割り当て，必要なデータをそこにコピー
- (デマンドページング)
- ページテーブルを作成し，仮想アドレス空間と物理アドレス空間をマッピング
- 所定のアドレスから実行開始すれば完了

追加割り当て時
- 新規にメモリ割り当て & ページテーブル作成
- 割り当てたメモリに対応する仮想アドレスをプロセスに返す

#### 実験
`mmap.c`
1. プロセスのメモリマップ情報(`proc/{pid}/maps`)を表示
2. メモリを新たに100Mバイト確保
3. 再度メモリマップ情報を表示

#### 上位レイヤによるメモリ割り当て
C言語の`malloc()`は，Linuxにおいては内部的に`mmap()`関数を呼び出している

`mmap()`関数はページ単位でメモリを獲得

`malloc()`関数はバイト単位でメモリを獲得
- glibcは事前に`mmap()`で大きなメモリ領域を確保してプールしておく
- `malloc()`関数発行時にその都度必要な量を切り出してプログラムに返す
- プールが少なくなれば`mmap()`で新たに獲得する

プログラムが報告する使用メモリの量と，システムが報告する仕様メモリの量が異なることがある(以上の理由から)

#### 問題の解決
メモリの断片化
- ページテーブルをうまく調整することにより，断片化した物理メモリを仮想的に一つの大きな領域として扱える

別用途のメモリにアクセスできてしまう
- 仮想アドレス空間はプロセスごとに作られるので，他のプロセスはそもそも他のプロセスの空想アドレス空間にアクセスできない
- カーネルのメモリはすべてのプロセスの仮想アドレス空間にマップされている(実装の都合)
    - アクセスにはCPUがカーネルモードでなければいけない

マルチプロセスの扱いが困難
- 仮想アドレス空間はプロセスごとに存在するため，プロセスはメモリアドレスの干渉を気にする必要がない

#### 仮想記憶の応用
- ファイルマップ
- デマンドページング
- コピーオンライト方式の高速なプロセス生成
- スワップ
- 階層型ページテーブル
- ヒュージページ

#### ファイルマップ
ファイルの領域を仮想アドレス空間上にメモリマップする．
マップしたファイルにはメモリアクセスと同じ方法でアクセスできる．
後ほど所定のタイミングでスレージデバイス上のファイルに書き戻される．

#### デマンドページング
メモリ割り当ての手順(前に説明した)はメモリを無駄に消費するという問題点がある．

獲得したメモリの中には，長時間あるいはプロセス終了まで使用しない領域がある
- 大きなプログラム中の，実行中に使用されなかった機能のためのコード領域やデータ領域
- glibcが確保したメモリプールのうち使用されなかった分

-> デマンドページングによる解決

プロセスの仮想アドレス空間内の各ページに対応する物理メモリが，該当ページにアクセスしたときに初めて割り当てられる．「プロセスには未割り当て」，「プロセスに割り当て済みで物理メモリも割り当て済み」 + 「プロセスに割り当て済みだが物理メモリは未割り当て」という状態になる．

物理メモリの割り当て手順
1. プログラムがエントリポイントにアクセス
2. CPUがページテーブルを参照し，エントリポイントが属するページに対応する仮想アドレスが，物理アドレスと紐づいていないことを検出
3. CPUにおいてページフォルトが発生
4. カーネルのページフォルトハンドラが，1. においてアクセスしたページに物理メモリを割り当てたうえで，ページテーブルを書き換える
5. ユーザモードに戻ってプロセスが実行を継続する

`mmap()`関数が成功した(仮想メモリを確保した)にも関わらず，獲得済みメモリへの書き込みの時点で物理メモリの枯渇が発生する可能性がある(物理メモリの確保に失敗)

実験により，以下のことを確認(demand-paging.c)
- メモリを確保すると，仮想メモリ使用料は増加するが，物理メモリ使用量は増加しないこと
- 獲得したメモリへのアクセス時に物理メモリが増加すること，およびその際にページフォルトが発生すること

`sar -r 1`を別の端末で実行しながら'demand-paging.c'を実行する

`sar -B 1`コマンドでページフォルト('faults/s')の階数を確認

メモリ獲得に失敗して異常終了
- 仮想メモリの枯渇
    - 仮想アドレス空間を使い切ったとき(物理メモリは関係ない)
    - 確保だけしてアクセスしないと起こる
    - x86アーキテクチャでは仮想アドレス空間が4Gしかなくデータベースなどの大型プログラムで頻発
    - x86_64アーキテクチャでは128Tある
- 物理メモリの枯渇

#### コピーオンライト(Copy on Write; CoW)
`fork()`システムコール(第3章)も仮想記憶により高速化される．
`fork()`システムコール発行時に，親プロセスのページテーブルのみを子プロセスにコピーする．
この際，親も子も全ページへの書き込み権限を無効にする．
その後，読み取りのみであればどちらのプロセスも共有された物理ページにアクセス可能．

どちらかが書き込みをしようとした際の共有解除手順
1. ページへの書き込みは許可されていないため，CPU上でページフォルトが発生
2. CPUがカーネルモードへ移行して，カーネルモードのページフォルトハンドラが動く
3. ページフォルトハンドラがアクセスされたページを別の場所にコピーして，書き込みをしようとしたプロセスに割り当てたうえで内容を書き換える
4. 各プロセスについて，共有が解除されたページに対応するテーブルエントリを更新する
    - 書き込みしたプロセス: 新たな物理ページと紐づけて書き込み許可
    - 別のプロセス: そのまま書き込み許可

`fork()`システムコールが成功しても物理ページに空きがなくて失敗する場合がある

実験(cow.c)
- `fork()`システムコールの実行後，書き込みが行われるまでメモリ領域は親子プロセスで共有される
- メモリ領域への書き込み時にはページフォルトが発生する

#### スワップ
OOMに対する救済措置「スワップ」: ストレージデバイスの一部を一時的にメモリの代わりとして使用するしくみ．

スワップ領域: 一時的に退避させたストレージデバイス上の領域(windowsでは「仮想メモリ」と呼ばれる．ややこしい)

スワップアウトされる領域は，カーネルの所定のアルゴリズムに基づいて，近いうちにつかわないであろう領域から決定される．

メモリが空いたら，スワップ領域に退避していたデータを物理メモリに戻す(スワップイン)．

スワップアウトとスワップインを含めてスワッピング

スラッシング: システムメモリが一時的ではなく，常に足りない状態ではメモリアクセスの度にスワッピングが発生する状態．

`$ swapon --show`: スワップ領域の確認

`$ free`: でも確認可能

`$ sar -W 1`: システム稼働中にはこれでスワッピングの発生を確認できる．

`$ sar -s`: 'kbswpused'フィールドでスワップ領域の使用量の推移を見る

メジャーフォルト: スワッピングのように，ストレージデバイスへのアクセスが発生するページフォルト

マイナーフォルト: ストレージデバイスへのアクセスが発生しないページフォルト

#### 階層型ページテーブル
プロセスのページテーブルには仮想アドレス空間のすべてのページについて対応する物理メモリが存在するかを示すデータを保持している． -> これをフラットな構造で実装すると...?

x86_64アーキテクチャにおいて仮想アドレス空間の大きさは128Tバイト，1ページ４Kバイト，ページテーブルエントリ8バイト: プロセス1つあたりのページテーブルに256Gバイト(=8*128T/4K)必要になる．

ページテーブルはフラットな構造ではなく階層構造になっている．

0-100, 100-200, 200-300, 300-400, 400-500,,, -> 0-400(0-100, 100-200, 200-300, 300-400), 400-800(,,,),,,

仮想メモリ量がある程度増えると，階層型のほうがフラット型よりも必要メモリ総量が大きくなるが，稀．
基本的には階層型のほうがメモリ量が少なくて済む．

x86_64アーキテクチャでは4段構造になっている．

`sar -r ALL 1`コマンドの，'kbpgtbl'フィールドでシステムが使用している物理メモリのうちページテーブルに使用している量を確認できる．

#### ヒュージページ
ページテーブルにおける物理メモリの使用量が増加すると，`fork()`システムコールも遅くなる(親プロセスのページテーブルをコピーするため)
-> ヒュージページ

通常より大きなサイズのページ．要は階層型をフラット型に近づける．

ヒュージページの使いかた
- C言語の`mmap()`関数の'flags'引数に'MAP_HUGETBL'を与える
- 既存のプログラムの利用設定を有効にする
- など

トランスペアレントヒュージページ: 仮想アドレス空間内の連続する複数の4Kバイトページが所定の条件を満たすと自動的にまとめてヒュージページにしてくれる機能
